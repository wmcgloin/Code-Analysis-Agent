{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce76b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\docs\\notebooks\n",
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('c:\\\\Users\\\\JaeHoBahng\\\\Desktop\\\\Georgetown\\\\2025_Spring\\\\DSAN_6725\\\\project\\\\spring-2025-final-project-project-group-2\\\\src')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c0133",
   "metadata": {},
   "source": [
    "# Create GRAPHRAG DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890de0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\src\\tools\\micro_process.py:217: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  repo_tree_string = generate_repo_tree(repo_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: deduplication\\bloom_filter.py\n",
      "Processing file: deduplication\\dedup.py\n",
      "Processing file: deduplication\\LSH.py\n",
      "Processing file: deduplication\\LSHForest.py\n",
      "Processing file: deduplication\\LSHImproved.py\n",
      "Processing file: deduplication\\__init__.py\n",
      "Processing file: deduplication\\__main__.py\n",
      "Processing file: utils\\use_cases.py\n",
      "Processing file: utils\\utils.py\n",
      "Processing file: utils\\visualizations.py\n",
      "Processing file: utils\\visualization_lsh.py\n",
      "Converting to graph documents...\n",
      "Adding graph documents to Neo4j...\n"
     ]
    }
   ],
   "source": [
    "from tools.micro_process import CodeAnalysisTools\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm_graph = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "gen_graphdb = CodeAnalysisTools(llm=llm, llm_graph=llm_graph)\n",
    "graph, graph_documents = gen_graphdb.analyze_codebase(repo_path='../docs/notebooks/assignment-2-mcdonald-s/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f080718",
   "metadata": {},
   "source": [
    "# Visualize Entire Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93ef301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved as semantic_graph.html\n"
     ]
    }
   ],
   "source": [
    "a = gen_graphdb.create_visualization(graph_documents = graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474ba62",
   "metadata": {},
   "source": [
    "# CREATE VECTOR DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d91d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.micro_text_answer import create_rag_system\n",
    "\n",
    "db_setup, query_engine = create_rag_system(\n",
    "    repo_path=\"../docs/notebooks/assignment-2-mcdonald-s/src\",\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    llm_model=\"claude-3-5-sonnet-20240620\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb0541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef7fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7e597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a86cee",
   "metadata": {},
   "source": [
    "# Cypher Query to generate initial Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4740742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\docs\\notebooks\n",
      "c:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('c:\\\\Users\\\\JaeHoBahng\\\\Desktop\\\\Georgetown\\\\2025_Spring\\\\DSAN_6725\\\\project\\\\spring-2025-final-project-project-group-2\\\\src')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5241cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "from tools.micro_cypher_chain import CypherGraphBuilder\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "url = os.getenv('NEO4J_URI')\n",
    "username = os.getenv('NEO4J_USERNAME')\n",
    "password = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "graph_db = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    enhanced_schema=True\n",
    ")\n",
    "\n",
    "builder = CypherGraphBuilder(llm=llm, graph_db=graph_db)\n",
    "cypher_graph = builder.create_cypher_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3037437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cypher_graph.invoke({\"question\": \"How is the Deduplication.LSH module related to the Deduplicatioin.__Main__ module?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99024b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'There are no connections between the Deduplication.LSH module and the Deduplication.__Main__ module in the database results you provided.',\n",
       " 'steps': ['generate_cypher',\n",
       "  'validate_cypher',\n",
       "  'correct_cypher',\n",
       "  'validate_cypher',\n",
       "  'generate_final_answer'],\n",
       " 'cypher_statement': \"MATCH path = (m1:Module {id: 'Deduplication.Lsh.Lsh'})-[*..5]-(m2:Module {id: 'Deduplication.Utils.Utils'}) WHERE NONE(n IN nodes(path) WHERE n:Package) RETURN path\",\n",
       " 'cypher_results': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44287b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.getcwd())\n",
    "# os.chdir('c:\\\\Users\\\\JaeHoBahng\\\\Desktop\\\\Georgetown\\\\2025_Spring\\\\DSAN_6725\\\\project\\\\spring-2025-final-project-project-group-2\\\\src')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# from tools.micro_tools import (retrieve_cypher_relationships)\n",
    "\n",
    "# cypher_generator = retrieve_cypher_relationships(\"How is the Utils.Utils module related to the Deduplicatioin.__Main__ module?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21060bcb",
   "metadata": {},
   "source": [
    "# Text Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd37ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context and the sentence to analyze, I can add relevant details to make the statement more informative and useful:\n",
      "\n",
      "The Utils.Utils module is intricately related to the Deduplication.__Main__ module through several intermediate connections, reflecting a well-structured framework that enhances document deduplication capabilities. This relationship is crucial for the efficient implementation of Locality Sensitive Hashing (LSH) techniques in document deduplication. Here are the key points of this relationship, with added details:\n",
      "\n",
      "1. **Function Utilization**: The Utils.Utils module provides essential text processing and hashing functions that are fundamental to the deduplication process. It includes `clean_document()` for text normalization, `shingle()` for generating k-shingles, and `minhash()` for creating document signatures. These functions are utilized by the `Compute_Minhash_Signatures` function from Deduplication.Lsh.Lsh, which is crucial for generating MinHash signatures for documents. This computational function is essential for the efficiency of the deduplication process, as it allows for quick similarity comparisons between documents based on their hashed representations.\n",
      "\n",
      "2. **Class Interdependency**: The Deduplication.Lsh.Lsh module invokes the `Model` class from Deduplication.__Main__. This class serves as the backbone of the deduplication system, encapsulating core functionalities and configurations necessary for implementing various LSH-based methods effectively. The `Model` class likely uses the utility functions from Utils.Utils to preprocess documents and compute MinHash signatures, which are then used in the LSH algorithm for efficient similarity detection.\n",
      "\n",
      "3. **Definition of Key Classes**: The Deduplication.__Main__ module independently defines both the `Model` class and the `__Main__` class. The `Model` class encompasses methods for executing the deduplication algorithms and managing the data structures involved, while the `__Main__` class is responsible for orchestrating the overall execution flow of the deduplication process. This design fosters a clear separation of concerns, allowing more straightforward debugging and enhancement in future iterations. The `__Main__` class likely handles command-line argument parsing, logging configuration, and performance measurement, as indicated in the context.\n",
      "\n",
      "4. **Indirect Connections through Variants**: Additional relationships exist through other modules like `Lshimproved.Lsh` and `Lshforest.Lshforest`, which are specifically imported by Deduplication.__Main__. These imports signify that the main deduplication script leverages advancements in LSH techniques, such as improved processes for handling larger datasets and enhanced accuracy in identifying duplicate documents. The `LSHImproved` class, for instance, implements Multi-Probe Locality Sensitive Hashing with MinHash and Banding techniques, which likely rely on the utility functions provided by Utils.Utils for text processing and hashing.\n",
      "\n",
      "5. **Data Structures and Algorithms**: The Utils.Utils module provides the `UnionFind` data structure, which is particularly useful for efficiently grouping related items in the deduplication process. This data structure is likely utilized within the LSH implementation to manage clusters of similar documents.\n",
      "\n",
      "6. **Performance Optimization**: The relationship between Utils.Utils and Deduplication.__Main__ is designed with performance in mind. The utility functions for text cleaning, shingle generation, and MinHash computation are optimized for efficiency, which is crucial when processing large volumes of documents. The main deduplication script leverages these optimizations and implements parallel processing for MinHash signature computation, further enhancing the system's performance.\n",
      "\n",
      "In summary, the Utils.Utils module provides foundational text processing, hashing, and data structure implementations that are extensively used throughout the deduplication system. Its connection to Deduplication.__Main__ through shared components and dependencies within the overall Deduplication system enables efficient and scalable document deduplication. The architecture aligns with best practices in software development by promoting modularity and reusability, enabling future enhancements like the implementation of new LSH-based algorithms without extensive modifications to the existing structure. This design not only streamlines the deduplication process but also positions the system for potential application in various scenarios, including plagiarism detection and content clustering, while maintaining high performance and scalability.\n"
     ]
    }
   ],
   "source": [
    "questions = \"\"\"\n",
    "The Utils.Utils module is intricately related to the Deduplication.__Main__ module through several intermediate connections, reflecting a well-structured framework that enhances document deduplication capabilities. Here are the key points of this relationship:\n",
    "\n",
    "1. **Function Utilization**: The Utils.Utils module utilizes the `Compute_Minhash_Signatures` function from Deduplication.Lsh.Lsh, which plays a crucial role in generating MinHash signatures for documents. This computational function is essential for the efficiency of the deduplication process, as it allows for quick similarity comparisons between documents based on their hashed representations.\n",
    "\n",
    "2. **Class Interdependency**: The Deduplication.Lsh.Lsh module invokes the `Model` class from Deduplication.__Main__. This class serves as the backbone of the deduplication system, encapsulating core functionalities and configurations necessary for implementing various LSH-based methods effectively. The relationship emphasizes a modular design where the Lsh module extends the capabilities defined in the __Main__ module, facilitating code maintainability and scalability.\n",
    "\n",
    "3. **Definition of Key Classes**: The Deduplication.__Main__ module independently defines both the `Model` class and the `__Main__` class. The `Model` class encompasses methods for executing the deduplication algorithms and managing the data structures involved, while the `__Main__` class is responsible for orchestrating the overall execution flow of the deduplication process. This design fosters a clear separation of concerns, allowing more straightforward debugging and enhancement in future iterations.\n",
    "\n",
    "4. **Indirect Connections through Variants**: Additional relationships exist through other modules like `Lshimproved.Lsh` and `Lshforest.Lshforest`, which are specifically imported by Deduplication.__Main__. These imports signify that the main deduplication script leverages advancements in LSH techniques, such as improved processes for handling larger datasets and enhanced accuracy in identifying duplicate documents.\n",
    "\n",
    "In summary, the Utils.Utils module is primarily connected to Deduplication.__Main__ through shared components and dependencies within the overall Deduplication system. These connections enhance the utility and efficiency of the deduplication process, particularly through the Lsh module and its associated classes and functions. The architecture aligns with best practices in software development by promoting modularity, enabling future enhancements like the implementation of new LSH-based algorithms without extensive modifications to the existing structure. This design not only streamlines the deduplication process but also positions the system for potential application in various scenarios, including plagiarism detection and content clustering.\n",
    "\"\"\"\n",
    "# Query the system\n",
    "answer = query_engine.query(questions)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7cc3a",
   "metadata": {},
   "source": [
    "# Visualization Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2d59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b93d747",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82a65e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
