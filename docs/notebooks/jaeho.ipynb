{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def plot_network(\n",
    "#     G,\n",
    "#     node_size=1000,\n",
    "#     node_color=\"skyblue\",\n",
    "#     edge_color=\"gray\",\n",
    "#     font_size=10,\n",
    "#     title=\"Network Graph\",\n",
    "#     figsize=(12, 8),\n",
    "#     with_labels=True,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"husl\",\n",
    "#     k=0.1,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plot a network graph with seaborn-style aesthetics.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     G : networkx.Graph\n",
    "#         The network graph to visualize\n",
    "#     node_size : int or list\n",
    "#         Size of nodes (can be a single value or list for different sizes)\n",
    "#     node_color : str or list\n",
    "#         Color of nodes (can be a single value or list for different colors)\n",
    "#     edge_color : str\n",
    "#         Color of edges\n",
    "#     font_size : int\n",
    "#         Size of node labels\n",
    "#     title : str\n",
    "#         Title of the plot\n",
    "#     figsize : tuple\n",
    "#         Figure size (width, height)\n",
    "#     with_labels : bool\n",
    "#         Whether to show node labels\n",
    "#     layout : str\n",
    "#         Type of layout ('spring', 'circular', 'random', 'shell')\n",
    "#     palette : str\n",
    "#         Seaborn color palette to use if node_color is not specified\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     fig, ax : tuple\n",
    "#         Matplotlib figure and axis objects\n",
    "#     \"\"\"\n",
    "#     # Set the style\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "\n",
    "#     # Create figure\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "#     # Choose layout\n",
    "#     layouts = {\n",
    "#         \"spring\": nx.spring_layout,\n",
    "#         \"circular\": nx.circular_layout,\n",
    "#         \"random\": nx.random_layout,\n",
    "#         \"shell\": nx.shell_layout,\n",
    "#     }\n",
    "#     pos = layouts.get(layout, nx.spring_layout)(G,k)\n",
    "\n",
    "#     # If node_color is not specified, use seaborn palette\n",
    "#     if isinstance(node_color, str) and node_color == \"skyblue\":\n",
    "#         colors = sns.color_palette(palette, n_colors=len(G.nodes()))\n",
    "#     else:\n",
    "#         colors = node_color\n",
    "\n",
    "#     # Draw the network\n",
    "#     nx.draw(\n",
    "#         G,\n",
    "#         pos,\n",
    "#         node_color=colors,\n",
    "#         node_size=node_size,\n",
    "#         edge_color=edge_color,\n",
    "#         with_labels=with_labels,\n",
    "#         font_size=font_size,\n",
    "#         font_weight=\"bold\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "\n",
    "#     # Add title\n",
    "#     plt.title(title, fontsize=font_size + 4, pad=20)\n",
    "\n",
    "#     return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: USING langchain_experimental.graph_transformers main.\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# # TypedDict\n",
    "# class Json(TypedDict):\n",
    "#     \"\"\"Json to return.\"\"\"\n",
    "\n",
    "#     setup: Annotated[dict, ..., \"The setup of the dict\"]\n",
    "#     depth: Annotated[int, ..., \"How many layers deep the dict is.\"]\n",
    "\n",
    "\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# text = \"\"\"\n",
    "# Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "# She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "# Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "# She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "# \"\"\"\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "# for i, doc in enumerate(graph_documents):\n",
    "#     logger.info(f\"document #{i+1}\")\n",
    "#     nodes = doc.nodes\n",
    "#     relationships = doc.relationships\n",
    "#     for n in nodes:\n",
    "#         logger.info(f\"Nodes:{n}\")\n",
    "#     for r in relationships:\n",
    "#         logger.info(f\"Relationships:{r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_graph import plot_network\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# nodes = [str(node) for node in graph_documents[0].nodes]\n",
    "# relationships = [\n",
    "#     (str(rel.source), str(rel.target)) for rel in graph_documents[0].relationships\n",
    "# ]\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_edges_from(relationships)\n",
    "\n",
    "# custom_colors = sns.color_palette(\"Set2\", n_colors=len(G.nodes()))\n",
    "# node_sizes = [3000 if d > 5 else 1000 for v, d in G.degree()]\n",
    "\n",
    "# fig, ax = plot_network(\n",
    "#     G,\n",
    "#     node_size=node_sizes,\n",
    "#     node_color=custom_colors,\n",
    "#     edge_color=\"#cccccc\",\n",
    "#     font_size=12,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"Set2\",\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with sample langchain code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:21:48,362] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code begins with the definition of an agent designed to hold a conversation while utilizing various tools. The first two lines import specific functionalities from the standard library and third-party libraries. The `__future__` import statement allows the code to use features from future versions of Python, in this case, type annotations. The `typing` module imports several types such as `Any`, `List`, `Optional`, and `Sequence`, which enable type hinting for better clarity and code safety.\n",
      "\n",
      "The `langchain_core` library imports several components: `deprecated` marks functions or classes as outdated, `BaseCallbackManager` manages callback functionality, `BaseLanguageModel` represents the base structure for a language model, `PromptTemplate` helps in creating structured prompts, and `BaseTool` serves as a base class for defining tools. The `pydantic` library imports `Field`, a function used to create data model fields with specific configurations. \n",
      "\n",
      "The `AGENT_DEPRECATION_WARNING` imports a deprecation warning message, while `Agent` and `AgentOutputParser` from `langchain.agents.agent` are fundamental classes for creating agents and parsing their output. Additionally, `AgentType` represents different agent types, `ConvoOutputParser` parses the output from conversational agents, and `FORMAT_INSTRUCTIONS`, `PREFIX`, and `SUFFIX` provide formatting details for prompts. The utility function `validate_tools_single_input` validates tools for singular input requirements, and `LLMChain` creates a chain for interacting with language models.\n",
      "\n",
      "The `ConversationalAgent` class inherits from the `Agent` class and is decorated with the `@deprecated` decorator, indicating that it will be removed in version 1.0. The class initializes two attributes: `ai_prefix`, set to \"AI\", which precedes the output from the AI, and `output_parser`, which instantiates a `ConvoOutputParser` as a default output parser for this specific agent.\n",
      "\n",
      "The class contains a class method `_get_default_output_parser`, which requires an optional `ai_prefix` argument with a default value of \"AI\". It creates an instance of `ConvoOutputParser` using this `ai_prefix`. The class property `_agent_type` returns a fixed identifier for the agent type indicating this is a conversational reactive agent.\n",
      "\n",
      "The property `observation_prefix` returns the string \"Observation: \", which is used to format the observations passed to the agent. The property `llm_prefix` returns \"Thought:\", a prefix utilized when input is generated for the language model. \n",
      "\n",
      "The class method `create_prompt` is designed to assemble prompts using provided tools. The method has several parameters: `tools`, which lists the tools the agent can use; `prefix` and `suffix`, which wrap the prompt; `format_instructions`, which dictate how the tools are to be used; `ai_prefix`, a string for AI output; `human_prefix`, a string for human input; and `input_variables`, which defaults to a predefined list of variables. Inside the method, `tool_strings` and `tool_names` are generated to create descriptive text for the available tools. These strings are used alongside the format instructions and other components to form a complete template for the prompt, which is returned as a `PromptTemplate`.\n",
      "\n",
      "The class method `_validate_tools` calls the superclass's validation method and also validates that all tools meet the criteria for single input using `validate_tools_single_input`, ensuring that the tools are correctly implemented for use by the agent.\n",
      "\n",
      "The class method `from_llm_and_tools` constructs a new instance of the `ConversationalAgent` using a specified `llm`, a sequence of `tools`, an optional `callback_manager`, and other optional parameters like `output_parser`, `prefix`, `suffix`, `format_instructions`, `ai_prefix`, `human_prefix`, and `input_variables`. The method starts by validating the tools, then calls the `create_prompt` method to generate the prompt based on the provided parameters. Next, an `LLMChain` instance is created using the language model and the prompt. The `tool_names` is created to list the names of the tools, and `_output_parser` is determined, defaulting to the agent's default parser if not specified. Finally, an instance of the `ConversationalAgent` is constructed with the `llm_chain`, allowed tools, AI prefix, the determined output parser, and other optional parameters.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        # \"Return a json that describes the logic flow of the given codebase. Only track call stacks. Output in Cypher compliant format for graph representation.\"\n",
    "        \"\"\"\n",
    "        Return the given code as natural language description with great detail.\n",
    "        Explain all the functions, parameters, and the logic flow of the code.\n",
    "        EXPLAIN EVERY SINGLE LINE OF THE CODE.\n",
    "        Do not use adjectives and adverbs. Only describe the code and do not give an overall summary.\n",
    "        Also include packages that were imported and how they were used in the code.\n",
    "        Do not give me bullet points, and output a paragraph format.\n",
    "        Do not use ambiguous pronouns and use exact names in every description.\n",
    "        Explain each class, function, and variable separately and do not include explanations such as 'as mentioned before' or anything that refers to a previous explanation.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        '''\n",
    "        # An agent designed to hold a conversation in addition to using tools.\n",
    "\n",
    "        from __future__ import annotations\n",
    "\n",
    "        from typing import Any, List, Optional, Sequence\n",
    "\n",
    "        from langchain_core._api import deprecated\n",
    "        from langchain_core.callbacks import BaseCallbackManager\n",
    "        from langchain_core.language_models import BaseLanguageModel\n",
    "        from langchain_core.prompts import PromptTemplate\n",
    "        from langchain_core.tools import BaseTool\n",
    "        from pydantic import Field\n",
    "\n",
    "        from langchain._api.deprecation import AGENT_DEPRECATION_WARNING\n",
    "        from langchain.agents.agent import Agent, AgentOutputParser\n",
    "        from langchain.agents.agent_types import AgentType\n",
    "        from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "        from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n",
    "        from langchain.agents.utils import validate_tools_single_input\n",
    "        from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "        @deprecated(\n",
    "            \"0.1.0\",\n",
    "            message=AGENT_DEPRECATION_WARNING,\n",
    "            removal=\"1.0\",\n",
    "        )\n",
    "        class ConversationalAgent(Agent):\n",
    "            \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\n",
    "\n",
    "            ai_prefix: str = \"AI\"\n",
    "            \"\"\"Prefix to use before AI output.\"\"\"\n",
    "            output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\n",
    "            \"\"\"Output parser for the agent.\"\"\"\n",
    "\n",
    "            @classmethod\n",
    "            def _get_default_output_parser(\n",
    "                cls, ai_prefix: str = \"AI\", **kwargs: Any\n",
    "            ) -> AgentOutputParser:\n",
    "                return ConvoOutputParser(ai_prefix=ai_prefix)\n",
    "\n",
    "            @property\n",
    "            def _agent_type(self) -> str:\n",
    "                \"\"\"Return Identifier of agent type.\"\"\"\n",
    "                return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n",
    "\n",
    "            @property\n",
    "            def observation_prefix(self) -> str:\n",
    "                \"\"\"Prefix to append the observation with.\n",
    "\n",
    "                Returns:\n",
    "                    \"Observation: \"\n",
    "                \"\"\"\n",
    "                return \"Observation: \"\n",
    "\n",
    "            @property\n",
    "            def llm_prefix(self) -> str:\n",
    "                \"\"\"Prefix to append the llm call with.\n",
    "\n",
    "                Returns:\n",
    "                    \"Thought: \"\n",
    "                \"\"\"\n",
    "                return \"Thought:\"\n",
    "\n",
    "            @classmethod\n",
    "            def create_prompt(\n",
    "                cls,\n",
    "                tools: Sequence[BaseTool],\n",
    "                prefix: str = PREFIX,\n",
    "                suffix: str = SUFFIX,\n",
    "                format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "                ai_prefix: str = \"AI\",\n",
    "                human_prefix: str = \"Human\",\n",
    "                input_variables: Optional[List[str]] = None,\n",
    "            ) -> PromptTemplate:\n",
    "                \"\"\"Create prompt in the style of the zero-shot agent.\n",
    "\n",
    "                Args:\n",
    "                    tools: List of tools the agent will have access to, used to format the\n",
    "                        prompt.\n",
    "                    prefix: String to put before the list of tools. Defaults to PREFIX.\n",
    "                    suffix: String to put after the list of tools. Defaults to SUFFIX.\n",
    "                    format_instructions: Instructions on how to use the tools. Defaults to\n",
    "                        FORMAT_INSTRUCTIONS\n",
    "                    ai_prefix: String to use before AI output. Defaults to \"AI\".\n",
    "                    human_prefix: String to use before human output.\n",
    "                        Defaults to \"Human\".\n",
    "                    input_variables: List of input variables the final prompt will expect.\n",
    "                        Defaults to [\"input\", \"chat_history\", \"agent_scratchpad\"].\n",
    "\n",
    "                Returns:\n",
    "                    A PromptTemplate with the template assembled from the pieces here.\n",
    "                \"\"\"\n",
    "                tool_strings = \"\\n\".join(\n",
    "                    [f\"> {tool.name}: {tool.description}\" for tool in tools]\n",
    "                )\n",
    "                tool_names = \", \".join([tool.name for tool in tools])\n",
    "                format_instructions = format_instructions.format(\n",
    "                    tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\n",
    "                )\n",
    "                template = \"\\n\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "                if input_variables is None:\n",
    "                    input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
    "                return PromptTemplate(template=template, input_variables=input_variables)\n",
    "\n",
    "            @classmethod\n",
    "            def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "                super()._validate_tools(tools)\n",
    "                validate_tools_single_input(cls.__name__, tools)\n",
    "\n",
    "            @classmethod\n",
    "            def from_llm_and_tools(\n",
    "                cls,\n",
    "                llm: BaseLanguageModel,\n",
    "                tools: Sequence[BaseTool],\n",
    "                callback_manager: Optional[BaseCallbackManager] = None,\n",
    "                output_parser: Optional[AgentOutputParser] = None,\n",
    "                prefix: str = PREFIX,\n",
    "                suffix: str = SUFFIX,\n",
    "                format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "                ai_prefix: str = \"AI\",\n",
    "                human_prefix: str = \"Human\",\n",
    "                input_variables: Optional[List[str]] = None,\n",
    "                **kwargs: Any,\n",
    "            ) -> Agent:\n",
    "                \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "                Args:\n",
    "                    llm: The language model to use.\n",
    "                    tools: A list of tools to use.\n",
    "                    callback_manager: The callback manager to use. Default is None.\n",
    "                    output_parser: The output parser to use. Default is None.\n",
    "                    prefix: The prefix to use in the prompt. Default is PREFIX.\n",
    "                    suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
    "                    format_instructions: The format instructions to use.\n",
    "                        Default is FORMAT_INSTRUCTIONS.\n",
    "                    ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
    "                    human_prefix: The prefix to use before human output.\n",
    "                        Default is \"Human\".\n",
    "                    input_variables: The input variables to use. Default is None.\n",
    "                    **kwargs: Any additional keyword arguments to pass to the agent.\n",
    "\n",
    "                Returns:\n",
    "                    An agent.\n",
    "                \"\"\"\n",
    "                cls._validate_tools(tools)\n",
    "                prompt = cls.create_prompt(\n",
    "                    tools,\n",
    "                    ai_prefix=ai_prefix,\n",
    "                    human_prefix=human_prefix,\n",
    "                    prefix=prefix,\n",
    "                    suffix=suffix,\n",
    "                    format_instructions=format_instructions,\n",
    "                    input_variables=input_variables,\n",
    "                )\n",
    "                llm_chain = LLMChain(  # type: ignore[misc]\n",
    "                    llm=llm,\n",
    "                    prompt=prompt,\n",
    "                    callback_manager=callback_manager,\n",
    "                )\n",
    "                tool_names = [tool.name for tool in tools]\n",
    "                _output_parser = output_parser or cls._get_default_output_parser(\n",
    "                    ai_prefix=ai_prefix\n",
    "                )\n",
    "                return cls(\n",
    "                    llm_chain=llm_chain,\n",
    "                    allowed_tools=tool_names,\n",
    "                    ai_prefix=ai_prefix,\n",
    "                    output_parser=_output_parser,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            '''\n",
    "    ),\n",
    "]\n",
    "\n",
    "# structured_llm = llm.with_structured_output(Json)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([response.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:00,978] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:08,679] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:12,179] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:20,678] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:30,410] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = response.content\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# # logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:30,449] p16596 {406922825.py:2} INFO - document #1\n",
      "[2025-03-15 12:22:30,449] p16596 {406922825.py:6} INFO - Nodes:id='Agent' type='Concept' properties={}\n",
      "[2025-03-15 12:22:30,449] p16596 {406922825.py:6} INFO - Nodes:id='Standard Library' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,449] p16596 {406922825.py:6} INFO - Nodes:id='Third-Party Libraries' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,459] p16596 {406922825.py:6} INFO - Nodes:id='__Future__' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,460] p16596 {406922825.py:6} INFO - Nodes:id='Typing' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,461] p16596 {406922825.py:6} INFO - Nodes:id='Langchain_Core' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,462] p16596 {406922825.py:6} INFO - Nodes:id='Pydantic' type='Library' properties={}\n",
      "[2025-03-15 12:22:30,463] p16596 {406922825.py:6} INFO - Nodes:id='Any' type='Type' properties={}\n",
      "[2025-03-15 12:22:30,464] p16596 {406922825.py:6} INFO - Nodes:id='List' type='Type' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Optional' type='Type' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Sequence' type='Type' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Deprecated' type='Component' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Basecallbackmanager' type='Component' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Baselanguagemodel' type='Component' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Prompttemplate' type='Component' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Basetool' type='Component' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:6} INFO - Nodes:id='Field' type='Function' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Concept', properties={}) target=Node(id='Standard Library', type='Library', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Concept', properties={}) target=Node(id='Third-Party Libraries', type='Library', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='__Future__', type='Library', properties={}) target=Node(id='Agent', type='Concept', properties={}) type='ENABLES' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Library', properties={}) target=Node(id='Agent', type='Concept', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Library', properties={}) target=Node(id='Any', type='Type', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Library', properties={}) target=Node(id='List', type='Type', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Library', properties={}) target=Node(id='Optional', type='Type', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Library', properties={}) target=Node(id='Sequence', type='Type', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Library', properties={}) target=Node(id='Deprecated', type='Component', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Library', properties={}) target=Node(id='Basecallbackmanager', type='Component', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Library', properties={}) target=Node(id='Baselanguagemodel', type='Component', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Library', properties={}) target=Node(id='Prompttemplate', type='Component', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Library', properties={}) target=Node(id='Basetool', type='Component', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Pydantic', type='Library', properties={}) target=Node(id='Field', type='Function', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,465] p16596 {406922825.py:2} INFO - document #2\n",
      "[2025-03-15 12:22:30,475] p16596 {406922825.py:6} INFO - Nodes:id='Agent_Deprecation_Warning' type='Message' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Agent' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Agentoutputparser' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Agenttype' type='Type' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Convooutputparser' type='Parser' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Format_Instructions' type='Instruction' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Prefix' type='Detail' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Suffix' type='Detail' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Validate_Tools_Single_Input' type='Function' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Llmchain' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='@Deprecated' type='Decorator' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Attribute' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Output_Parser' type='Attribute' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent_Deprecation_Warning', type='Message', properties={}) target=Node(id='Deprecation Warning Message', type='Concept', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Class', properties={}) target=Node(id='Agentoutputparser', type='Class', properties={}) type='RELATED_TO' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Class', properties={}) target=Node(id='Agenttype', type='Type', properties={}) type='RELATED_TO' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Class', properties={}) target=Node(id='Convooutputparser', type='Parser', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Agent', type='Class', properties={}) target=Node(id='Validate_Tools_Single_Input', type='Function', properties={}) type='VALIDATES' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Llmchain', type='Class', properties={}) target=Node(id='Agent', type='Class', properties={}) type='INTERACTS_WITH' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Agent', type='Class', properties={}) type='INHERITS_FROM' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='@Deprecated', type='Decorator', properties={}) type='DECORATED_WITH' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Ai_Prefix', type='Attribute', properties={}) type='HAS_ATTRIBUTE' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Output_Parser', type='Attribute', properties={}) type='HAS_ATTRIBUTE' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Output_Parser', type='Attribute', properties={}) target=Node(id='Convooutputparser', type='Parser', properties={}) type='INSTANTIATES' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:2} INFO - document #3\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='_Get_Default_Output_Parser' type='Method' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Argument' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Convooutputparser' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='_Agent_Type' type='Property' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Observation_Prefix' type='Property' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Llm_Prefix' type='Property' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:6} INFO - Nodes:id='Conversational_Reactive_Agent' type='Agent type' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}) target=Node(id='Ai_Prefix', type='Argument', properties={}) type='REQUIRES' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}) target=Node(id='Convooutputparser', type='Class', properties={}) type='CREATES_INSTANCE' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Agent_Type', type='Property', properties={}) target=Node(id='Conversational_Reactive_Agent', type='Agent type', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-15 12:22:30,476] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Observation_Prefix', type='Property', properties={}) target=Node(id='Observation: ', type='String', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Llm_Prefix', type='Property', properties={}) target=Node(id='Thought:', type='String', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:2} INFO - document #4\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Create_Prompt' type='Method' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='_Validate_Tools' type='Method' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Tools' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Suffix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Format_Instructions' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Human_Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Input_Variables' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Tool_Strings' type='Variable' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Tool_Names' type='Variable' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Prompttemplate' type='Object' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Tools', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Prefix', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Suffix', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Format_Instructions', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Ai_Prefix', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Human_Prefix', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Input_Variables', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Tool_Strings', type='Variable', properties={}) type='GENERATES' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Tool_Names', type='Variable', properties={}) type='GENERATES' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Prompttemplate', type='Object', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Validate_Tools', type='Method', properties={}) target=Node(id='Tools', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Validate_Tools', type='Method', properties={}) target=Node(id='Validate_Tools_Single_Input', type='Method', properties={}) type='CALLS' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='_Validate_Tools', type='Method', properties={}) target=Node(id='Tools', type='Parameter', properties={}) type='VALIDATES' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:2} INFO - document #5\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='From_Llm_And_Tools' type='Method' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Llm' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Tools' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Callback_Manager' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Output_Parser' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Suffix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,492] p16596 {406922825.py:6} INFO - Nodes:id='Format_Instructions' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,508] p16596 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,508] p16596 {406922825.py:6} INFO - Nodes:id='Human_Prefix' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,509] p16596 {406922825.py:6} INFO - Nodes:id='Input_Variables' type='Parameter' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:6} INFO - Nodes:id='Create_Prompt' type='Method' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:6} INFO - Nodes:id='Llmchain' type='Class' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:6} INFO - Nodes:id='Tool_Names' type='Variable' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:6} INFO - Nodes:id='_Output_Parser' type='Variable' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Conversationalagent', type='Class', properties={}) type='CONSTRUCTS' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Llm', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Tools', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Callback_Manager', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Output_Parser', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Prefix', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Suffix', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Format_Instructions', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Ai_Prefix', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Human_Prefix', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Input_Variables', type='Parameter', properties={}) type='USES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Create_Prompt', type='Method', properties={}) type='CALLS' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Llmchain', type='Class', properties={}) type='CREATES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Tool_Names', type='Variable', properties={}) type='CREATES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='_Output_Parser', type='Variable', properties={}) type='DETERMINES' properties={}\n",
      "[2025-03-15 12:22:30,510] p16596 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Conversationalagent', type='Class', properties={}) type='CONSTRUCTS' properties={}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(graph_documents):\n",
    "    logger.info(f\"document #{i+1}\")\n",
    "    nodes = doc.nodes\n",
    "    relationships = doc.relationships\n",
    "    for n in nodes:\n",
    "        logger.info(f\"Nodes:{n}\")\n",
    "    for r in relationships:\n",
    "        logger.info(f\"Relationships:{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[Node(id='Agent', type='Concept', properties={}), Node(id='Standard Library', type='Library', properties={}), Node(id='Third-Party Libraries', type='Library', properties={}), Node(id='__Future__', type='Library', properties={}), Node(id='Typing', type='Library', properties={}), Node(id='Langchain_Core', type='Library', properties={}), Node(id='Pydantic', type='Library', properties={}), Node(id='Any', type='Type', properties={}), Node(id='List', type='Type', properties={}), Node(id='Optional', type='Type', properties={}), Node(id='Sequence', type='Type', properties={}), Node(id='Deprecated', type='Component', properties={}), Node(id='Basecallbackmanager', type='Component', properties={}), Node(id='Baselanguagemodel', type='Component', properties={}), Node(id='Prompttemplate', type='Component', properties={}), Node(id='Basetool', type='Component', properties={}), Node(id='Field', type='Function', properties={})], relationships=[Relationship(source=Node(id='Agent', type='Concept', properties={}), target=Node(id='Standard Library', type='Library', properties={}), type='USES', properties={}), Relationship(source=Node(id='Agent', type='Concept', properties={}), target=Node(id='Third-Party Libraries', type='Library', properties={}), type='USES', properties={}), Relationship(source=Node(id='__Future__', type='Library', properties={}), target=Node(id='Agent', type='Concept', properties={}), type='ENABLES', properties={}), Relationship(source=Node(id='Typing', type='Library', properties={}), target=Node(id='Agent', type='Concept', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Library', properties={}), target=Node(id='Any', type='Type', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Library', properties={}), target=Node(id='List', type='Type', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Library', properties={}), target=Node(id='Optional', type='Type', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Library', properties={}), target=Node(id='Sequence', type='Type', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Library', properties={}), target=Node(id='Deprecated', type='Component', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Library', properties={}), target=Node(id='Basecallbackmanager', type='Component', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Library', properties={}), target=Node(id='Baselanguagemodel', type='Component', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Library', properties={}), target=Node(id='Prompttemplate', type='Component', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Library', properties={}), target=Node(id='Basetool', type='Component', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Pydantic', type='Library', properties={}), target=Node(id='Field', type='Function', properties={}), type='IMPORTS', properties={})], source=Document(metadata={}, page_content='The code begins with the definition of an agent designed to hold a conversation while utilizing various tools. The first two lines import specific functionalities from the standard library and third-party libraries. The `__future__` import statement allows the code to use features from future versions of Python, in this case, type annotations. The `typing` module imports several types such as `Any`, `List`, `Optional`, and `Sequence`, which enable type hinting for better clarity and code safety.\\n\\nThe `langchain_core` library imports several components: `deprecated` marks functions or classes as outdated, `BaseCallbackManager` manages callback functionality, `BaseLanguageModel` represents the base structure for a language model, `PromptTemplate` helps in creating structured prompts, and `BaseTool` serves as a base class for defining tools. The `pydantic` library imports `Field`, a function used to create data model fields with specific configurations.')),\n",
       " GraphDocument(nodes=[Node(id='Agent_Deprecation_Warning', type='Message', properties={}), Node(id='Agent', type='Class', properties={}), Node(id='Agentoutputparser', type='Class', properties={}), Node(id='Agenttype', type='Type', properties={}), Node(id='Convooutputparser', type='Parser', properties={}), Node(id='Format_Instructions', type='Instruction', properties={}), Node(id='Prefix', type='Detail', properties={}), Node(id='Suffix', type='Detail', properties={}), Node(id='Validate_Tools_Single_Input', type='Function', properties={}), Node(id='Llmchain', type='Class', properties={}), Node(id='Conversationalagent', type='Class', properties={}), Node(id='@Deprecated', type='Decorator', properties={}), Node(id='Ai_Prefix', type='Attribute', properties={}), Node(id='Output_Parser', type='Attribute', properties={})], relationships=[Relationship(source=Node(id='Agent_Deprecation_Warning', type='Message', properties={}), target=Node(id='Deprecation Warning Message', type='Concept', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Agent', type='Class', properties={}), target=Node(id='Agentoutputparser', type='Class', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Agent', type='Class', properties={}), target=Node(id='Agenttype', type='Type', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Agent', type='Class', properties={}), target=Node(id='Convooutputparser', type='Parser', properties={}), type='USES', properties={}), Relationship(source=Node(id='Agent', type='Class', properties={}), target=Node(id='Validate_Tools_Single_Input', type='Function', properties={}), type='VALIDATES', properties={}), Relationship(source=Node(id='Llmchain', type='Class', properties={}), target=Node(id='Agent', type='Class', properties={}), type='INTERACTS_WITH', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Agent', type='Class', properties={}), type='INHERITS_FROM', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='@Deprecated', type='Decorator', properties={}), type='DECORATED_WITH', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Ai_Prefix', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Output_Parser', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Output_Parser', type='Attribute', properties={}), target=Node(id='Convooutputparser', type='Parser', properties={}), type='INSTANTIATES', properties={})], source=Document(metadata={}, page_content='The `AGENT_DEPRECATION_WARNING` imports a deprecation warning message, while `Agent` and `AgentOutputParser` from `langchain.agents.agent` are fundamental classes for creating agents and parsing their output. Additionally, `AgentType` represents different agent types, `ConvoOutputParser` parses the output from conversational agents, and `FORMAT_INSTRUCTIONS`, `PREFIX`, and `SUFFIX` provide formatting details for prompts. The utility function `validate_tools_single_input` validates tools for singular input requirements, and `LLMChain` creates a chain for interacting with language models.\\n\\nThe `ConversationalAgent` class inherits from the `Agent` class and is decorated with the `@deprecated` decorator, indicating that it will be removed in version 1.0. The class initializes two attributes: `ai_prefix`, set to \"AI\", which precedes the output from the AI, and `output_parser`, which instantiates a `ConvoOutputParser` as a default output parser for this specific agent.')),\n",
       " GraphDocument(nodes=[Node(id='_Get_Default_Output_Parser', type='Method', properties={}), Node(id='Ai_Prefix', type='Argument', properties={}), Node(id='Convooutputparser', type='Class', properties={}), Node(id='_Agent_Type', type='Property', properties={}), Node(id='Observation_Prefix', type='Property', properties={}), Node(id='Llm_Prefix', type='Property', properties={}), Node(id='Conversational_Reactive_Agent', type='Agent type', properties={})], relationships=[Relationship(source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}), target=Node(id='Ai_Prefix', type='Argument', properties={}), type='REQUIRES', properties={}), Relationship(source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}), target=Node(id='Convooutputparser', type='Class', properties={}), type='CREATES_INSTANCE', properties={}), Relationship(source=Node(id='_Agent_Type', type='Property', properties={}), target=Node(id='Conversational_Reactive_Agent', type='Agent type', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='Observation_Prefix', type='Property', properties={}), target=Node(id='Observation: ', type='String', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='Llm_Prefix', type='Property', properties={}), target=Node(id='Thought:', type='String', properties={}), type='RETURNS', properties={})], source=Document(metadata={}, page_content='The class contains a class method `_get_default_output_parser`, which requires an optional `ai_prefix` argument with a default value of \"AI\". It creates an instance of `ConvoOutputParser` using this `ai_prefix`. The class property `_agent_type` returns a fixed identifier for the agent type indicating this is a conversational reactive agent.\\n\\nThe property `observation_prefix` returns the string \"Observation: \", which is used to format the observations passed to the agent. The property `llm_prefix` returns \"Thought:\", a prefix utilized when input is generated for the language model.')),\n",
       " GraphDocument(nodes=[Node(id='Create_Prompt', type='Method', properties={}), Node(id='_Validate_Tools', type='Method', properties={}), Node(id='Tools', type='Parameter', properties={}), Node(id='Prefix', type='Parameter', properties={}), Node(id='Suffix', type='Parameter', properties={}), Node(id='Format_Instructions', type='Parameter', properties={}), Node(id='Ai_Prefix', type='Parameter', properties={}), Node(id='Human_Prefix', type='Parameter', properties={}), Node(id='Input_Variables', type='Parameter', properties={}), Node(id='Tool_Strings', type='Variable', properties={}), Node(id='Tool_Names', type='Variable', properties={}), Node(id='Prompttemplate', type='Object', properties={})], relationships=[Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Tools', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Prefix', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Suffix', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Format_Instructions', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Ai_Prefix', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Human_Prefix', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Input_Variables', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Tool_Strings', type='Variable', properties={}), type='GENERATES', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Tool_Names', type='Variable', properties={}), type='GENERATES', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Prompttemplate', type='Object', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='_Validate_Tools', type='Method', properties={}), target=Node(id='Tools', type='Parameter', properties={}), type='HAS_PARAMETER', properties={}), Relationship(source=Node(id='_Validate_Tools', type='Method', properties={}), target=Node(id='Validate_Tools_Single_Input', type='Method', properties={}), type='CALLS', properties={}), Relationship(source=Node(id='_Validate_Tools', type='Method', properties={}), target=Node(id='Tools', type='Parameter', properties={}), type='VALIDATES', properties={})], source=Document(metadata={}, page_content=\"The class method `create_prompt` is designed to assemble prompts using provided tools. The method has several parameters: `tools`, which lists the tools the agent can use; `prefix` and `suffix`, which wrap the prompt; `format_instructions`, which dictate how the tools are to be used; `ai_prefix`, a string for AI output; `human_prefix`, a string for human input; and `input_variables`, which defaults to a predefined list of variables. Inside the method, `tool_strings` and `tool_names` are generated to create descriptive text for the available tools. These strings are used alongside the format instructions and other components to form a complete template for the prompt, which is returned as a `PromptTemplate`.\\n\\nThe class method `_validate_tools` calls the superclass's validation method and also validates that all tools meet the criteria for single input using `validate_tools_single_input`, ensuring that the tools are correctly implemented for use by the agent.\")),\n",
       " GraphDocument(nodes=[Node(id='From_Llm_And_Tools', type='Method', properties={}), Node(id='Conversationalagent', type='Class', properties={}), Node(id='Llm', type='Parameter', properties={}), Node(id='Tools', type='Parameter', properties={}), Node(id='Callback_Manager', type='Parameter', properties={}), Node(id='Output_Parser', type='Parameter', properties={}), Node(id='Prefix', type='Parameter', properties={}), Node(id='Suffix', type='Parameter', properties={}), Node(id='Format_Instructions', type='Parameter', properties={}), Node(id='Ai_Prefix', type='Parameter', properties={}), Node(id='Human_Prefix', type='Parameter', properties={}), Node(id='Input_Variables', type='Parameter', properties={}), Node(id='Create_Prompt', type='Method', properties={}), Node(id='Llmchain', type='Class', properties={}), Node(id='Tool_Names', type='Variable', properties={}), Node(id='_Output_Parser', type='Variable', properties={})], relationships=[Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Conversationalagent', type='Class', properties={}), type='CONSTRUCTS', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Llm', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Tools', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Callback_Manager', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Output_Parser', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Prefix', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Suffix', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Format_Instructions', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Ai_Prefix', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Human_Prefix', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Input_Variables', type='Parameter', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Create_Prompt', type='Method', properties={}), type='CALLS', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Llmchain', type='Class', properties={}), type='CREATES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Tool_Names', type='Variable', properties={}), type='CREATES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='_Output_Parser', type='Variable', properties={}), type='DETERMINES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Conversationalagent', type='Class', properties={}), type='CONSTRUCTS', properties={})], source=Document(metadata={}, page_content=\"The class method `from_llm_and_tools` constructs a new instance of the `ConversationalAgent` using a specified `llm`, a sequence of `tools`, an optional `callback_manager`, and other optional parameters like `output_parser`, `prefix`, `suffix`, `format_instructions`, `ai_prefix`, `human_prefix`, and `input_variables`. The method starts by validating the tools, then calls the `create_prompt` method to generate the prompt based on the provided parameters. Next, an `LLMChain` instance is created using the language model and the prompt. The `tool_names` is created to list the names of the tools, and `_output_parser` is determined, defaulting to the agent's default parser if not specified. Finally, an instance of the `ConversationalAgent` is constructed with the `llm_chain`, allowed tools, AI prefix, the determined output parser, and other optional parameters.\"))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_graph import plot_network\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# nodes = [str(node) for graph in graph_documents for node in graph.nodes]\n",
    "# relationships = [(str(rel.source), str(rel.target)) for graph in graph_documents for rel in graph.relationships]\n",
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_edges_from(relationships)\n",
    "\n",
    "# custom_colors = sns.color_palette(\"Set2\", n_colors=len(G.nodes()))\n",
    "# node_sizes = [3000 if d > 5 else 1000 for v, d in G.degree()]\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plot_network(\n",
    "#     G,\n",
    "#     node_size=node_sizes,\n",
    "#     node_color=custom_colors,\n",
    "#     edge_color=\"#cccccc\",\n",
    "#     font_size=10,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"Set2\",\n",
    "#     k=0.1,\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "\n",
    "# net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# for graph in graph_documents:\n",
    "#     for rel in graph.relationships:\n",
    "#         net.add_node(str(rel.source))\n",
    "#         net.add_node(str(rel.target))\n",
    "#         net.add_edge(str(rel.source), str(rel.target))\n",
    "\n",
    "# # Save and display\n",
    "# net.save_graph(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph_documents[0].relationships[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Concept'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(a,'type',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relationship(source=Node(id='Agent', type='Concept', properties={}), target=Node(id='Standard Library', type='Library', properties={}), type='USES', properties={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents[0].relationships[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Pyvis network\n",
    "net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# Create a NetworkX graph for analysis\n",
    "G = nx.Graph()\n",
    "\n",
    "# Dictionary to store node attributes\n",
    "node_types = {}\n",
    "\n",
    "# Add nodes and edges, extracting types\n",
    "for graph in graph_documents:\n",
    "    for rel in graph.relationships:\n",
    "        source, target = str(rel.source.id), str(rel.target.id)\n",
    "        source_type = rel.source.type\n",
    "        target_type = rel.target.type\n",
    "        rel_type = rel.type  # Relationship type (e.g., 'IMPORTS')\n",
    "\n",
    "        # Store node types\n",
    "        node_types[source] = source_type\n",
    "        node_types[target] = target_type\n",
    "\n",
    "        # Add nodes if not already added\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "\n",
    "        # Add edge with label\n",
    "        G.add_edge(source, target, label=rel_type)\n",
    "\n",
    "# Get unique node types and assign colors\n",
    "unique_types = list(set(node_types.values()))\n",
    "color_map = plt.get_cmap(\"tab10\")  # Use a categorical colormap\n",
    "type_colors = {t: color_map(i / len(unique_types)) for i, t in enumerate(unique_types)}\n",
    "\n",
    "# Convert colors to RGBA format\n",
    "type_colors_rgba = {\n",
    "    t: f'rgba({int(c[0] * 255)}, {int(c[1] * 255)}, {int(c[2] * 255)}, 0.8)' for t, c in type_colors.items()\n",
    "}\n",
    "\n",
    "# Determine node sizes based on degrees\n",
    "degrees = dict(G.degree())\n",
    "min_size, max_size = 10, 50\n",
    "size_scale = {node: min_size + (max_size - min_size) * (deg / max(degrees.values())) for node, deg in degrees.items()}\n",
    "\n",
    "# Add nodes with dynamic colors and sizes\n",
    "for node in G.nodes():\n",
    "    node_type = node_types.get(node, \"default\")\n",
    "    net.add_node(\n",
    "        node,\n",
    "        label=node,  # Show node ID\n",
    "        size=size_scale[node],  # Adjust size\n",
    "        color=type_colors_rgba.get(node_type, \"gray\")  # Assign color based on type\n",
    "    )\n",
    "\n",
    "# Add edges with labels for relationship type\n",
    "for edge in G.edges(data=True):\n",
    "    source, target, attr = edge\n",
    "    rel_label = attr.get(\"label\", \"\")  # Get relationship type\n",
    "    net.add_edge(source, target, title=rel_label, label=rel_label)  # Show label on hover and as text\n",
    "\n",
    "# Save and display\n",
    "net.save_graph(\"graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/graphs/memgraph/\n",
    "\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "import os\n",
    "\n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    " \n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:31,826] p16596 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the database is empty\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    " \n",
    "# Create KG\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:31,854] p16596 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node labels and properties (name and type) are:\n",
      "- labels: (:Function)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Detail)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Class)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Instruction)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Object)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Variable)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Parameter)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Attribute)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Decorator)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Property)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Argument)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Type)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Agent type)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Component)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Method)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Library)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Concept)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Parser)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Message)\n",
      "  properties:\n",
      "    - id: string\n",
      "\n",
      "Nodes are connected with the following relationships:\n",
      "(:Method)-[:CALLS]->(:Method)\n",
      "(:Method)-[:USES]->(:Argument)\n",
      "(:Method)-[:USES]->(:Attribute)\n",
      "(:Method)-[:USES]->(:Detail)\n",
      "(:Method)-[:USES]->(:Instruction)\n",
      "(:Method)-[:VALIDATES]->(:Parameter)\n",
      "(:Method)-[:CALLS]->(:Function)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Parameter)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Argument)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Attribute)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Detail)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Instruction)\n",
      "(:Method)-[:CREATES_INSTANCE]->(:Class)\n",
      "(:Class)-[:RELATED_TO]->(:Class)\n",
      "(:Method)-[:CONSTRUCTS]->(:Class)\n",
      "(:Concept)-[:USES]->(:Parser)\n",
      "(:Message)-[:IMPORTS]->(:)\n",
      "(:Method)-[:DETERMINES]->(:Variable)\n",
      "(:Library)-[:IMPORTS]->(:Concept)\n",
      "(:Library)-[:ENABLES]->(:Concept)\n",
      "(:Concept)-[:VALIDATES]->(:Function)\n",
      "(:Class)-[:RELATED_TO]->(:Type)\n",
      "(:Library)-[:IMPORTS]->(:Function)\n",
      "(:Library)-[:IMPORTS]->(:Component)\n",
      "(:Concept)-[:RELATED_TO]->(:Type)\n",
      "(:Method)-[:CREATES]->(:Class)\n",
      "(:Method)-[:GENERATES]->(:Variable)\n",
      "(:Class)-[:HAS_ATTRIBUTE]->(:Attribute)\n",
      "(:Method)-[:USES]->(:Parameter)\n",
      "(:Method)-[:RETURNS]->(:Object)\n",
      "(:Concept)-[:USES]->(:Library)\n",
      "(:Concept)-[:RELATED_TO]->(:Class)\n",
      "(:Class)-[:USES]->(:Parser)\n",
      "(:Class)-[:INTERACTS_WITH]->(:Concept)\n",
      "(:Class)-[:INTERACTS_WITH]->(:Class)\n",
      "(:Class)-[:INHERITS_FROM]->(:Concept)\n",
      "(:Library)-[:IMPORTS]->(:Type)\n",
      "(:Class)-[:VALIDATES]->(:Function)\n",
      "(:Class)-[:INHERITS_FROM]->(:Class)\n",
      "(:Method)-[:CREATES]->(:Variable)\n",
      "(:Method)-[:REQUIRES]->(:Attribute)\n",
      "(:Class)-[:DECORATED_WITH]->(:Decorator)\n",
      "(:Property)-[:RETURNS]->(:)\n",
      "(:Method)-[:CREATES_INSTANCE]->(:Parser)\n",
      "(:Attribute)-[:INSTANTIATES]->(:Parser)\n",
      "(:Method)-[:REQUIRES]->(:Argument)\n",
      "(:Property)-[:RETURNS]->(:Agent type)\n",
      "(:Method)-[:RETURNS]->(:Component)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"\n",
    "Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\",temperature=0)\n",
    "schema = graph.schema\n",
    "\n",
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not use atomic operations in your Cypher queries.\n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "MEMGRAPH_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=MEMGRAPH_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    llm,\n",
    "    cypher_prompt=MEMGRAPH_GENERATION_PROMPT,\n",
    "    graph=graph,\n",
    "    model_id=\"gpt-4o-turbo\",\n",
    "    return_intermediate_steps=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:33,195] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:33,611] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Steps :  [{'query': \"MATCH (c:Class {id: 'Conversationalagent'})<-[:CREATES_INSTANCE]-(m:Method) RETURN m\"}, {'context': []}]\n",
      "Final Response :  I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "q = \"what methods are related to class Conversationalagent?\"\n",
    "response = chain.invoke(q)\n",
    "print(\"Intermediate Steps : \", response['intermediate_steps'])\n",
    "print(\"Final Response : \", response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c:Class {id: 'Conversationalagent'})-[:TAKES_PARAMETER]->(p:Parameter) RETURN p.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:34,433] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "MATCH (n)-[r]->(m) WHERE n.id = 'Conversationalagent' RETURN n, r, m\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}), 'm': {'id': 'Agent'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}), 'm': {'id': 'Agent'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'DECORATED_WITH', {'id': '@Deprecated'}), 'm': {'id': '@Deprecated'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Output_Parser'}), 'm': {'id': 'Output_Parser'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:35,237] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is related to the id Conversationalagent',\n",
       " 'result': 'The id Conversationalagent is related to the following: it inherits from the id Agent, is decorated with @Deprecated, and has attributes Ai_Prefix and Output_Parser.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "qa_chainn = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True,allow_dangerous_requests=True)\n",
    "response = qa_chainn.invoke({\"query\": \"what is related to the id Conversationalagent\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}),\n",
       "  'n': {'id': 'Agent'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}),\n",
       "  'n': {'id': 'Agent'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'DECORATED_WITH',\n",
       "   {'id': '@Deprecated'}),\n",
       "  'n': {'id': '@Deprecated'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Ai_Prefix'}),\n",
       "  'n': {'id': 'Ai_Prefix'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'HAS_ATTRIBUTE',\n",
       "   {'id': 'Output_Parser'}),\n",
       "  'n': {'id': 'Output_Parser'}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c)-[r]->(n) WHERE c.id = 'Conversationalagent' RETURN c, r, n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "# How many people played in Top Gun?\n",
    "MATCH (m:Movie {{name:\"Top Gun\"}})<-[:ACTED_IN]-()\n",
    "RETURN count(*) AS numberOfActors\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_transformer_filtered = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Nationality\", \"Concept\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"INVOLVED_IN\", \"COLLABORATES_WITH\"],\n",
    "# )\n",
    "# graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "#     documents\n",
    "# )\n",
    "\n",
    "# print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents_filtered[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \\n to \\\\n\n",
    "\n",
    "code = '''\n",
    "# An agent designed to hold a conversation in addition to using tools.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "from langchain_core._api import deprecated\n",
    "from langchain_core.callbacks import BaseCallbackManager\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import Field\n",
    "\n",
    "from langchain._api.deprecation import AGENT_DEPRECATION_WARNING\n",
    "from langchain.agents.agent import Agent, AgentOutputParser\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n",
    "from langchain.agents.utils import validate_tools_single_input\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "@deprecated(\n",
    "    \"0.1.0\",\n",
    "    message=AGENT_DEPRECATION_WARNING,\n",
    "    removal=\"1.0\",\n",
    ")\n",
    "class ConversationalAgent(Agent):\n",
    "    \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\n",
    "\n",
    "    ai_prefix: str = \"AI\"\n",
    "    \"\"\"Prefix to use before AI output.\"\"\"\n",
    "    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\n",
    "    \"\"\"Output parser for the agent.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_default_output_parser(\n",
    "        cls, ai_prefix: str = \"AI\", **kwargs: Any\n",
    "    ) -> AgentOutputParser:\n",
    "        return ConvoOutputParser(ai_prefix=ai_prefix)\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        \"\"\"Return Identifier of agent type.\"\"\"\n",
    "        return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n",
    "\n",
    "    @property\n",
    "    def observation_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the observation with.\n",
    "\n",
    "        Returns:\n",
    "            \"Observation: \"\n",
    "        \"\"\"\n",
    "        return \"Observation: \"\n",
    "\n",
    "    @property\n",
    "    def llm_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the llm call with.\n",
    "\n",
    "        Returns:\n",
    "            \"Thought: \"\n",
    "        \"\"\"\n",
    "        return \"Thought:\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_prompt(\n",
    "        cls,\n",
    "        tools: Sequence[BaseTool],\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "    ) -> PromptTemplate:\n",
    "        \"\"\"Create prompt in the style of the zero-shot agent.\n",
    "\n",
    "        Args:\n",
    "            tools: List of tools the agent will have access to, used to format the\n",
    "                prompt.\n",
    "            prefix: String to put before the list of tools. Defaults to PREFIX.\n",
    "            suffix: String to put after the list of tools. Defaults to SUFFIX.\n",
    "            format_instructions: Instructions on how to use the tools. Defaults to\n",
    "                FORMAT_INSTRUCTIONS\n",
    "            ai_prefix: String to use before AI output. Defaults to \"AI\".\n",
    "            human_prefix: String to use before human output.\n",
    "                Defaults to \"Human\".\n",
    "            input_variables: List of input variables the final prompt will expect.\n",
    "                Defaults to [\"input\", \"chat_history\", \"agent_scratchpad\"].\n",
    "\n",
    "        Returns:\n",
    "            A PromptTemplate with the template assembled from the pieces here.\n",
    "        \"\"\"\n",
    "        tool_strings = \"\\\\n\".join(\n",
    "            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n",
    "        )\n",
    "        tool_names = \", \".join([tool.name for tool in tools])\n",
    "        format_instructions = format_instructions.format(\n",
    "            tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\n",
    "        )\n",
    "        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "        if input_variables is None:\n",
    "            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
    "        return PromptTemplate(template=template, input_variables=input_variables)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "        super()._validate_tools(tools)\n",
    "        validate_tools_single_input(cls.__name__, tools)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        output_parser: Optional[AgentOutputParser] = None,\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Agent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "        Args:\n",
    "            llm: The language model to use.\n",
    "            tools: A list of tools to use.\n",
    "            callback_manager: The callback manager to use. Default is None.\n",
    "            output_parser: The output parser to use. Default is None.\n",
    "            prefix: The prefix to use in the prompt. Default is PREFIX.\n",
    "            suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
    "            format_instructions: The format instructions to use.\n",
    "                Default is FORMAT_INSTRUCTIONS.\n",
    "            ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
    "            human_prefix: The prefix to use before human output.\n",
    "                Default is \"Human\".\n",
    "            input_variables: The input variables to use. Default is None.\n",
    "            **kwargs: Any additional keyword arguments to pass to the agent.\n",
    "\n",
    "        Returns:\n",
    "            An agent.\n",
    "        \"\"\"\n",
    "        cls._validate_tools(tools)\n",
    "        prompt = cls.create_prompt(\n",
    "            tools,\n",
    "            ai_prefix=ai_prefix,\n",
    "            human_prefix=human_prefix,\n",
    "            prefix=prefix,\n",
    "            suffix=suffix,\n",
    "            format_instructions=format_instructions,\n",
    "            input_variables=input_variables,\n",
    "        )\n",
    "        llm_chain = LLMChain(  # type: ignore[misc]\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            callback_manager=callback_manager,\n",
    "        )\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        _output_parser = output_parser or cls._get_default_output_parser(\n",
    "            ai_prefix=ai_prefix\n",
    "        )\n",
    "        return cls(\n",
    "            llm_chain=llm_chain,\n",
    "            allowed_tools=tool_names,\n",
    "            ai_prefix=ai_prefix,\n",
    "            output_parser=_output_parser,\n",
    "            **kwargs,\n",
    "        )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: ['__copy__', '__deepcopy__', '__delattr__', '__eq__', '__getattr__', '__getstate__', '__init__', '__iter__', '__pretty__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__str__', '_calculate_keys', '_check_frozen', '_construct_scratchpad', '_copy_and_set_values', '_fix_text', '_iter', 'aplan', 'copy', 'dict', 'get_allowed_tools', 'get_full_inputs', 'json', 'model_copy', 'model_dump', 'model_dump_json', 'model_post_init', 'plan', 'return_stopped_response', 'save', 'tool_run_logging_kwargs', 'validate_prompt']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n",
    "# List all methods of the class\n",
    "methods = inspect.getmembers(ConversationalAgent, predicate=inspect.isfunction)\n",
    "print(\"Methods:\", [method[0] for method in methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_code.ConversationalAgent"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConversationalAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code for method 'from_llm_and_tools':\n",
      "\n",
      "    @classmethod\n",
      "    def from_llm_and_tools(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        tools: Sequence[BaseTool],\n",
      "        callback_manager: Optional[BaseCallbackManager] = None,\n",
      "        output_parser: Optional[AgentOutputParser] = None,\n",
      "        prefix: str = PREFIX,\n",
      "        suffix: str = SUFFIX,\n",
      "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
      "        ai_prefix: str = \"AI\",\n",
      "        human_prefix: str = \"Human\",\n",
      "        input_variables: Optional[List[str]] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Agent:\n",
      "        \"\"\"Construct an agent from an LLM and tools.\n",
      "\n",
      "        Args:\n",
      "            llm: The language model to use.\n",
      "            tools: A list of tools to use.\n",
      "            callback_manager: The callback manager to use. Default is None.\n",
      "            output_parser: The output parser to use. Default is None.\n",
      "            prefix: The prefix to use in the prompt. Default is PREFIX.\n",
      "            suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
      "            format_instructions: The format instructions to use.\n",
      "                Default is FORMAT_INSTRUCTIONS.\n",
      "            ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
      "            human_prefix: The prefix to use before human output.\n",
      "                Default is \"Human\".\n",
      "            input_variables: The input variables to use. Default is None.\n",
      "            **kwargs: Any additional keyword arguments to pass to the agent.\n",
      "\n",
      "        Returns:\n",
      "            An agent.\n",
      "        \"\"\"\n",
      "        cls._validate_tools(tools)\n",
      "        prompt = cls.create_prompt(\n",
      "            tools,\n",
      "            ai_prefix=ai_prefix,\n",
      "            human_prefix=human_prefix,\n",
      "            prefix=prefix,\n",
      "            suffix=suffix,\n",
      "            format_instructions=format_instructions,\n",
      "            input_variables=input_variables,\n",
      "        )\n",
      "        llm_chain = LLMChain(  # type: ignore[misc]\n",
      "            llm=llm,\n",
      "            prompt=prompt,\n",
      "            callback_manager=callback_manager,\n",
      "        )\n",
      "        tool_names = [tool.name for tool in tools]\n",
      "        _output_parser = output_parser or cls._get_default_output_parser(\n",
      "            ai_prefix=ai_prefix\n",
      "        )\n",
      "        return cls(\n",
      "            llm_chain=llm_chain,\n",
      "            allowed_tools=tool_names,\n",
      "            ai_prefix=ai_prefix,\n",
      "            output_parser=_output_parser,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to get the full source code of a specific method\n",
    "def get_method_source(class_obj, method_name):\n",
    "    \"\"\"Retrieve the full source code of a method from a class.\"\"\"\n",
    "    try:\n",
    "        method = getattr(class_obj, method_name)\n",
    "        return inspect.getsource(method)\n",
    "    except AttributeError:\n",
    "        return f\"Method '{method_name}' not found.\"\n",
    "    except TypeError:\n",
    "        return f\"Could not retrieve source for '{method_name}'.\"\n",
    "\n",
    "# Example Usage: Get the code for a specific method\n",
    "method_name = \"from_llm_and_tools\"  # Change this to the method you want\n",
    "# ConversationalAgent = 'agent_code.ConversationalAgent'\n",
    "\n",
    "method_code = get_method_source(ConversationalAgent, method_name)\n",
    "\n",
    "print(f\"\\nCode for method '{method_name}':\\n\")\n",
    "print(method_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'ConversationalAgent' found in: ./agent_code.py\n",
      "\n",
      "Could not retrieve source for class 'ConversationalAgent'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_source(module, class_name):\n",
    "    \"\"\"Retrieve the source code of a class if it exists in a module.\"\"\"\n",
    "    class_obj = getattr(module, class_name, None)\n",
    "    if class_obj:\n",
    "        try:\n",
    "            return inspect.getsourcefile(ConversationalAgent)\n",
    "        except TypeError:\n",
    "            return f\"Could not retrieve source for class '{class_name}'.\"\n",
    "    return None\n",
    "\n",
    "def search_for_class(root_dir, class_name=\"ConversationalAgent\"):\n",
    "    \"\"\"Search for the given class in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if module:\n",
    "            class_source = get_class_source(module, class_name)\n",
    "            if class_source:\n",
    "                print(f\"Class '{class_name}' found in: {file}\\n\")\n",
    "                print(class_source)\n",
    "                return  # Stop after finding the first occurrence\n",
    "    print(f\"Class '{class_name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_class(root_repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'ConversationalAgent' not found in any file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_source(module, search_name):\n",
    "    \"\"\"Retrieve the source code of a class if it exists in a module.\"\"\"\n",
    "    class_obj = getattr(module, search_name, None)\n",
    "    if class_obj:\n",
    "        try:\n",
    "            return inspect.getsource(class_obj)  # Extract class definition only\n",
    "        except TypeError:\n",
    "            return None  # Unable to get source, likely a built-in or compiled class\n",
    "    return None\n",
    "\n",
    "def search_for_class(root_dir, search_name):\n",
    "    \"\"\"Search for the given class in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if module:\n",
    "            class_source = get_class_source(module, search_name)\n",
    "            if class_source:\n",
    "                print(f\"Class '{search_name}' found in: {file}\\n\")\n",
    "                print(class_source)  # Print only the class definition\n",
    "                return  # Stop after finding the first occurrence\n",
    "    print(f\"Class '{search_name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_class(root_repo_path, search_name = 'ConversationalAgent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'ConversationalAgent'\n",
    "class_obj = getattr(module, class_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_code.ConversationalAgent"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_agent_type' found in class 'Agent' in: ./agent_code.py\n",
      "\n",
      "    @property\n",
      "    def _agent_type(self) -> str:\n",
      "        \"\"\"Return Identifier of an agent type.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_source(obj):\n",
    "    \"\"\"Retrieve the source code of a function, method, class, or property.\"\"\"\n",
    "    try:\n",
    "        if isinstance(obj, classmethod):\n",
    "            return inspect.getsource(obj.__func__)  # Unwrap class method\n",
    "        elif isinstance(obj, staticmethod):\n",
    "            return inspect.getsource(obj.__func__)  # Unwrap static method\n",
    "        elif isinstance(obj, property):\n",
    "            return inspect.getsource(obj.fget)  # Get property getter\n",
    "        else:\n",
    "            return inspect.getsource(obj)  # Default for functions, methods, classes\n",
    "    except TypeError:\n",
    "        return None  # If the object is not directly accessible\n",
    "\n",
    "def search_for_name(root_dir, name):\n",
    "    \"\"\"Search for a class, function, or method in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    \n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if not module:\n",
    "            continue\n",
    "        \n",
    "        # 1 First, search for a class or function at the module level\n",
    "        obj = getattr(module, name, None)\n",
    "        if obj:\n",
    "            source_code = get_source(obj)\n",
    "            if source_code:\n",
    "                print(f\"'{name}' found in: {file}\\n\")\n",
    "                print(source_code)\n",
    "                return  # Stop after finding the first occurrence\n",
    "\n",
    "        # 2 Next, search inside classes for methods and properties\n",
    "        for class_name, class_obj in inspect.getmembers(module, inspect.isclass):\n",
    "            # Search for class methods, instance methods, and properties\n",
    "            for method_name, method_obj in inspect.getmembers(class_obj):\n",
    "                if method_name == name:\n",
    "                    source_code = get_source(method_obj)\n",
    "                    if source_code:\n",
    "                        print(f\"'{name}' found in class '{class_name}' in: {file}\\n\")\n",
    "                        print(source_code)\n",
    "                        return  # Stop after finding the first occurrence\n",
    "\n",
    "    print(f\"'{name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_name(root_repo_path, \"_agent_type\")  # Change to search for any class, method, or function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ConversationalAgent' not found in any file.\n"
     ]
    }
   ],
   "source": [
    "search_for_name(root_repo_path, \"ConversationalAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
