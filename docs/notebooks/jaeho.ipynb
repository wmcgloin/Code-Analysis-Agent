{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def plot_network(\n",
    "#     G,\n",
    "#     node_size=1000,\n",
    "#     node_color=\"skyblue\",\n",
    "#     edge_color=\"gray\",\n",
    "#     font_size=10,\n",
    "#     title=\"Network Graph\",\n",
    "#     figsize=(12, 8),\n",
    "#     with_labels=True,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"husl\",\n",
    "#     k=0.1,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plot a network graph with seaborn-style aesthetics.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     G : networkx.Graph\n",
    "#         The network graph to visualize\n",
    "#     node_size : int or list\n",
    "#         Size of nodes (can be a single value or list for different sizes)\n",
    "#     node_color : str or list\n",
    "#         Color of nodes (can be a single value or list for different colors)\n",
    "#     edge_color : str\n",
    "#         Color of edges\n",
    "#     font_size : int\n",
    "#         Size of node labels\n",
    "#     title : str\n",
    "#         Title of the plot\n",
    "#     figsize : tuple\n",
    "#         Figure size (width, height)\n",
    "#     with_labels : bool\n",
    "#         Whether to show node labels\n",
    "#     layout : str\n",
    "#         Type of layout ('spring', 'circular', 'random', 'shell')\n",
    "#     palette : str\n",
    "#         Seaborn color palette to use if node_color is not specified\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     fig, ax : tuple\n",
    "#         Matplotlib figure and axis objects\n",
    "#     \"\"\"\n",
    "#     # Set the style\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "\n",
    "#     # Create figure\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "#     # Choose layout\n",
    "#     layouts = {\n",
    "#         \"spring\": nx.spring_layout,\n",
    "#         \"circular\": nx.circular_layout,\n",
    "#         \"random\": nx.random_layout,\n",
    "#         \"shell\": nx.shell_layout,\n",
    "#     }\n",
    "#     pos = layouts.get(layout, nx.spring_layout)(G,k)\n",
    "\n",
    "#     # If node_color is not specified, use seaborn palette\n",
    "#     if isinstance(node_color, str) and node_color == \"skyblue\":\n",
    "#         colors = sns.color_palette(palette, n_colors=len(G.nodes()))\n",
    "#     else:\n",
    "#         colors = node_color\n",
    "\n",
    "#     # Draw the network\n",
    "#     nx.draw(\n",
    "#         G,\n",
    "#         pos,\n",
    "#         node_color=colors,\n",
    "#         node_size=node_size,\n",
    "#         edge_color=edge_color,\n",
    "#         with_labels=with_labels,\n",
    "#         font_size=font_size,\n",
    "#         font_weight=\"bold\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "\n",
    "#     # Add title\n",
    "#     plt.title(title, fontsize=font_size + 4, pad=20)\n",
    "\n",
    "#     return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: USING langchain_experimental.graph_transformers main.\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# # TypedDict\n",
    "# class Json(TypedDict):\n",
    "#     \"\"\"Json to return.\"\"\"\n",
    "\n",
    "#     setup: Annotated[dict, ..., \"The setup of the dict\"]\n",
    "#     depth: Annotated[int, ..., \"How many layers deep the dict is.\"]\n",
    "\n",
    "\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# text = \"\"\"\n",
    "# Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "# She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "# Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "# She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "# \"\"\"\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "# for i, doc in enumerate(graph_documents):\n",
    "#     logger.info(f\"document #{i+1}\")\n",
    "#     nodes = doc.nodes\n",
    "#     relationships = doc.relationships\n",
    "#     for n in nodes:\n",
    "#         logger.info(f\"Nodes:{n}\")\n",
    "#     for r in relationships:\n",
    "#         logger.info(f\"Relationships:{r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_graph import plot_network\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# nodes = [str(node) for node in graph_documents[0].nodes]\n",
    "# relationships = [\n",
    "#     (str(rel.source), str(rel.target)) for rel in graph_documents[0].relationships\n",
    "# ]\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_edges_from(relationships)\n",
    "\n",
    "# custom_colors = sns.color_palette(\"Set2\", n_colors=len(G.nodes()))\n",
    "# node_sizes = [3000 if d > 5 else 1000 for v, d in G.degree()]\n",
    "\n",
    "# fig, ax = plot_network(\n",
    "#     G,\n",
    "#     node_size=node_sizes,\n",
    "#     node_color=custom_colors,\n",
    "#     edge_color=\"#cccccc\",\n",
    "#     font_size=12,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"Set2\",\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"id='Marie Curie' type='Person' properties={}\",\n",
       " \"id='Pierre Curie' type='Person' properties={}\",\n",
       " \"id='University Of Paris' type='Institution' properties={}\",\n",
       " \"id='Nobel Prize' type='Award' properties={}\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='Pierre Curie' type='Person' properties={}\"),\n",
       " (\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='Nobel Prize' type='Award' properties={}\"),\n",
       " (\"id='Pierre Curie' type='Person' properties={}\",\n",
       "  \"id='Nobel Prize' type='Award' properties={}\"),\n",
       " (\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='University Of Paris' type='Institution' properties={}\"),\n",
       " (\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='Nobel Prize' type='Award' properties={}\"),\n",
       " (\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='Nobel Prize' type='Award' properties={}\"),\n",
       " (\"id='Marie Curie' type='Person' properties={}\",\n",
       "  \"id='Nobel Prize' type='Award' properties={}\")]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with sample langchain code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 08:53:25,808] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided code implements a class called `ConversationalAgent`, which is designed to facilitate conversational interactions while enabling the usage of various tools. The code starts by importing several packages and modules, including future annotations support for type hints, various core functionalities from the `langchain_core` library, and components from the `langchain` library like agents and callback management.\n",
      "\n",
      "The first imported module is `__future__`, imported with a focus on enabling forward compatibility with future Python versions through annotations. Next, `Any`, `List`, `Optional`, and `Sequence` are imported from the `typing` module to allow for type hinting across the agent's function definitions. The core classes and functions imported from `langchain_core` include `deprecated`, `BaseCallbackManager`, `BaseLanguageModel`, `PromptTemplate`, and `BaseTool`. The use of `deprecated` indicates that the `ConversationalAgent` class is marked for potential removal in an upcoming version of the library. The `Field` class from the `pydantic` module is imported for data validation and class field management.\n",
      "\n",
      "The `ConversationalAgent` class inherits from the `Agent` class. The class starts with a deprecation decorator that specifies the version when it was marked deprecated, a warning message, and a version when it will be removed. Within the class, there are two primary attributes defined with default values: `ai_prefix`, which is set to `\"AI\"` and serves as a prefix for AI responses, and `output_parser`, which uses `Field` and is initialized with a default factory that creates an instance of `ConvoOutputParser`, assigning it to format the agent's output.\n",
      "\n",
      "The `_get_default_output_parser` class method returns an instance of `ConvoOutputParser`, with an optional `ai_prefix` parameter defaulting to `\"AI\"`. The type hint indicates that this method returns an instance of `AgentOutputParser`. The `_agent_type` property defines the agent type as `AgentType.CONVERSATIONAL_REACT_DESCRIPTION`, specifying how the agent should be classified. The `observation_prefix` property returns the string `\"Observation: \"` which serves as a prefix for observations made by the agent. Similarly, the `llm_prefix` property returns the string `\"Thought:\"` to prefix AI thoughts.\n",
      "\n",
      "The `create_prompt` class method assembles a prompt template based on the provided tools while allowing customization of its format through multiple parameters. The `tools` parameter expects a sequence of `BaseTool` instances that the agent can utilize. The method constructs a string representation for each tool, combining their names and descriptions. Then, it joins these strings with line breaks and formats the instructions using provided parameters and any defaults. If `input_variables` is not specified, it defaults to a list containing `\"input\"`, `\"chat_history\"`, and `\"agent_scratchpad\"`. Ultimately, the method returns a `PromptTemplate` instance with the assembled template and input variables.\n",
      "\n",
      "Following this, the `_validate_tools` class method invokes the parent class's tool validation and additionally invokes `validate_tools_single_input`, ensuring a single input format for tools is adhered to by the agent. This method takes a sequence of `BaseTool` items as its only parameter.\n",
      "\n",
      "The `from_llm_and_tools` class method acts as a factory method for constructing an agent using a language model and a collection of tools. It begins by calling `_validate_tools` to ensure that the provided tools are valid. Then, it calls the `create_prompt` method to generate a prompt, which is used to instantiate an `LLMChain`. The `llm_chain` is created by passing the language model, the generated prompt, and a callback manager if specified. After gathering the names of the tools into a list, an output parser is determined either from the specified input or through the `_get_default_output_parser` method. Finally, this method returns an instance of the `ConversationalAgent`, initializing it with the composed `llm_chain`, list of allowed tools, AI prefix, output parser, and any additional keyword arguments passed through `**kwargs`. The code ends with a few comments noting the specific arguments and giving insight into the return type of the function.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        # \"Return a json that describes the logic flow of the given codebase. Only track call stacks. Output in Cypher compliant format for graph representation.\"\n",
    "        \"\"\"\n",
    "        Return the given code as natural language description with great detail.\n",
    "        Explain all the functions, parameters, and the logic flow of the code.\n",
    "        EXPLAIN EVERY SINGLE LINE OF THE CODE.\n",
    "        Do not use adjectives and adverbs. Only describe the code and do not give an overall summary.\n",
    "        Also include packages that were imported and how they were used in the code.\n",
    "        Do not give me bullet points, and output a paragraph format.\n",
    "        Do not use ambiguous pronouns and use exact names in every description.\n",
    "        Explain each class, function, and variable separately and do not include explanations such as 'as mentioned before' or anything that refers to a previous explanation.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        '''\n",
    "        # An agent designed to hold a conversation in addition to using tools.\n",
    "\n",
    "        from __future__ import annotations\n",
    "\n",
    "        from typing import Any, List, Optional, Sequence\n",
    "\n",
    "        from langchain_core._api import deprecated\n",
    "        from langchain_core.callbacks import BaseCallbackManager\n",
    "        from langchain_core.language_models import BaseLanguageModel\n",
    "        from langchain_core.prompts import PromptTemplate\n",
    "        from langchain_core.tools import BaseTool\n",
    "        from pydantic import Field\n",
    "\n",
    "        from langchain._api.deprecation import AGENT_DEPRECATION_WARNING\n",
    "        from langchain.agents.agent import Agent, AgentOutputParser\n",
    "        from langchain.agents.agent_types import AgentType\n",
    "        from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "        from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n",
    "        from langchain.agents.utils import validate_tools_single_input\n",
    "        from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "        @deprecated(\n",
    "            \"0.1.0\",\n",
    "            message=AGENT_DEPRECATION_WARNING,\n",
    "            removal=\"1.0\",\n",
    "        )\n",
    "        class ConversationalAgent(Agent):\n",
    "            \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\n",
    "\n",
    "            ai_prefix: str = \"AI\"\n",
    "            \"\"\"Prefix to use before AI output.\"\"\"\n",
    "            output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\n",
    "            \"\"\"Output parser for the agent.\"\"\"\n",
    "\n",
    "            @classmethod\n",
    "            def _get_default_output_parser(\n",
    "                cls, ai_prefix: str = \"AI\", **kwargs: Any\n",
    "            ) -> AgentOutputParser:\n",
    "                return ConvoOutputParser(ai_prefix=ai_prefix)\n",
    "\n",
    "            @property\n",
    "            def _agent_type(self) -> str:\n",
    "                \"\"\"Return Identifier of agent type.\"\"\"\n",
    "                return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n",
    "\n",
    "            @property\n",
    "            def observation_prefix(self) -> str:\n",
    "                \"\"\"Prefix to append the observation with.\n",
    "\n",
    "                Returns:\n",
    "                    \"Observation: \"\n",
    "                \"\"\"\n",
    "                return \"Observation: \"\n",
    "\n",
    "            @property\n",
    "            def llm_prefix(self) -> str:\n",
    "                \"\"\"Prefix to append the llm call with.\n",
    "\n",
    "                Returns:\n",
    "                    \"Thought: \"\n",
    "                \"\"\"\n",
    "                return \"Thought:\"\n",
    "\n",
    "            @classmethod\n",
    "            def create_prompt(\n",
    "                cls,\n",
    "                tools: Sequence[BaseTool],\n",
    "                prefix: str = PREFIX,\n",
    "                suffix: str = SUFFIX,\n",
    "                format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "                ai_prefix: str = \"AI\",\n",
    "                human_prefix: str = \"Human\",\n",
    "                input_variables: Optional[List[str]] = None,\n",
    "            ) -> PromptTemplate:\n",
    "                \"\"\"Create prompt in the style of the zero-shot agent.\n",
    "\n",
    "                Args:\n",
    "                    tools: List of tools the agent will have access to, used to format the\n",
    "                        prompt.\n",
    "                    prefix: String to put before the list of tools. Defaults to PREFIX.\n",
    "                    suffix: String to put after the list of tools. Defaults to SUFFIX.\n",
    "                    format_instructions: Instructions on how to use the tools. Defaults to\n",
    "                        FORMAT_INSTRUCTIONS\n",
    "                    ai_prefix: String to use before AI output. Defaults to \"AI\".\n",
    "                    human_prefix: String to use before human output.\n",
    "                        Defaults to \"Human\".\n",
    "                    input_variables: List of input variables the final prompt will expect.\n",
    "                        Defaults to [\"input\", \"chat_history\", \"agent_scratchpad\"].\n",
    "\n",
    "                Returns:\n",
    "                    A PromptTemplate with the template assembled from the pieces here.\n",
    "                \"\"\"\n",
    "                tool_strings = \"\\n\".join(\n",
    "                    [f\"> {tool.name}: {tool.description}\" for tool in tools]\n",
    "                )\n",
    "                tool_names = \", \".join([tool.name for tool in tools])\n",
    "                format_instructions = format_instructions.format(\n",
    "                    tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\n",
    "                )\n",
    "                template = \"\\n\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "                if input_variables is None:\n",
    "                    input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
    "                return PromptTemplate(template=template, input_variables=input_variables)\n",
    "\n",
    "            @classmethod\n",
    "            def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "                super()._validate_tools(tools)\n",
    "                validate_tools_single_input(cls.__name__, tools)\n",
    "\n",
    "            @classmethod\n",
    "            def from_llm_and_tools(\n",
    "                cls,\n",
    "                llm: BaseLanguageModel,\n",
    "                tools: Sequence[BaseTool],\n",
    "                callback_manager: Optional[BaseCallbackManager] = None,\n",
    "                output_parser: Optional[AgentOutputParser] = None,\n",
    "                prefix: str = PREFIX,\n",
    "                suffix: str = SUFFIX,\n",
    "                format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "                ai_prefix: str = \"AI\",\n",
    "                human_prefix: str = \"Human\",\n",
    "                input_variables: Optional[List[str]] = None,\n",
    "                **kwargs: Any,\n",
    "            ) -> Agent:\n",
    "                \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "                Args:\n",
    "                    llm: The language model to use.\n",
    "                    tools: A list of tools to use.\n",
    "                    callback_manager: The callback manager to use. Default is None.\n",
    "                    output_parser: The output parser to use. Default is None.\n",
    "                    prefix: The prefix to use in the prompt. Default is PREFIX.\n",
    "                    suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
    "                    format_instructions: The format instructions to use.\n",
    "                        Default is FORMAT_INSTRUCTIONS.\n",
    "                    ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
    "                    human_prefix: The prefix to use before human output.\n",
    "                        Default is \"Human\".\n",
    "                    input_variables: The input variables to use. Default is None.\n",
    "                    **kwargs: Any additional keyword arguments to pass to the agent.\n",
    "\n",
    "                Returns:\n",
    "                    An agent.\n",
    "                \"\"\"\n",
    "                cls._validate_tools(tools)\n",
    "                prompt = cls.create_prompt(\n",
    "                    tools,\n",
    "                    ai_prefix=ai_prefix,\n",
    "                    human_prefix=human_prefix,\n",
    "                    prefix=prefix,\n",
    "                    suffix=suffix,\n",
    "                    format_instructions=format_instructions,\n",
    "                    input_variables=input_variables,\n",
    "                )\n",
    "                llm_chain = LLMChain(  # type: ignore[misc]\n",
    "                    llm=llm,\n",
    "                    prompt=prompt,\n",
    "                    callback_manager=callback_manager,\n",
    "                )\n",
    "                tool_names = [tool.name for tool in tools]\n",
    "                _output_parser = output_parser or cls._get_default_output_parser(\n",
    "                    ai_prefix=ai_prefix\n",
    "                )\n",
    "                return cls(\n",
    "                    llm_chain=llm_chain,\n",
    "                    allowed_tools=tool_names,\n",
    "                    ai_prefix=ai_prefix,\n",
    "                    output_parser=_output_parser,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            '''\n",
    "    ),\n",
    "]\n",
    "\n",
    "# structured_llm = llm.with_structured_output(Json)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([response.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 08:54:34,651] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:54:44,102] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:54:47,421] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:54:51,188] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:54:54,530] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:54:57,385] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 08:55:02,658] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = response.content\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# # logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 08:55:41,073] p10912 {406922825.py:2} INFO - document #1\n",
      "[2025-03-14 08:55:41,076] p10912 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,077] p10912 {406922825.py:6} INFO - Nodes:id='Langchain_Core' type='Library' properties={}\n",
      "[2025-03-14 08:55:41,078] p10912 {406922825.py:6} INFO - Nodes:id='Langchain' type='Library' properties={}\n",
      "[2025-03-14 08:55:41,078] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Langchain_Core', type='Library', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,079] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Langchain', type='Library', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,080] p10912 {406922825.py:2} INFO - document #2\n",
      "[2025-03-14 08:55:41,080] p10912 {406922825.py:6} INFO - Nodes:id='__Future__' type='Module' properties={}\n",
      "[2025-03-14 08:55:41,081] p10912 {406922825.py:6} INFO - Nodes:id='Typing' type='Module' properties={}\n",
      "[2025-03-14 08:55:41,081] p10912 {406922825.py:6} INFO - Nodes:id='Langchain_Core' type='Module' properties={}\n",
      "[2025-03-14 08:55:41,082] p10912 {406922825.py:6} INFO - Nodes:id='Pydantic' type='Module' properties={}\n",
      "[2025-03-14 08:55:41,082] p10912 {406922825.py:6} INFO - Nodes:id='Any' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,083] p10912 {406922825.py:6} INFO - Nodes:id='List' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,083] p10912 {406922825.py:6} INFO - Nodes:id='Optional' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,083] p10912 {406922825.py:6} INFO - Nodes:id='Sequence' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,084] p10912 {406922825.py:6} INFO - Nodes:id='Deprecated' type='Function' properties={}\n",
      "[2025-03-14 08:55:41,084] p10912 {406922825.py:6} INFO - Nodes:id='Basecallbackmanager' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,085] p10912 {406922825.py:6} INFO - Nodes:id='Baselanguagemodel' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,085] p10912 {406922825.py:6} INFO - Nodes:id='Prompttemplate' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,086] p10912 {406922825.py:6} INFO - Nodes:id='Basetool' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,087] p10912 {406922825.py:6} INFO - Nodes:id='Field' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,087] p10912 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,088] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='__Future__', type='Module', properties={}) target=Node(id='__Future__', type='Module', properties={}) type='ENABLED_FORWARD_COMPATIBILITY' properties={}\n",
      "[2025-03-14 08:55:41,088] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Module', properties={}) target=Node(id='Any', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,089] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Module', properties={}) target=Node(id='List', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,089] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Module', properties={}) target=Node(id='Optional', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,090] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Typing', type='Module', properties={}) target=Node(id='Sequence', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Deprecated', type='Function', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Basecallbackmanager', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Baselanguagemodel', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Prompttemplate', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Basetool', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,091] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Pydantic', type='Module', properties={}) target=Node(id='Field', type='Class', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-14 08:55:41,100] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Langchain_Core', type='Module', properties={}) target=Node(id='Conversationalagent', type='Class', properties={}) type='MARKED_FOR_REMOVAL' properties={}\n",
      "[2025-03-14 08:55:41,104] p10912 {406922825.py:2} INFO - document #3\n",
      "[2025-03-14 08:55:41,107] p10912 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,108] p10912 {406922825.py:6} INFO - Nodes:id='Agent' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,109] p10912 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Attribute' properties={}\n",
      "[2025-03-14 08:55:41,109] p10912 {406922825.py:6} INFO - Nodes:id='Output_Parser' type='Attribute' properties={}\n",
      "[2025-03-14 08:55:41,111] p10912 {406922825.py:6} INFO - Nodes:id='Convooutputparser' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,112] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Agent', type='Class', properties={}) type='INHERITS_FROM' properties={}\n",
      "[2025-03-14 08:55:41,114] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Ai_Prefix', type='Attribute', properties={}) type='HAS_ATTRIBUTE' properties={}\n",
      "[2025-03-14 08:55:41,115] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Conversationalagent', type='Class', properties={}) target=Node(id='Output_Parser', type='Attribute', properties={}) type='HAS_ATTRIBUTE' properties={}\n",
      "[2025-03-14 08:55:41,116] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Output_Parser', type='Attribute', properties={}) target=Node(id='Convooutputparser', type='Class', properties={}) type='USES' properties={}\n",
      "[2025-03-14 08:55:41,116] p10912 {406922825.py:2} INFO - document #4\n",
      "[2025-03-14 08:55:41,117] p10912 {406922825.py:6} INFO - Nodes:id='_Get_Default_Output_Parser' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,118] p10912 {406922825.py:6} INFO - Nodes:id='Convooutputparser' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,119] p10912 {406922825.py:6} INFO - Nodes:id='Agentoutputparser' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,119] p10912 {406922825.py:6} INFO - Nodes:id='_Agent_Type' type='Property' properties={}\n",
      "[2025-03-14 08:55:41,120] p10912 {406922825.py:6} INFO - Nodes:id='Agenttype.Conversational_React_Description' type='Enum' properties={}\n",
      "[2025-03-14 08:55:41,120] p10912 {406922825.py:6} INFO - Nodes:id='Observation_Prefix' type='Property' properties={}\n",
      "[2025-03-14 08:55:41,121] p10912 {406922825.py:6} INFO - Nodes:id='Llm_Prefix' type='Property' properties={}\n",
      "[2025-03-14 08:55:41,122] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}) target=Node(id='Convooutputparser', type='Class', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-14 08:55:41,123] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}) target=Node(id='Agentoutputparser', type='Class', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-14 08:55:41,123] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='_Agent_Type', type='Property', properties={}) target=Node(id='Agenttype.Conversational_React_Description', type='Enum', properties={}) type='DEFINES' properties={}\n",
      "[2025-03-14 08:55:41,124] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Observation_Prefix', type='Property', properties={}) target=Node(id='Observation: ', type='String', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-14 08:55:41,125] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Llm_Prefix', type='Property', properties={}) target=Node(id='Thought:', type='String', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-14 08:55:41,125] p10912 {406922825.py:2} INFO - document #5\n",
      "[2025-03-14 08:55:41,125] p10912 {406922825.py:6} INFO - Nodes:id='Create_Prompt' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,126] p10912 {406922825.py:6} INFO - Nodes:id='Basetool' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,126] p10912 {406922825.py:6} INFO - Nodes:id='Prompttemplate' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,127] p10912 {406922825.py:6} INFO - Nodes:id='Input_Variables' type='Parameter' properties={}\n",
      "[2025-03-14 08:55:41,127] p10912 {406922825.py:6} INFO - Nodes:id='Chat_History' type='Variable' properties={}\n",
      "[2025-03-14 08:55:41,128] p10912 {406922825.py:6} INFO - Nodes:id='Agent_Scratchpad' type='Variable' properties={}\n",
      "[2025-03-14 08:55:41,128] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Basetool', type='Class', properties={}) type='USES' properties={}\n",
      "[2025-03-14 08:55:41,129] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Input_Variables', type='Parameter', properties={}) type='CUSTOMIZES' properties={}\n",
      "[2025-03-14 08:55:41,129] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Create_Prompt', type='Method', properties={}) target=Node(id='Prompttemplate', type='Class', properties={}) type='RETURNS' properties={}\n",
      "[2025-03-14 08:55:41,130] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Input_Variables', type='Parameter', properties={}) target=Node(id='Chat_History', type='Variable', properties={}) type='DEFAULTS_TO' properties={}\n",
      "[2025-03-14 08:55:41,130] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Input_Variables', type='Parameter', properties={}) target=Node(id='Agent_Scratchpad', type='Variable', properties={}) type='DEFAULTS_TO' properties={}\n",
      "[2025-03-14 08:55:41,130] p10912 {406922825.py:2} INFO - document #6\n",
      "[2025-03-14 08:55:41,131] p10912 {406922825.py:6} INFO - Nodes:id='_Validate_Tools' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,132] p10912 {406922825.py:6} INFO - Nodes:id='Parent Class' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,133] p10912 {406922825.py:6} INFO - Nodes:id='Validate_Tools_Single_Input' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,134] p10912 {406922825.py:6} INFO - Nodes:id='Basetool' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,135] p10912 {406922825.py:6} INFO - Nodes:id='Agent' type='Entity' properties={}\n",
      "[2025-03-14 08:55:41,136] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='_Validate_Tools', type='Method', properties={}) target=Node(id='Parent Class', type='Class', properties={}) type='INVOKES' properties={}\n",
      "[2025-03-14 08:55:41,138] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='_Validate_Tools', type='Method', properties={}) target=Node(id='Validate_Tools_Single_Input', type='Method', properties={}) type='INVOKES' properties={}\n",
      "[2025-03-14 08:55:41,139] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Validate_Tools_Single_Input', type='Method', properties={}) target=Node(id='Basetool', type='Class', properties={}) type='TAKES_PARAMETER' properties={}\n",
      "[2025-03-14 08:55:41,140] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='Validate_Tools_Single_Input', type='Method', properties={}) target=Node(id='Agent', type='Entity', properties={}) type='ENSURES' properties={}\n",
      "[2025-03-14 08:55:41,141] p10912 {406922825.py:2} INFO - document #7\n",
      "[2025-03-14 08:55:41,141] p10912 {406922825.py:6} INFO - Nodes:id='From_Llm_And_Tools' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,142] p10912 {406922825.py:6} INFO - Nodes:id='_Validate_Tools' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,143] p10912 {406922825.py:6} INFO - Nodes:id='Create_Prompt' type='Method' properties={}\n",
      "[2025-03-14 08:55:41,144] p10912 {406922825.py:6} INFO - Nodes:id='Llmchain' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,145] p10912 {406922825.py:6} INFO - Nodes:id='Conversationalagent' type='Class' properties={}\n",
      "[2025-03-14 08:55:41,146] p10912 {406922825.py:6} INFO - Nodes:id='Output_Parser' type='Concept' properties={}\n",
      "[2025-03-14 08:55:41,148] p10912 {406922825.py:6} INFO - Nodes:id='Kwargs' type='Concept' properties={}\n",
      "[2025-03-14 08:55:41,149] p10912 {406922825.py:6} INFO - Nodes:id='Callback_Manager' type='Concept' properties={}\n",
      "[2025-03-14 08:55:41,149] p10912 {406922825.py:6} INFO - Nodes:id='Tools' type='Concept' properties={}\n",
      "[2025-03-14 08:55:41,149] p10912 {406922825.py:6} INFO - Nodes:id='Ai_Prefix' type='Concept' properties={}\n",
      "[2025-03-14 08:55:41,150] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='_Validate_Tools', type='Method', properties={}) type='CALLS' properties={}\n",
      "[2025-03-14 08:55:41,151] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Create_Prompt', type='Method', properties={}) type='CALLS' properties={}\n",
      "[2025-03-14 08:55:41,152] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Llmchain', type='Class', properties={}) type='CREATES' properties={}\n",
      "[2025-03-14 08:55:41,153] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Callback_Manager', type='Concept', properties={}) type='USES' properties={}\n",
      "[2025-03-14 08:55:41,154] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Tools', type='Concept', properties={}) type='USES' properties={}\n",
      "[2025-03-14 08:55:41,155] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Output_Parser', type='Concept', properties={}) type='DETERMINES' properties={}\n",
      "[2025-03-14 08:55:41,157] p10912 {406922825.py:8} INFO - Relationships:source=Node(id='From_Llm_And_Tools', type='Method', properties={}) target=Node(id='Conversationalagent', type='Class', properties={}) type='CREATES' properties={}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(graph_documents):\n",
    "    logger.info(f\"document #{i+1}\")\n",
    "    nodes = doc.nodes\n",
    "    relationships = doc.relationships\n",
    "    for n in nodes:\n",
    "        logger.info(f\"Nodes:{n}\")\n",
    "    for r in relationships:\n",
    "        logger.info(f\"Relationships:{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[Node(id='Conversationalagent', type='Class', properties={}), Node(id='Langchain_Core', type='Library', properties={}), Node(id='Langchain', type='Library', properties={})], relationships=[Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Langchain_Core', type='Library', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Langchain', type='Library', properties={}), type='IMPORTS', properties={})], source=Document(metadata={}, page_content='The provided code implements a class called `ConversationalAgent`, which is designed to facilitate conversational interactions while enabling the usage of various tools. The code starts by importing several packages and modules, including future annotations support for type hints, various core functionalities from the `langchain_core` library, and components from the `langchain` library like agents and callback management.')),\n",
       " GraphDocument(nodes=[Node(id='__Future__', type='Module', properties={}), Node(id='Typing', type='Module', properties={}), Node(id='Langchain_Core', type='Module', properties={}), Node(id='Pydantic', type='Module', properties={}), Node(id='Any', type='Class', properties={}), Node(id='List', type='Class', properties={}), Node(id='Optional', type='Class', properties={}), Node(id='Sequence', type='Class', properties={}), Node(id='Deprecated', type='Function', properties={}), Node(id='Basecallbackmanager', type='Class', properties={}), Node(id='Baselanguagemodel', type='Class', properties={}), Node(id='Prompttemplate', type='Class', properties={}), Node(id='Basetool', type='Class', properties={}), Node(id='Field', type='Class', properties={}), Node(id='Conversationalagent', type='Class', properties={})], relationships=[Relationship(source=Node(id='__Future__', type='Module', properties={}), target=Node(id='__Future__', type='Module', properties={}), type='ENABLED_FORWARD_COMPATIBILITY', properties={}), Relationship(source=Node(id='Typing', type='Module', properties={}), target=Node(id='Any', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Module', properties={}), target=Node(id='List', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Module', properties={}), target=Node(id='Optional', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Typing', type='Module', properties={}), target=Node(id='Sequence', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Deprecated', type='Function', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Basecallbackmanager', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Baselanguagemodel', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Prompttemplate', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Basetool', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Pydantic', type='Module', properties={}), target=Node(id='Field', type='Class', properties={}), type='IMPORTS', properties={}), Relationship(source=Node(id='Langchain_Core', type='Module', properties={}), target=Node(id='Conversationalagent', type='Class', properties={}), type='MARKED_FOR_REMOVAL', properties={})], source=Document(metadata={}, page_content=\"The first imported module is `__future__`, imported with a focus on enabling forward compatibility with future Python versions through annotations. Next, `Any`, `List`, `Optional`, and `Sequence` are imported from the `typing` module to allow for type hinting across the agent's function definitions. The core classes and functions imported from `langchain_core` include `deprecated`, `BaseCallbackManager`, `BaseLanguageModel`, `PromptTemplate`, and `BaseTool`. The use of `deprecated` indicates that the `ConversationalAgent` class is marked for potential removal in an upcoming version of the library. The `Field` class from the `pydantic` module is imported for data validation and class field management.\")),\n",
       " GraphDocument(nodes=[Node(id='Conversationalagent', type='Class', properties={}), Node(id='Agent', type='Class', properties={}), Node(id='Ai_Prefix', type='Attribute', properties={}), Node(id='Output_Parser', type='Attribute', properties={}), Node(id='Convooutputparser', type='Class', properties={})], relationships=[Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Agent', type='Class', properties={}), type='INHERITS_FROM', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Ai_Prefix', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Output_Parser', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Output_Parser', type='Attribute', properties={}), target=Node(id='Convooutputparser', type='Class', properties={}), type='USES', properties={})], source=Document(metadata={}, page_content='The `ConversationalAgent` class inherits from the `Agent` class. The class starts with a deprecation decorator that specifies the version when it was marked deprecated, a warning message, and a version when it will be removed. Within the class, there are two primary attributes defined with default values: `ai_prefix`, which is set to `\"AI\"` and serves as a prefix for AI responses, and `output_parser`, which uses `Field` and is initialized with a default factory that creates an instance of `ConvoOutputParser`, assigning it to format the agent\\'s output.')),\n",
       " GraphDocument(nodes=[Node(id='_Get_Default_Output_Parser', type='Method', properties={}), Node(id='Convooutputparser', type='Class', properties={}), Node(id='Agentoutputparser', type='Class', properties={}), Node(id='_Agent_Type', type='Property', properties={}), Node(id='Agenttype.Conversational_React_Description', type='Enum', properties={}), Node(id='Observation_Prefix', type='Property', properties={}), Node(id='Llm_Prefix', type='Property', properties={})], relationships=[Relationship(source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}), target=Node(id='Convooutputparser', type='Class', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='_Get_Default_Output_Parser', type='Method', properties={}), target=Node(id='Agentoutputparser', type='Class', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='_Agent_Type', type='Property', properties={}), target=Node(id='Agenttype.Conversational_React_Description', type='Enum', properties={}), type='DEFINES', properties={}), Relationship(source=Node(id='Observation_Prefix', type='Property', properties={}), target=Node(id='Observation: ', type='String', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='Llm_Prefix', type='Property', properties={}), target=Node(id='Thought:', type='String', properties={}), type='RETURNS', properties={})], source=Document(metadata={}, page_content='The `_get_default_output_parser` class method returns an instance of `ConvoOutputParser`, with an optional `ai_prefix` parameter defaulting to `\"AI\"`. The type hint indicates that this method returns an instance of `AgentOutputParser`. The `_agent_type` property defines the agent type as `AgentType.CONVERSATIONAL_REACT_DESCRIPTION`, specifying how the agent should be classified. The `observation_prefix` property returns the string `\"Observation: \"` which serves as a prefix for observations made by the agent. Similarly, the `llm_prefix` property returns the string `\"Thought:\"` to prefix AI thoughts.')),\n",
       " GraphDocument(nodes=[Node(id='Create_Prompt', type='Method', properties={}), Node(id='Basetool', type='Class', properties={}), Node(id='Prompttemplate', type='Class', properties={}), Node(id='Input_Variables', type='Parameter', properties={}), Node(id='Chat_History', type='Variable', properties={}), Node(id='Agent_Scratchpad', type='Variable', properties={})], relationships=[Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Basetool', type='Class', properties={}), type='USES', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Input_Variables', type='Parameter', properties={}), type='CUSTOMIZES', properties={}), Relationship(source=Node(id='Create_Prompt', type='Method', properties={}), target=Node(id='Prompttemplate', type='Class', properties={}), type='RETURNS', properties={}), Relationship(source=Node(id='Input_Variables', type='Parameter', properties={}), target=Node(id='Chat_History', type='Variable', properties={}), type='DEFAULTS_TO', properties={}), Relationship(source=Node(id='Input_Variables', type='Parameter', properties={}), target=Node(id='Agent_Scratchpad', type='Variable', properties={}), type='DEFAULTS_TO', properties={})], source=Document(metadata={}, page_content='The `create_prompt` class method assembles a prompt template based on the provided tools while allowing customization of its format through multiple parameters. The `tools` parameter expects a sequence of `BaseTool` instances that the agent can utilize. The method constructs a string representation for each tool, combining their names and descriptions. Then, it joins these strings with line breaks and formats the instructions using provided parameters and any defaults. If `input_variables` is not specified, it defaults to a list containing `\"input\"`, `\"chat_history\"`, and `\"agent_scratchpad\"`. Ultimately, the method returns a `PromptTemplate` instance with the assembled template and input variables.')),\n",
       " GraphDocument(nodes=[Node(id='_Validate_Tools', type='Method', properties={}), Node(id='Parent Class', type='Class', properties={}), Node(id='Validate_Tools_Single_Input', type='Method', properties={}), Node(id='Basetool', type='Class', properties={}), Node(id='Agent', type='Entity', properties={})], relationships=[Relationship(source=Node(id='_Validate_Tools', type='Method', properties={}), target=Node(id='Parent Class', type='Class', properties={}), type='INVOKES', properties={}), Relationship(source=Node(id='_Validate_Tools', type='Method', properties={}), target=Node(id='Validate_Tools_Single_Input', type='Method', properties={}), type='INVOKES', properties={}), Relationship(source=Node(id='Validate_Tools_Single_Input', type='Method', properties={}), target=Node(id='Basetool', type='Class', properties={}), type='TAKES_PARAMETER', properties={}), Relationship(source=Node(id='Validate_Tools_Single_Input', type='Method', properties={}), target=Node(id='Agent', type='Entity', properties={}), type='ENSURES', properties={})], source=Document(metadata={}, page_content=\"Following this, the `_validate_tools` class method invokes the parent class's tool validation and additionally invokes `validate_tools_single_input`, ensuring a single input format for tools is adhered to by the agent. This method takes a sequence of `BaseTool` items as its only parameter.\")),\n",
       " GraphDocument(nodes=[Node(id='From_Llm_And_Tools', type='Method', properties={}), Node(id='_Validate_Tools', type='Method', properties={}), Node(id='Create_Prompt', type='Method', properties={}), Node(id='Llmchain', type='Class', properties={}), Node(id='Conversationalagent', type='Class', properties={}), Node(id='Output_Parser', type='Concept', properties={}), Node(id='Kwargs', type='Concept', properties={}), Node(id='Callback_Manager', type='Concept', properties={}), Node(id='Tools', type='Concept', properties={}), Node(id='Ai_Prefix', type='Concept', properties={})], relationships=[Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='_Validate_Tools', type='Method', properties={}), type='CALLS', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Create_Prompt', type='Method', properties={}), type='CALLS', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Llmchain', type='Class', properties={}), type='CREATES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Callback_Manager', type='Concept', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Tools', type='Concept', properties={}), type='USES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Output_Parser', type='Concept', properties={}), type='DETERMINES', properties={}), Relationship(source=Node(id='From_Llm_And_Tools', type='Method', properties={}), target=Node(id='Conversationalagent', type='Class', properties={}), type='CREATES', properties={})], source=Document(metadata={}, page_content='The `from_llm_and_tools` class method acts as a factory method for constructing an agent using a language model and a collection of tools. It begins by calling `_validate_tools` to ensure that the provided tools are valid. Then, it calls the `create_prompt` method to generate a prompt, which is used to instantiate an `LLMChain`. The `llm_chain` is created by passing the language model, the generated prompt, and a callback manager if specified. After gathering the names of the tools into a list, an output parser is determined either from the specified input or through the `_get_default_output_parser` method. Finally, this method returns an instance of the `ConversationalAgent`, initializing it with the composed `llm_chain`, list of allowed tools, AI prefix, output parser, and any additional keyword arguments passed through `**kwargs`. The code ends with a few comments noting the specific arguments and giving insight into the return type of the function.'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_graph import plot_network\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# nodes = [str(node) for graph in graph_documents for node in graph.nodes]\n",
    "# relationships = [(str(rel.source), str(rel.target)) for graph in graph_documents for rel in graph.relationships]\n",
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_edges_from(relationships)\n",
    "\n",
    "# custom_colors = sns.color_palette(\"Set2\", n_colors=len(G.nodes()))\n",
    "# node_sizes = [3000 if d > 5 else 1000 for v, d in G.degree()]\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plot_network(\n",
    "#     G,\n",
    "#     node_size=node_sizes,\n",
    "#     node_color=custom_colors,\n",
    "#     edge_color=\"#cccccc\",\n",
    "#     font_size=10,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"Set2\",\n",
    "#     k=0.1,\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "\n",
    "# net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# for graph in graph_documents:\n",
    "#     for rel in graph.relationships:\n",
    "#         net.add_node(str(rel.source))\n",
    "#         net.add_node(str(rel.target))\n",
    "#         net.add_edge(str(rel.source), str(rel.target))\n",
    "\n",
    "# # Save and display\n",
    "# net.save_graph(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph_documents[0].relationships[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(a,'type',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relationship(source=Node(id='Conversationalagent', type='Class', properties={}), target=Node(id='Langchain_Core', type='Library', properties={}), type='IMPORTS', properties={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents[0].relationships[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Pyvis network\n",
    "net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# Create a NetworkX graph for analysis\n",
    "G = nx.Graph()\n",
    "\n",
    "# Dictionary to store node attributes\n",
    "node_types = {}\n",
    "\n",
    "# Add nodes and edges, extracting types\n",
    "for graph in graph_documents:\n",
    "    for rel in graph.relationships:\n",
    "        source, target = str(rel.source.id), str(rel.target.id)\n",
    "        source_type = rel.source.type\n",
    "        target_type = rel.target.type\n",
    "        rel_type = rel.type  # Relationship type (e.g., 'IMPORTS')\n",
    "\n",
    "        # Store node types\n",
    "        node_types[source] = source_type\n",
    "        node_types[target] = target_type\n",
    "\n",
    "        # Add nodes if not already added\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "\n",
    "        # Add edge with label\n",
    "        G.add_edge(source, target, label=rel_type)\n",
    "\n",
    "# Get unique node types and assign colors\n",
    "unique_types = list(set(node_types.values()))\n",
    "color_map = plt.get_cmap(\"tab10\")  # Use a categorical colormap\n",
    "type_colors = {t: color_map(i / len(unique_types)) for i, t in enumerate(unique_types)}\n",
    "\n",
    "# Convert colors to RGBA format\n",
    "type_colors_rgba = {\n",
    "    t: f'rgba({int(c[0] * 255)}, {int(c[1] * 255)}, {int(c[2] * 255)}, 0.8)' for t, c in type_colors.items()\n",
    "}\n",
    "\n",
    "# Determine node sizes based on degrees\n",
    "degrees = dict(G.degree())\n",
    "min_size, max_size = 10, 50\n",
    "size_scale = {node: min_size + (max_size - min_size) * (deg / max(degrees.values())) for node, deg in degrees.items()}\n",
    "\n",
    "# Add nodes with dynamic colors and sizes\n",
    "for node in G.nodes():\n",
    "    node_type = node_types.get(node, \"default\")\n",
    "    net.add_node(\n",
    "        node,\n",
    "        label=node,  # Show node ID\n",
    "        size=size_scale[node],  # Adjust size\n",
    "        color=type_colors_rgba.get(node_type, \"gray\")  # Assign color based on type\n",
    "    )\n",
    "\n",
    "# Add edges with labels for relationship type\n",
    "for edge in G.edges(data=True):\n",
    "    source, target, attr = edge\n",
    "    rel_label = attr.get(\"label\", \"\")  # Get relationship type\n",
    "    net.add_edge(source, target, title=rel_label, label=rel_label)  # Show label on hover and as text\n",
    "\n",
    "# Save and display\n",
    "net.save_graph(\"graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/graphs/memgraph/\n",
    "\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "import os\n",
    "\n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    " \n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 08:58:41,533] p10912 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the database is empty\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    " \n",
    "# Create KG\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 08:58:41,775] p10912 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node labels and properties (name and type) are:\n",
      "- labels: (:Module)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Library)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Function)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Method)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Attribute)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Property)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Concept)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Enum)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Class)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Parameter)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Entity)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Variable)\n",
      "  properties:\n",
      "    - id: string\n",
      "\n",
      "Nodes are connected with the following relationships:\n",
      "(:Method)-[:USES]->(:Concept)\n",
      "(:Method)-[:DETERMINES]->(:Attribute)\n",
      "(:Method)-[:ENSURES]->(:Class)\n",
      "(:Method)-[:CALLS]->(:Method)\n",
      "(:Method)-[:INVOKES]->(:Method)\n",
      "(:Method)-[:INVOKES]->(:Class)\n",
      "(:Module)-[:IMPORTS]->(:Function)\n",
      "(:Method)-[:RETURNS]->(:Class)\n",
      "(:Class)-[:INHERITS_FROM]->(:Class)\n",
      "(:Method)-[:CREATES]->(:Class)\n",
      "(:Class)-[:HAS_ATTRIBUTE]->(:Attribute)\n",
      "(:Module)-[:MARKED_FOR_REMOVAL]->(:Class)\n",
      "(:Method)-[:TAKES_PARAMETER]->(:Class)\n",
      "(:Property)-[:DEFINES]->(:Enum)\n",
      "(:Library)-[:MARKED_FOR_REMOVAL]->(:Class)\n",
      "(:Method)-[:DETERMINES]->(:Concept)\n",
      "(:Class)-[:IMPORTS]->(:Library)\n",
      "(:Attribute)-[:USES]->(:Class)\n",
      "(:Library)-[:IMPORTS]->(:Function)\n",
      "(:Library)-[:IMPORTS]->(:Class)\n",
      "(:Method)-[:CUSTOMIZES]->(:Parameter)\n",
      "(:Method)-[:USES]->(:Class)\n",
      "(:Module)-[:ENABLED_FORWARD_COMPATIBILITY]->(:Module)\n",
      "(:Method)-[:ENSURES]->(:Entity)\n",
      "(:Parameter)-[:DEFAULTS_TO]->(:Variable)\n",
      "(:Property)-[:RETURNS]->(:)\n",
      "(:Module)-[:IMPORTS]->(:Class)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"\n",
    "Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\",temperature=0)\n",
    "schema = graph.schema\n",
    "\n",
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not use atomic operations in your Cypher queries.\n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "MEMGRAPH_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=MEMGRAPH_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    llm,\n",
    "    cypher_prompt=MEMGRAPH_GENERATION_PROMPT,\n",
    "    graph=graph,\n",
    "    model_id=\"gpt-4o-turbo\",\n",
    "    return_intermediate_steps=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-14 09:30:56,520] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-14 09:30:56,786] p10912 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Steps :  [{'query': \"MATCH (m:Method)-[:ENSURES]->(c:Class {id: 'Conversationalagent'}) RETURN m\"}, {'context': []}]\n",
      "Final Response :  I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "q = \"what methods are related to class Conversationalagent?\"\n",
    "response = chain.invoke(q)\n",
    "print(\"Intermediate Steps : \", response['intermediate_steps'])\n",
    "print(\"Final Response : \", response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c:Class {id: 'Conversationalagent'})-[:TAKES_PARAMETER]->(p:Parameter) RETURN p.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 10:23:22,446] p15296 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "MATCH (n)-[r]->(m) WHERE n.id = 'Conversationalagent' RETURN n, r, m\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'IMPORTS', {'id': '__Future__.Annotations'}), 'm': {'id': '__Future__.Annotations'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'IMPORTS', {'id': 'Typing'}), 'm': {'id': 'Typing'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'IMPORTS', {'id': 'Langchain_Core'}), 'm': {'id': 'Langchain_Core'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'IMPORTS', {'id': 'Pydantic'}), 'm': {'id': 'Pydantic'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'DERIVES_FROM', {'id': 'Agent'}), 'm': {'id': 'Agent'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'PASSING', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'PASSING', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'PASSING', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'PASSING', {'id': 'Output_Parser'}), 'm': {'id': 'Output_Parser'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 10:23:23,509] p15296 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is related to the id Conversationalagent',\n",
       " 'result': 'The id Conversationalagent is related to the following: it imports Future.Annotations, Typing, Langchain_Core, and Pydantic. It derives from Agent and has the attribute Ai_Prefix. Additionally, it is involved in passing Ai_Prefix and Output_Parser.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "qa_chainn = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True,allow_dangerous_requests=True)\n",
    "response = qa_chainn.invoke({\"query\": \"what is related to the id Conversationalagent\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'EXTENDS', {'id': 'Agent'}),\n",
       "  'n': {'id': 'Agent'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'USES', {'id': 'Convooutputparser'}),\n",
       "  'n': {'id': 'Convooutputparser'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'HAS_PROPERTY', {'id': 'Ai_Prefix'}),\n",
       "  'n': {'id': 'Ai_Prefix'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'HAS_PROPERTY',\n",
       "   {'id': 'Output_Parser'}),\n",
       "  'n': {'id': 'Output_Parser'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'USES', {'id': '@Deprecated'}),\n",
       "  'n': {'id': '@Deprecated'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'USES',\n",
       "   {'id': 'Agent_Deprecation_Warning'}),\n",
       "  'n': {'id': 'Agent_Deprecation_Warning'}}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c)-[r]->(n) WHERE c.id = 'Conversationalagent' RETURN c, r, n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "# How many people played in Top Gun?\n",
    "MATCH (m:Movie {{name:\"Top Gun\"}})<-[:ACTED_IN]-()\n",
    "RETURN count(*) AS numberOfActors\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_transformer_filtered = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Nationality\", \"Concept\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"INVOLVED_IN\", \"COLLABORATES_WITH\"],\n",
    "# )\n",
    "# graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "#     documents\n",
    "# )\n",
    "\n",
    "# print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents_filtered[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \\n to \\\\n\n",
    "\n",
    "code = '''\n",
    "# An agent designed to hold a conversation in addition to using tools.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "from langchain_core._api import deprecated\n",
    "from langchain_core.callbacks import BaseCallbackManager\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import Field\n",
    "\n",
    "from langchain._api.deprecation import AGENT_DEPRECATION_WARNING\n",
    "from langchain.agents.agent import Agent, AgentOutputParser\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n",
    "from langchain.agents.utils import validate_tools_single_input\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "@deprecated(\n",
    "    \"0.1.0\",\n",
    "    message=AGENT_DEPRECATION_WARNING,\n",
    "    removal=\"1.0\",\n",
    ")\n",
    "class ConversationalAgent(Agent):\n",
    "    \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\n",
    "\n",
    "    ai_prefix: str = \"AI\"\n",
    "    \"\"\"Prefix to use before AI output.\"\"\"\n",
    "    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\n",
    "    \"\"\"Output parser for the agent.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_default_output_parser(\n",
    "        cls, ai_prefix: str = \"AI\", **kwargs: Any\n",
    "    ) -> AgentOutputParser:\n",
    "        return ConvoOutputParser(ai_prefix=ai_prefix)\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        \"\"\"Return Identifier of agent type.\"\"\"\n",
    "        return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n",
    "\n",
    "    @property\n",
    "    def observation_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the observation with.\n",
    "\n",
    "        Returns:\n",
    "            \"Observation: \"\n",
    "        \"\"\"\n",
    "        return \"Observation: \"\n",
    "\n",
    "    @property\n",
    "    def llm_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the llm call with.\n",
    "\n",
    "        Returns:\n",
    "            \"Thought: \"\n",
    "        \"\"\"\n",
    "        return \"Thought:\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_prompt(\n",
    "        cls,\n",
    "        tools: Sequence[BaseTool],\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "    ) -> PromptTemplate:\n",
    "        \"\"\"Create prompt in the style of the zero-shot agent.\n",
    "\n",
    "        Args:\n",
    "            tools: List of tools the agent will have access to, used to format the\n",
    "                prompt.\n",
    "            prefix: String to put before the list of tools. Defaults to PREFIX.\n",
    "            suffix: String to put after the list of tools. Defaults to SUFFIX.\n",
    "            format_instructions: Instructions on how to use the tools. Defaults to\n",
    "                FORMAT_INSTRUCTIONS\n",
    "            ai_prefix: String to use before AI output. Defaults to \"AI\".\n",
    "            human_prefix: String to use before human output.\n",
    "                Defaults to \"Human\".\n",
    "            input_variables: List of input variables the final prompt will expect.\n",
    "                Defaults to [\"input\", \"chat_history\", \"agent_scratchpad\"].\n",
    "\n",
    "        Returns:\n",
    "            A PromptTemplate with the template assembled from the pieces here.\n",
    "        \"\"\"\n",
    "        tool_strings = \"\\\\n\".join(\n",
    "            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n",
    "        )\n",
    "        tool_names = \", \".join([tool.name for tool in tools])\n",
    "        format_instructions = format_instructions.format(\n",
    "            tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\n",
    "        )\n",
    "        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "        if input_variables is None:\n",
    "            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
    "        return PromptTemplate(template=template, input_variables=input_variables)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "        super()._validate_tools(tools)\n",
    "        validate_tools_single_input(cls.__name__, tools)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        output_parser: Optional[AgentOutputParser] = None,\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Agent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "        Args:\n",
    "            llm: The language model to use.\n",
    "            tools: A list of tools to use.\n",
    "            callback_manager: The callback manager to use. Default is None.\n",
    "            output_parser: The output parser to use. Default is None.\n",
    "            prefix: The prefix to use in the prompt. Default is PREFIX.\n",
    "            suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
    "            format_instructions: The format instructions to use.\n",
    "                Default is FORMAT_INSTRUCTIONS.\n",
    "            ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
    "            human_prefix: The prefix to use before human output.\n",
    "                Default is \"Human\".\n",
    "            input_variables: The input variables to use. Default is None.\n",
    "            **kwargs: Any additional keyword arguments to pass to the agent.\n",
    "\n",
    "        Returns:\n",
    "            An agent.\n",
    "        \"\"\"\n",
    "        cls._validate_tools(tools)\n",
    "        prompt = cls.create_prompt(\n",
    "            tools,\n",
    "            ai_prefix=ai_prefix,\n",
    "            human_prefix=human_prefix,\n",
    "            prefix=prefix,\n",
    "            suffix=suffix,\n",
    "            format_instructions=format_instructions,\n",
    "            input_variables=input_variables,\n",
    "        )\n",
    "        llm_chain = LLMChain(  # type: ignore[misc]\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            callback_manager=callback_manager,\n",
    "        )\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        _output_parser = output_parser or cls._get_default_output_parser(\n",
    "            ai_prefix=ai_prefix\n",
    "        )\n",
    "        return cls(\n",
    "            llm_chain=llm_chain,\n",
    "            allowed_tools=tool_names,\n",
    "            ai_prefix=ai_prefix,\n",
    "            output_parser=_output_parser,\n",
    "            **kwargs,\n",
    "        )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: ['__copy__', '__deepcopy__', '__delattr__', '__eq__', '__getattr__', '__getstate__', '__init__', '__iter__', '__pretty__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__str__', '_calculate_keys', '_check_frozen', '_construct_scratchpad', '_copy_and_set_values', '_fix_text', '_iter', 'aplan', 'copy', 'dict', 'get_allowed_tools', 'get_full_inputs', 'json', 'model_copy', 'model_dump', 'model_dump_json', 'model_post_init', 'plan', 'return_stopped_response', 'save', 'tool_run_logging_kwargs', 'validate_prompt']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n",
    "# List all methods of the class\n",
    "methods = inspect.getmembers(ConversationalAgent, predicate=inspect.isfunction)\n",
    "print(\"Methods:\", [method[0] for method in methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agent_code' from 'c:\\\\Users\\\\JaeHoBahng\\\\Desktop\\\\Georgetown\\\\2025_Spring\\\\DSAN_6725\\\\project\\\\spring-2025-final-project-project-group-2\\\\docs\\\\notebooks\\\\agent_code.py'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the full source code of a specific method\n",
    "def get_method_source(class_obj, method_name):\n",
    "    \"\"\"Retrieve the full source code of a method from a class.\"\"\"\n",
    "    try:\n",
    "        method = getattr(class_obj, method_name)\n",
    "        return inspect.getsource(method)\n",
    "    except AttributeError:\n",
    "        return f\"Method '{method_name}' not found.\"\n",
    "    except TypeError:\n",
    "        return f\"Could not retrieve source for '{method_name}'.\"\n",
    "\n",
    "# Example Usage: Get the code for a specific method\n",
    "method_name = \"from_llm_and_tools\"  # Change this to the method you want\n",
    "method_code = get_method_source(ConversationalAgent, method_name)\n",
    "\n",
    "print(f\"\\nCode for method '{method_name}':\\n\")\n",
    "print(method_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_source(module, class_name):\n",
    "    \"\"\"Retrieve the source code of a class if it exists in a module.\"\"\"\n",
    "    class_obj = getattr(module, class_name, None)\n",
    "    if class_obj:\n",
    "        try:\n",
    "            return inspect.getsource(class_obj)\n",
    "        except TypeError:\n",
    "            return f\"Could not retrieve source for class '{class_name}'.\"\n",
    "    return None\n",
    "\n",
    "def search_for_class(root_dir, class_name=\"ConversationalAgent\"):\n",
    "    \"\"\"Search for the given class in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if module:\n",
    "            class_source = get_class_source(module, class_name)\n",
    "            if class_source:\n",
    "                print(f\"Class '{class_name}' found in: {file}\\n\")\n",
    "                print(class_source)\n",
    "                return  # Stop after finding the first occurrence\n",
    "    print(f\"Class '{class_name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"/path/to/your/root_repo\"  # Change this to your actual repo path\n",
    "search_for_class(root_repo_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
