{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def plot_network(\n",
    "#     G,\n",
    "#     node_size=1000,\n",
    "#     node_color=\"skyblue\",\n",
    "#     edge_color=\"gray\",\n",
    "#     font_size=10,\n",
    "#     title=\"Network Graph\",\n",
    "#     figsize=(12, 8),\n",
    "#     with_labels=True,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"husl\",\n",
    "#     k=0.1,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plot a network graph with seaborn-style aesthetics.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     G : networkx.Graph\n",
    "#         The network graph to visualize\n",
    "#     node_size : int or list\n",
    "#         Size of nodes (can be a single value or list for different sizes)\n",
    "#     node_color : str or list\n",
    "#         Color of nodes (can be a single value or list for different colors)\n",
    "#     edge_color : str\n",
    "#         Color of edges\n",
    "#     font_size : int\n",
    "#         Size of node labels\n",
    "#     title : str\n",
    "#         Title of the plot\n",
    "#     figsize : tuple\n",
    "#         Figure size (width, height)\n",
    "#     with_labels : bool\n",
    "#         Whether to show node labels\n",
    "#     layout : str\n",
    "#         Type of layout ('spring', 'circular', 'random', 'shell')\n",
    "#     palette : str\n",
    "#         Seaborn color palette to use if node_color is not specified\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     fig, ax : tuple\n",
    "#         Matplotlib figure and axis objects\n",
    "#     \"\"\"\n",
    "#     # Set the style\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "\n",
    "#     # Create figure\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "#     # Choose layout\n",
    "#     layouts = {\n",
    "#         \"spring\": nx.spring_layout,\n",
    "#         \"circular\": nx.circular_layout,\n",
    "#         \"random\": nx.random_layout,\n",
    "#         \"shell\": nx.shell_layout,\n",
    "#     }\n",
    "#     pos = layouts.get(layout, nx.spring_layout)(G,k)\n",
    "\n",
    "#     # If node_color is not specified, use seaborn palette\n",
    "#     if isinstance(node_color, str) and node_color == \"skyblue\":\n",
    "#         colors = sns.color_palette(palette, n_colors=len(G.nodes()))\n",
    "#     else:\n",
    "#         colors = node_color\n",
    "\n",
    "#     # Draw the network\n",
    "#     nx.draw(\n",
    "#         G,\n",
    "#         pos,\n",
    "#         node_color=colors,\n",
    "#         node_size=node_size,\n",
    "#         edge_color=edge_color,\n",
    "#         with_labels=with_labels,\n",
    "#         font_size=font_size,\n",
    "#         font_weight=\"bold\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "\n",
    "#     # Add title\n",
    "#     plt.title(title, fontsize=font_size + 4, pad=20)\n",
    "\n",
    "#     return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: USING langchain_experimental.graph_transformers main.\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# # TypedDict\n",
    "# class Json(TypedDict):\n",
    "#     \"\"\"Json to return.\"\"\"\n",
    "\n",
    "#     setup: Annotated[dict, ..., \"The setup of the dict\"]\n",
    "#     depth: Annotated[int, ..., \"How many layers deep the dict is.\"]\n",
    "\n",
    "\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# text = \"\"\"\n",
    "# Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "# She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "# Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "# She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "# \"\"\"\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "# for i, doc in enumerate(graph_documents):\n",
    "#     logger.info(f\"document #{i+1}\")\n",
    "#     nodes = doc.nodes\n",
    "#     relationships = doc.relationships\n",
    "#     for n in nodes:\n",
    "#         logger.info(f\"Nodes:{n}\")\n",
    "#     for r in relationships:\n",
    "#         logger.info(f\"Relationships:{r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_graph import plot_network\n",
    "# import networkx as nx\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# nodes = [str(node) for node in graph_documents[0].nodes]\n",
    "# relationships = [\n",
    "#     (str(rel.source), str(rel.target)) for rel in graph_documents[0].relationships\n",
    "# ]\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_edges_from(relationships)\n",
    "\n",
    "# custom_colors = sns.color_palette(\"Set2\", n_colors=len(G.nodes()))\n",
    "# node_sizes = [3000 if d > 5 else 1000 for v, d in G.degree()]\n",
    "\n",
    "# fig, ax = plot_network(\n",
    "#     G,\n",
    "#     node_size=node_sizes,\n",
    "#     node_color=custom_colors,\n",
    "#     edge_color=\"#cccccc\",\n",
    "#     font_size=12,\n",
    "#     layout=\"spring\",\n",
    "#     palette=\"Set2\",\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/DSAN6700-24Fall/assignment-2-mcdonald-s.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── src/\n",
      "│   ├── deduplication/\n",
      "│   │   ├── bloom_filter.py\n",
      "│   │   ├── dedup.py\n",
      "│   │   ├── LSH.py\n",
      "│   │   ├── LSHForest.py\n",
      "│   │   ├── LSHImproved.py\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── __main__.py\n",
      "│   ├── utils/\n",
      "│   │   ├── use_cases.py\n",
      "│   │   ├── utils.py\n",
      "│   │   ├── visualizations.py\n",
      "│   │   ├── visualization_lsh.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def generate_repo_tree(repo_path, indent=\"\"):\n",
    "    tree_string = \"\"\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        # Filter out __pycache__ and hidden directories\n",
    "        dirs[:] = [d for d in dirs if d != \"__pycache__\" and not d.startswith(\".\")]\n",
    "        files = [f for f in files if not f.startswith(\".\")]\n",
    "\n",
    "        level = root.replace(repo_path, \"\").count(os.sep)\n",
    "        indent = \"│   \" * level + \"├── \"  # Formatting the tree\n",
    "        tree_string += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "\n",
    "        sub_indent = \"│   \" * (level + 1) + \"├── \"\n",
    "        for file in files:\n",
    "            tree_string += f\"{sub_indent}{file}\\n\"\n",
    "\n",
    "    return tree_string\n",
    "\n",
    "# Set your repo path\n",
    "repo_path = \"./assignment-2-mcdonald-s/src\"  # Change this to your cloned repo path\n",
    "\n",
    "# Generate tree and store as string\n",
    "repo_tree_string = generate_repo_tree(repo_path)\n",
    "\n",
    "# Print the repo tree\n",
    "print(repo_tree_string)\n",
    "\n",
    "# Store it as a variable to feed into an LLM\n",
    "llm_input = f\"Here is the repository structure:\\n{repo_tree_string}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\bloom_filter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:07,589] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\dedup.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:19,296] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\LSH.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:34,428] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\LSHForest.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:37,851] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\LSHImproved.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:54,156] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:27:55,095] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\deduplication\\__main__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:28:05,232] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\utils\\use_cases.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:28:11,280] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\utils\\utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:28:26,774] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\utils\\visualizations.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:28:35,797] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./assignment-2-mcdonald-s/src\\utils\\visualization_lsh.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:28:42,383] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are an expert in analyzing Python code and generating structured natural language descriptions for graph-based querying in Cypher. Given a Python codebase, extract meaningful relationships between functions, classes, parameters, and imported modules. \n",
    "    You will be given a repository tree structure and a specific file and a code of the specific file.\n",
    "    \n",
    "    Return the given code as natural language description suitable for cypher querying.\n",
    "    Only explain relationships between functions, classes, parameters, and imported modules.\n",
    "    Do not give explanations for the code logic or functionality.\n",
    "    Do not use adjectives and adverbs. \n",
    "    Only describe the code and do not give an overall summary.\n",
    "    Do not use ambiguous pronouns and use exact names in every description.\n",
    "    Explain each class, function, and variable separately and do not include explanations such as 'as mentioned before' or anything that refers to a previous explanation.\n",
    "\n",
    "    Each outermost class or method should be connected to the source file referred as modules\n",
    "        - Example(Class LSH is defined in module LSH, method deduplication is defined in Module LSH)\n",
    "    Each imported package should be connected to the function, method or class where it was used.\n",
    "\n",
    "    Use the structure so that the node id, type and property are used in the format '{id} which is a {type} with property {property}'.\n",
    "    The property does not always have to be mentioned.\n",
    "    Only use the list of types provided below:\n",
    "    - class\n",
    "    - method\n",
    "    - function\n",
    "    - package\n",
    "    - module\n",
    "    - variable\n",
    "\n",
    "\n",
    "    Example Output (Structured Natural Language):\n",
    "    LSH is a module that defines the LSH which is a class, which consists of hash_function which is a method.\n",
    "    LSH which is a class inherits from BaseLSH which is a module.\n",
    "    numpy which is a package is imported and used in hash_function which is a method.\n",
    "    \n",
    "\n",
    "    Provide a structured response that allows for easy conversion into Cypher queries.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "description = \"\"\n",
    "\n",
    "for root, dirs, files in os.walk(repo_path):\n",
    "    \n",
    "\n",
    "    # Skip hidden directories (e.g., .git, .idea, __pycache__)\n",
    "    dirs[:] = [d for d in dirs if not d.startswith(\".\") and d != \"__pycache__\"]\n",
    "    \n",
    "    for file in files:\n",
    "        i += 1\n",
    "\n",
    "        if not file.startswith(\".\"):  # Skip hidden files\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(file_path)  # Do whatever you want with each file\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lsh_code = file.read()\n",
    "        \n",
    "        # print(lsh_code)\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                system_prompt\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                f'''\n",
    "                Tree:\n",
    "                {repo_tree_string}\n",
    "\n",
    "                Current File:\n",
    "                {file_path}\n",
    "\n",
    "                Code:\n",
    "                {lsh_code}\n",
    "                '''\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # structured_llm = llm.with_structured_output(Json)\n",
    "        response = llm.invoke(messages)\n",
    "        description += response.content + \" \"\n",
    "        # print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file bloom_filter.py is a module that defines the BloomFilter which is a class. \n",
      "\n",
      "BloomFilter is a class that has an __init__ method, which is a method that initializes the BloomFilter class with parameters n and f. n is an argument that represents the maximum number of elements. f is an argument that represents the desired false positive rate. BloomFilter contains an add method, which is a method that adds an item of type str to the BloomFilter. BloomFilter also contains a query method, which is a method that checks if an item of type str might be in the BloomFilter.\n",
      "\n",
      "math is a package that is imported and used in the __init__ method and add method of the BloomFilter class. \n",
      "bitarray is a package that is imported and used in the __init__ method of the BloomFilter class. \n",
      "mmh3 is a package that is imported and used in the add method and query method of the BloomFilter class. \n",
      "ngrams is a package that is imported from nltk and used in the add method and query method of the BloomFilter class. \n",
      "random is a package that is imported and used in the __init__ method and add method of the BloomFilter_Uni_Hash class.\n",
      "\n",
      "BloomFilter_KM_Opt is a class that has an __init__ method, which is a method that initializes the BloomFilter_KM_Opt class with parameters n and f. BloomFilter_KM_Opt contains an add method, which is a method that adds an item of type str to the BloomFilter_KM_Opt. BloomFilter_KM_Opt also contains a query method, which is a method that checks if an item of type str might be in the BloomFilter_KM_Opt.\n",
      "\n",
      "BloomFilter_Uni_Hash is a class that has an __init__ method, which is a method that initializes the BloomFilter_Uni_Hash class with parameters n and f. BloomFilter_Uni_Hash contains an add method, which is a method that adds an item of type str to the BloomFilter_Uni_Hash. BloomFilter_Uni_Hash also contains a query method, which is a method that checks if an item of type str might be in the BloomFilter_Uni_Hash.\n",
      "\n",
      "BloomFilter_QF is a class that has an __init__ method, which is a method that initializes the BloomFilter_QF class with parameters n and f. BloomFilter_QF contains a _hash method, which is a method that hashes an item. BloomFilter_QF also contains an add method, which is a method that adds an item of type str to the BloomFilter_QF. BloomFilter_QF also contains a query method, which is a method that checks if an item of type str might be in the BloomFilter_QF.\n",
      "\n",
      "BloomFilter_KM_Opt, BloomFilter_Uni_Hash, and BloomFilter_QF classes do not import any additional packages. dedup which is a module defines Baseline which is a class that consists of __init__ which is a method.\n",
      "Baseline which is a class has hash_set which is a variable.\n",
      "Baseline which is a class consists of collection_deduplication which is a method that has documents_dict which is an argument.\n",
      "collection_deduplication which is a method uses defaultdict which is a package and creates hash_to_docs which is a variable.\n",
      "collection_deduplication which is a method uses hashlib which is a package.\n",
      "collection_deduplication which is a method has clusters which is a variable.\n",
      "collection_deduplication which is a method has cluster_id which is a variable.\n",
      "Baseline which is a class consists of detect_duplicates which is a method that has documents which is an argument.\n",
      "detect_duplicates which is a method has duplicates which is a variable.\n",
      "detect_duplicates which is a method has hash_set which is a variable.\n",
      "detect_duplicates which is a method uses hashlib which is a package.\n",
      "Baseline which is a class consists of length_based_baseline which is a method that has documents which is an argument.\n",
      "length_based_baseline which is a method has length_map which is a variable.\n",
      "length_based_baseline which is a method has duplicates which is a variable.\n",
      "Baseline which is a class consists of tokenize which is a method that has document which is an argument.\n",
      "tokenize which is a method uses Counter which is a package.\n",
      "tokenize which is a method has tokens which is a variable.\n",
      "Baseline which is a class consists of word_count_baseline which is a method that has documents which is an argument and threshold which is an argument.\n",
      "word_count_baseline which is a method has duplicates which is a variable.\n",
      "word_count_baseline which is a method uses Counter which is a package.\n",
      "word_count_baseline which is a method has doc1_tokens which is a variable.\n",
      "word_count_baseline which is a method has doc2_tokens which is a variable.\n",
      "word_count_baseline which is a method has total_words which is a variable.\n",
      "word_count_baseline which is a method has common_words which is a variable.\n",
      "word_count_baseline which is a method has overlap_ratio which is a variable. hashlib is a package that is imported and used in the LSH which is a class defined in the LSH module.  \n",
      "re is a package that is imported and used in the LSH which is a class defined in the LSH module.  \n",
      "defaultdict is a package that is imported and used in the LSH which is a class defined in the LSH module.  \n",
      "combinations is a package that is imported and used in the LSH which is a class defined in the LSH module.  \n",
      "clean_document is a function that is imported from utils.utils and used in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "shingle is a function that is imported from utils.utils and used in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "minhash is a function that is imported from utils.utils and used in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "Parallel is a package that is imported from joblib and used in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "delayed is a package that is imported from joblib and used in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "LSH is a class that consists of __init__ which is a method.  \n",
      "LSH is a class that consists of remove_duplicates which is a method.  \n",
      "LSH is a class that consists of compute_minhash_signatures which is a method.  \n",
      "LSH is a class that consists of banding which is a method.  \n",
      "__init__ is a method that has num_hashes which is an argument.  \n",
      "__init__ is a method that has num_bands which is an argument.  \n",
      "__init__ is a method that has rows_per_band which is an argument.  \n",
      "__init__ is a method that has k which is an argument.  \n",
      "remove_duplicates is a method that has docs which is an argument.  \n",
      "compute_minhash_signatures is a method that has docs which is an argument.  \n",
      "banding is a method that has signatures which is an argument.  \n",
      "num_hashes is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "num_bands is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "rows_per_band is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "index is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "unique_docs is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "cleaned_docs is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "candidate_pairs is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "exact_duplicates is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "k is a variable that is defined in the __init__ method which is a method of the LSH class.  \n",
      "shingle_sets is a variable that is defined in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "signatures is a variable that is defined in the compute_minhash_signatures method which is a method of the LSH class.  \n",
      "original_id is a variable that is defined in the remove_duplicates method which is a method of the LSH class.  \n",
      "seen_docs is a variable that is defined in the remove_duplicates method which is a method of the LSH class.  \n",
      "band is a variable that is defined in the banding method which is a method of the LSH class.  \n",
      "doc_id is a variable that is defined in the remove_duplicates method which is a method of the LSH class.  \n",
      "doc is a variable that is defined in the remove_duplicates method which is a method of the LSH class.  \n",
      "sig is a variable that is defined in the banding method which is a method of the LSH class.   LSHForest is a module that defines the LSHForest which is a class that inherits from LSH which is a class defined in deduplication.LSH module. \n",
      "\n",
      "LSHForest has an __init__ which is a method that takes num_hashes which is an argument with a default value of 200, num_bands which is an argument with a default value of 10, rows_per_band which is an argument with a default value of 4, num_trees which is an argument with a default value of 5, and k which is an argument with a default value of 5. LSHForest calls the __init__ method of LSH with super(). \n",
      "\n",
      "LSHForest has a banding which is a method that takes signatures which is an argument. \n",
      "\n",
      "defaultdict is a package imported and used in banding which is a method. \n",
      "combinations is a package imported and used in banding which is a method. \n",
      "split_dict is a function imported from utils.utils and used in banding which is a method. \n",
      "majority_vote is a function imported from utils.utils and used in banding which is a method. hashlib is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "re is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "defaultdict is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "combinations is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "clean_document is a function imported from utils.utils and used in compute_minhash_signatures which is a method of LSHImproved.\n",
      "shingle is a function imported from utils.utils and used in compute_minhash_signatures which is a method of LSHImproved.\n",
      "minhash is a function imported from utils.utils and used in compute_minhash_signatures which is a method of LSHImproved.\n",
      "np is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "Parallel is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "delayed is a package imported in LSHImproved which is a class defined in module LSHImproved.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method __init__.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method remove_duplicates.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method compute_minhash_signatures.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method nearby_banding.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method bit_flip.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method gaussian.\n",
      "LSHImproved is a class defined in module LSHImproved which contains the method banding.\n",
      "num_hashes is an argument of __init__ which is a method of LSHImproved.\n",
      "num_bands is an argument of __init__ which is a method of LSHImproved.\n",
      "rows_per_band is an argument of __init__ which is a method of LSHImproved.\n",
      "k is an argument of __init__ which is a method of LSHImproved.\n",
      "num_probes is an argument of __init__ which is a method of LSHImproved.\n",
      "banding_method is an argument of __init__ which is a method of LSHImproved.\n",
      "docs is an argument of remove_duplicates which is a method of LSHImproved.\n",
      "docs is an argument of compute_minhash_signatures which is a method of LSHImproved.\n",
      "signatures is an argument of banding which is a method of LSHImproved.\n",
      "band is an argument of nearby_banding which is a method of LSHImproved.\n",
      "num_probes is an argument of nearby_banding which is a method of LSHImproved.\n",
      "band is an argument of bit_flip which is a method of LSHImproved.\n",
      "num_probes is an argument of bit_flip which is a method of LSHImproved.\n",
      "band is an argument of gaussian which is a method of LSHImproved.\n",
      "num_probes is an argument of gaussian which is a method of LSHImproved.\n",
      "std_dev is an argument of gaussian which is a method of LSHImproved.\n",
      "signatures is an argument of banding which is a method of LSHImproved. The file __init__.py is a module that imports version from importlib.metadata which is a package. The variable __version__ is defined in __init__.py which is a variable assigned the value of version(\"deduplication\"). The module __main__ defines the method log_memory_usage which is a function. The function log_memory_usage takes the argument message. The module __main__ defines one argument parser which is an instance of argparse.ArgumentParser. The argument parser has several arguments, including indir, case, save, example, numhash, numband, row, shinlen, treesize, and method. The argument indir is required. The argument case is required. The argument save is optional. The argument example is optional. The argument numhash is optional. The argument numband is optional. The argument row is optional. The argument shinlen is optional. The argument treesize is optional. The argument method is optional and has a default value of 'LSH'.\n",
      "\n",
      "The module __main__ imports the package argparse. The package time is imported and used in the functions log_memory_usage, model, and in multiple sections of code for timing purposes. The package logging is imported and used for logging events and information throughout the module. The package os is imported and used for file handling. The package psutil is imported and used in the function log_memory_usage. The package sys is imported and used for system exit calls.\n",
      "\n",
      "The module __main__ utilizes the class Baseline which is defined in the module deduplication.dedup. The class LSH is imported from the module deduplication.LSH and used in the function model. The class LSHImproved is imported from the module deduplication.LSHImproved and used in the function model. The class LSHForest is imported from the module deduplication.LSHForest and used in the function model. The function collection_deduplication is imported from the module utils.use_cases and used in the function model. The function read_tsv is imported from the module utils.utils and used for reading input files. The function nearest_neighbor_search is imported from the module utils.use_cases and used in the control flow with the condition if (args.case).lower() == 'ann'.\n",
      "\n",
      "The variable method is declared in the module __main__ and is assigned the value of args.method. The variable num_hashes, num_bands, rows_per_band, k, and num_trees are defined in the module __main__ and are assigned values based on the parsed arguments or default values. The variable tsv_dict is defined in the module __main__ from the return value of read_tsv. The variable clusters is defined in the deduplication process related to the method 'baseline' and assigned from a method call collection_deduplication or directly from lsh.  use_cases is a module that defines collection_deduplication which is a function and nearest_neighbor_search which is a function. \n",
      "\n",
      "collection_deduplication which is a function has lsh which is an argument. \n",
      "collection_deduplication uses UnionFind which is a package and is instantiated as uf which is a variable.\n",
      "collection_deduplication uses defaultdict which is a package and is instantiated as clusters which is a variable.\n",
      "collection_deduplication uses lsh.candidate_pairs which is a variable and is accessed from lsh which is an argument.\n",
      "collection_deduplication uses lsh.unique_docs which is a variable and is accessed from lsh which is an argument.\n",
      "collection_deduplication uses lsh.exact_duplicates which is a variable and is accessed from lsh which is an argument.\n",
      "collection_deduplication returns clusters which is a variable.\n",
      "\n",
      "nearest_neighbor_search is a function which has query_doc which is an argument and lsh which is an argument.\n",
      "nearest_neighbor_search uses clean_document which is a function and is accessed from utils.utils which is a module.\n",
      "nearest_neighbor_search uses shingle which is a function and is accessed from utils.utils which is a module.\n",
      "nearest_neighbor_search uses minhash which is a function and is accessed from utils.utils which is a module.\n",
      "nearest_neighbor_search uses lsh.k which is a variable and is accessed from lsh which is an argument.\n",
      "nearest_neighbor_search uses lsh.num_hashes which is a variable and is accessed from lsh which is an argument.\n",
      "nearest_neighbor_search uses lsh.num_bands which is a variable and is accessed from lsh which is an argument.\n",
      "nearest_neighbor_search uses lsh.rows_per_band which is a variable and is accessed from lsh which is an argument.\n",
      "nearest_neighbor_search returns candidate_pairs which is a variable. The module utils contains multiple functions and a class. \n",
      "\n",
      "hashlib is a package that is imported and used in the function minhash.\n",
      "re is a package that is imported and used in the function clean_document.\n",
      "xxhash is a package that is imported and used in the function minhash.\n",
      "collections is a package that is imported and used in the function majority_vote.\n",
      "\n",
      "clean_document is a function that takes the argument text which is a variable of type string. \n",
      "clean_document returns a variable of type string.\n",
      "\n",
      "shingle is a function that takes the arguments text which is a variable of type string and k which is an argument with a default type integer.\n",
      "shingle returns a variable of type set.\n",
      "\n",
      "minhash is a function that takes the arguments shingles which is a variable of type set and num_hashes which is an argument with a default type integer.\n",
      "minhash returns a variable of type list.\n",
      "\n",
      "UnionFind is a class defined in the module utils. \n",
      "UnionFind has an __init__ method which initializes a variable parent which is a variable of type dictionary.\n",
      "\n",
      "find is a method of the class UnionFind that takes the argument x which is a variable.\n",
      "find returns a variable.\n",
      "\n",
      "union is a method of the class UnionFind that takes the arguments x and y which are variables.\n",
      "union does not return a value.\n",
      "\n",
      "read_tsv is a function that takes the argument tsv which is a variable of type string.\n",
      "read_tsv returns a variable of type dictionary.\n",
      "\n",
      "split_dict is a function that takes the arguments input_dict which is a variable of type dictionary and num_splits which is an argument of type integer.\n",
      "split_dict returns a variable of type list.\n",
      "\n",
      "majority_vote is a function that takes the argument candidate_sets which is a variable of type list.\n",
      "majority_vote returns a variable of type list. ```\n",
      "utils.visualizations is a module that imports mmh3 which is a package and uses mmh3 in the add which is a method and query which is a method of BloomFilter which is a class. \n",
      "utils.visualizations is a module that imports math which is a package and uses math in the __init__ which is a method of BloomFilter which is a class. \n",
      "utils.visualizations is a module that imports bitarray which is a package and uses bitarray in the __init__ which is a method of BloomFilter which is a class. \n",
      "utils.visualizations is a module that imports random which is a package and uses random in calculate_false_positive_rate which is a function and in plot_false_positive_rate_vs_hash_functions which is a function. \n",
      "utils.visualizations is a module that imports matplotlib.pyplot as plt which is a package and uses plt in plot_false_positive_rate_vs_hash_functions which is a function. \n",
      "utils.visualizations is a module that imports plotly.graph_objects as go which is a package but does not use go in any function or method. \n",
      "utils.visualizations is a module that imports plotly.io as pio which is a package but does not use pio in any function or method. \n",
      "utils.visualizations is a module that imports numpy as np which is a package and uses np in calculate_false_positive_rate which is a function. \n",
      "\n",
      "BloomFilter is a class that has the following methods: \n",
      "__init__ is a method of BloomFilter which takes n which is an argument of type int, f which is an argument of type float, and k which is an argument of type int.\n",
      "add is a method of BloomFilter which takes item which is an argument of type str. \n",
      "query is a method of BloomFilter which takes item which is an argument of type str. \n",
      "\n",
      "calculate_false_positive_rate is a function that takes n which is an argument, f which is an argument, and k which is an argument. \n",
      "\n",
      "plot_false_positive_rate_vs_hash_functions is a function that takes n which is an argument, f which is an argument, and max_k which is an argument.\n",
      "``` visualization_lsh is a module that defines the plot_s_curves which is a function. \n",
      "numpy which is a package is imported and used in plot_s_curves which is a function. \n",
      "matplotlib.pyplot which is a package is imported and used in plot_s_curves which is a function. \n",
      "fixed_r is an argument of plot_s_curves which is a function. \n",
      "fixed_b is an argument of plot_s_curves which is a function. \n",
      "s_range is an argument of plot_s_curves which is a function. \n",
      "s_values is a variable within plot_s_curves which is a function. \n",
      "fig is a variable within plot_s_curves which is a function. \n",
      "axes is a variable within plot_s_curves which is a function. \n",
      "prob is a variable within plot_s_curves which is a function. \n",
      "b is a variable within plot_s_curves which is a function. \n",
      "r is a variable within plot_s_curves which is a function. \n"
     ]
    }
   ],
   "source": [
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:48:55,927] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:48:59,716] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:03,060] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:08,015] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:11,826] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:17,035] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:20,799] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:27,665] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:30,980] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:33,395] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:37,006] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:40,275] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:45,011] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:48,011] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:54,146] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:49:59,665] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:04,463] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:07,696] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:11,312] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:14,784] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:17,209] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:19,971] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:25,038] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:27,517] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:30,070] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:32,486] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:35,258] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:39,900] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:45,572] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:48,705] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:51,499] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:50:53,741] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:01,427] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:05,398] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:06,934] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:09,084] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:11,304] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:14,673] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:18,032] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:21,360] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:24,071] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:26,818] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:30,115] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:35,042] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:38,399] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:42,583] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:45,897] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:51:48,893] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:01,283] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:11,813] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:24,506] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:36,624] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:40,934] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:43,359] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:46,396] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:48,564] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:51,298] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:52:54,243] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:01,153] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:02,686] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:05,464] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:06,940] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:10,412] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:12,788] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:16,269] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:19,918] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:22,522] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:24,324] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:27,145] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:29,104] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:30,997] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:33,028] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:37,123] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:38,989] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:41,048] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:43,168] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:48,478] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:53,136] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:56,120] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:53:59,213] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:54:05,439] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 11:54:09,503] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    # chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        r\"\\n\\n\",\n",
    "        r\"\\n\",\n",
    "        r\"\\\\n\"\n",
    "    ],\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([description])\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=[\"class\", \"method\", \"function\",'package','module','variable','argument'],\n",
    "    # allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n",
    "    # node_properties=[\"born_year\"],\n",
    ")\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(graph_documents):\n",
    "#     logger.info(f\"document #{i+1}\")\n",
    "#     nodes = doc.nodes\n",
    "#     relationships = doc.relationships\n",
    "#     for n in nodes:\n",
    "#         logger.info(f\"Nodes:{n}\")\n",
    "#     for r in relationships:\n",
    "#         logger.info(f\"Relationships:{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with legend saved as graph_complex.html\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Pyvis network\n",
    "net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# Create a NetworkX graph for analysis\n",
    "G = nx.Graph()\n",
    "\n",
    "# Dictionary to store node attributes\n",
    "node_types = {}\n",
    "\n",
    "# Add nodes and edges, extracting types\n",
    "for graph in graph_documents:\n",
    "    for rel in graph.relationships:\n",
    "        source, target = str(rel.source.id), str(rel.target.id)\n",
    "        source_type = rel.source.type\n",
    "        target_type = rel.target.type\n",
    "        rel_type = rel.type  # Relationship type (e.g., 'IMPORTS')\n",
    "\n",
    "        # Store node types\n",
    "        node_types[source] = source_type\n",
    "        node_types[target] = target_type\n",
    "\n",
    "        # Add nodes if not already added\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "\n",
    "        # Add edge with label\n",
    "        G.add_edge(source, target, label=rel_type)\n",
    "\n",
    "# Get unique node types and assign colors\n",
    "unique_types = list(set(node_types.values()))\n",
    "color_map = plt.get_cmap(\"tab10\")  # Use a categorical colormap\n",
    "type_colors = {t: color_map(i / len(unique_types)) for i, t in enumerate(unique_types)}\n",
    "\n",
    "# Convert colors to RGBA format\n",
    "type_colors_rgba = {\n",
    "    t: f'rgba({int(c[0] * 255)}, {int(c[1] * 255)}, {int(c[2] * 255)}, 0.8)' for t, c in type_colors.items()\n",
    "}\n",
    "\n",
    "# Determine node sizes based on degrees\n",
    "degrees = dict(G.degree())\n",
    "min_size, max_size = 10, 50\n",
    "size_scale = {node: min_size + (max_size - min_size) * (deg / max(degrees.values())) for node, deg in degrees.items()}\n",
    "\n",
    "# Add nodes with dynamic colors and sizes\n",
    "for node in G.nodes():\n",
    "    node_type = node_types.get(node, \"default\")\n",
    "    net.add_node(\n",
    "        node,\n",
    "        label=node,  # Show node ID\n",
    "        size=size_scale[node],  # Adjust size\n",
    "        color=type_colors_rgba.get(node_type, \"gray\")  # Assign color based on type\n",
    "    )\n",
    "\n",
    "# Add edges with labels for relationship type\n",
    "for edge in G.edges(data=True):\n",
    "    source, target, attr = edge\n",
    "    rel_label = attr.get(\"label\", \"\")  # Get relationship type\n",
    "    net.add_edge(source, target, title=rel_label, label=rel_label)  # Show label on hover and as text\n",
    "\n",
    "# Save the graph\n",
    "net.save_graph(\"graph_complex.html\")\n",
    "\n",
    "# Generate legend HTML block\n",
    "legend_html = \"\"\"\n",
    "<div id=\"legend\" style=\"position: absolute; top: 10px; left: 10px; background: white; padding: 10px; border-radius: 8px; box-shadow: 0px 0px 5px rgba(0,0,0,0.2); font-family: Arial, sans-serif; z-index: 1000;\">\n",
    "    <h4 style=\"margin: 0; padding-bottom: 5px;\">Node Legend</h4>\n",
    "\"\"\"\n",
    "\n",
    "for node_type, color in type_colors_rgba.items():\n",
    "    legend_html += f'<div style=\"display: flex; align-items: center; margin-bottom: 5px;\"><div style=\"width: 15px; height: 15px; background:{color}; margin-right: 5px; border-radius: 50%;\"></div> {node_type}</div>'\n",
    "\n",
    "legend_html += \"</div>\"\n",
    "\n",
    "# Inject the legend into the HTML file\n",
    "with open(\"graph_complex.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Insert the legend just before closing </body> tag\n",
    "html_content = html_content.replace(\"</body>\", legend_html + \"</body>\")\n",
    "\n",
    "with open(\"graph_complex.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Graph with legend saved as graph_complex.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the LSH.py file\n",
    "current_file = \"src/deduplication/LSH.py\"\n",
    "lsh_file_path = f\"assignment-2-mcdonald-s/{current_file}\"\n",
    "\n",
    "# Read the contents of the file\n",
    "with open(lsh_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lsh_code = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 08:45:23,381] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashlib is a package that is imported and used in LSH which is a class defined in module LSH.\n",
      "re is a package that is imported and used in LSH which is a class defined in module LSH.\n",
      "defaultdict is a class imported from collections which is used in LSH which is a class defined in module LSH.\n",
      "combinations is a function imported from itertools which is used in LSH which is a class defined in module LSH.\n",
      "clean_document is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\n",
      "shingle is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\n",
      "minhash is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\n",
      "Parallel is a class imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\n",
      "delayed is a function imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH. \n",
      "\n",
      "LSH is a class that consists of __init__ which is a method.\n",
      "LSH is a class that consists of remove_duplicates which is a method.\n",
      "LSH is a class that consists of compute_minhash_signatures which is a method.\n",
      "LSH is a class that consists of banding which is a method.\n",
      "\n",
      "__init__ is a method in LSH which is a class defined in module LSH that has parameters num_hashes, num_bands, rows_per_band, k.\n",
      "remove_duplicates is a method in LSH which is a class defined in module LSH that has a parameter docs.\n",
      "compute_minhash_signatures is a method in LSH which is a class defined in module LSH that has a parameter docs.\n",
      "banding is a method in LSH which is a class defined in module LSH that has a parameter signatures.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are an expert in analyzing Python code and generating structured natural language descriptions for graph-based querying in Cypher. Given a Python codebase, extract meaningful relationships between functions, classes, parameters, and imported modules. \n",
    "    You will be given a repository tree structure and a specific file and a code of the specific file.\n",
    "    \n",
    "    Return the given code as natural language description suitable for cypher querying.\n",
    "    Only explain relationships between functions, classes, parameters, and imported modules.\n",
    "    Do not give explanations for the code logic or functionality.\n",
    "    Do not use adjectives and adverbs. \n",
    "    Only describe the code and do not give an overall summary.\n",
    "    Do not use ambiguous pronouns and use exact names in every description.\n",
    "    Explain each class, function, and variable separately and do not include explanations such as 'as mentioned before' or anything that refers to a previous explanation.\n",
    "\n",
    "    Each outermost class or method should be connected to the source file referred as modules\n",
    "        - Example(Class LSH is defined in module LSH, method deduplication is defined in Module LSH)\n",
    "    Each imported package should be connected to the function, method or class where it was used.\n",
    "\n",
    "    Use the structure so that the node id ad type are used in the format '{id} which is a {type}'.\n",
    "\n",
    "\n",
    "    Example Output (Structured Natural Language):\n",
    "    LSH is a module that defines the LSH which is a class, which consists of hash_function which is a method.\n",
    "    LSH which is a class inherits from BaseLSH which is a module.\n",
    "    numpy which is a package is imported and used in hash_function which is a method.\n",
    "    \n",
    "\n",
    "    Provide a structured response that allows for easy conversion into Cypher queries.\n",
    "    \"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        system_prompt\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        f'''\n",
    "        Tree:\n",
    "        {repo_tree_string}\n",
    "\n",
    "        Current File:\n",
    "        {current_file}\n",
    "\n",
    "        Code:\n",
    "        {lsh_code}\n",
    "        '''\n",
    "    ),\n",
    "]\n",
    "\n",
    "# structured_llm = llm.with_structured_output(Json)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hashlib is a package that is imported and used in LSH which is a class defined in module LSH.\\nre is a package that is imported and used in LSH which is a class defined in module LSH.\\ndefaultdict is a class imported from collections which is used in LSH which is a class defined in module LSH.\\ncombinations is a function imported from itertools which is used in LSH which is a class defined in module LSH.\\nclean_document is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\\nshingle is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\\nminhash is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\\nParallel is a class imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.\\ndelayed is a function imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH. \\n\\nLSH is a class that consists of __init__ which is a method.\\nLSH is a class that consists of remove_duplicates which is a method.\\nLSH is a class that consists of compute_minhash_signatures which is a method.\\nLSH is a class that consists of banding which is a method.\\n\\n__init__ is a method in LSH which is a class defined in module LSH that has parameters num_hashes, num_bands, rows_per_band, k.\\nremove_duplicates is a method in LSH which is a class defined in module LSH that has a parameter docs.\\ncompute_minhash_signatures is a method in LSH which is a class defined in module LSH that has a parameter docs.\\nbanding is a method in LSH which is a class defined in module LSH that has a parameter signatures.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed_nodes = [\"Person\", \"Organization\", \"Location\", \"Award\", \"ResearchField\"]\n",
    "# allowed_relationships = [\n",
    "#     (\"Person\", \"SPOUSE\", \"Person\"),\n",
    "#     (\"Person\", \"AWARD\", \"Award\"),\n",
    "#     (\"Person\", \"WORKS_AT\", \"Organization\"),\n",
    "#     (\"Organization\", \"IN_LOCATION\", \"Location\"),\n",
    "#     (\"Person\", \"FIELD_OF_RESEARCH\", \"ResearchField\")\n",
    "# ]\n",
    "# node_properties=[\"birth_date\", \"death_date\"]\n",
    "# relationship_properties=[\"start_date\"]\n",
    "# props_defined = LLMGraphTransformer(\n",
    "#   llm=llm, \n",
    "#   allowed_nodes=allowed_nodes,\n",
    "#   allowed_relationships=allowed_relationships,\n",
    "#   node_properties=node_properties,\n",
    "#   relationship_properties=relationship_properties\n",
    "# )\n",
    "# data = await props_defined.aconvert_to_graph_documents(documents)\n",
    "# graph.add_graph_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 08:45:25,981] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:27,756] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:30,054] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:32,188] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:34,306] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:37,058] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:40,408] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:42,301] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:44,927] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:46,280] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:47,691] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:48,986] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:50,223] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:54,361] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:56,359] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:45:59,855] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-19 08:46:01,685] p10496 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    # chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        r\"\\n\\n\",\n",
    "        r\"\\n\",\n",
    "        r\"\\\\n\"\n",
    "    ],\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([response.content])\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer_props = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Country\", \"Organization\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n",
    "#     node_properties=[\"born_year\"],\n",
    "# )\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# logger.info(f\"documents:{documents}\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='hashlib is a package that is imported and used in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='re is a package that is imported and used in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='defaultdict is a class imported from collections which is used in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='combinations is a function imported from itertools which is used in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='clean_document is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='shingle is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='minhash is a function imported from utils.utils which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='Parallel is a class imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH.'),\n",
       " Document(metadata={}, page_content='delayed is a function imported from joblib which is used in compute_minhash_signatures which is a method in LSH which is a class defined in module LSH. '),\n",
       " Document(metadata={}, page_content='LSH is a class that consists of __init__ which is a method.'),\n",
       " Document(metadata={}, page_content='LSH is a class that consists of remove_duplicates which is a method.'),\n",
       " Document(metadata={}, page_content='LSH is a class that consists of compute_minhash_signatures which is a method.'),\n",
       " Document(metadata={}, page_content='LSH is a class that consists of banding which is a method.'),\n",
       " Document(metadata={}, page_content='__init__ is a method in LSH which is a class defined in module LSH that has parameters num_hashes, num_bands, rows_per_band, k.'),\n",
       " Document(metadata={}, page_content='remove_duplicates is a method in LSH which is a class defined in module LSH that has a parameter docs.'),\n",
       " Document(metadata={}, page_content='compute_minhash_signatures is a method in LSH which is a class defined in module LSH that has a parameter docs.'),\n",
       " Document(metadata={}, page_content='banding is a method in LSH which is a class defined in module LSH that has a parameter signatures.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = response.content\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "# documents = [Document(page_content=text)]\n",
    "# # logger.info(f\"documents:{documents}\")\n",
    "# graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19 08:46:01,762] p10496 {406922825.py:2} INFO - document #1\n",
      "[2025-03-19 08:46:01,763] p10496 {406922825.py:6} INFO - Nodes:id='Hashlib' type='Package' properties={}\n",
      "[2025-03-19 08:46:01,763] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,764] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,764] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,765] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Hashlib', type='Package', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='USED_IN' properties={}\n",
      "[2025-03-19 08:46:01,765] p10496 {406922825.py:2} INFO - document #2\n",
      "[2025-03-19 08:46:01,765] p10496 {406922825.py:6} INFO - Nodes:id='Re' type='Package' properties={}\n",
      "[2025-03-19 08:46:01,766] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,766] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,767] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Re', type='Package', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='USED_IN' properties={}\n",
      "[2025-03-19 08:46:01,767] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,768] p10496 {406922825.py:2} INFO - document #3\n",
      "[2025-03-19 08:46:01,769] p10496 {406922825.py:6} INFO - Nodes:id='Defaultdict' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,770] p10496 {406922825.py:6} INFO - Nodes:id='Collections' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,770] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,770] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Defaultdict', type='Class', properties={}) target=Node(id='Collections', type='Module', properties={}) type='IMPORTED_FROM' properties={}\n",
      "[2025-03-19 08:46:01,771] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Collections', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,771] p10496 {406922825.py:2} INFO - document #4\n",
      "[2025-03-19 08:46:01,771] p10496 {406922825.py:6} INFO - Nodes:id='Combinations' type='Function' properties={}\n",
      "[2025-03-19 08:46:01,772] p10496 {406922825.py:6} INFO - Nodes:id='Itertools' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,773] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,774] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Combinations', type='Function', properties={}) target=Node(id='Itertools', type='Module', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-19 08:46:01,774] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Itertools', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,774] p10496 {406922825.py:2} INFO - document #5\n",
      "[2025-03-19 08:46:01,775] p10496 {406922825.py:6} INFO - Nodes:id='Clean_Document' type='Function' properties={}\n",
      "[2025-03-19 08:46:01,775] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,775] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,776] p10496 {406922825.py:6} INFO - Nodes:id='Utils.Utils' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,776] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Clean_Document', type='Function', properties={}) target=Node(id='Utils.Utils', type='Module', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-19 08:46:01,777] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='METHOD_IN' properties={}\n",
      "[2025-03-19 08:46:01,777] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Utils.Utils', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,777] p10496 {406922825.py:2} INFO - document #6\n",
      "[2025-03-19 08:46:01,778] p10496 {406922825.py:6} INFO - Nodes:id='Shingle' type='Function' properties={}\n",
      "[2025-03-19 08:46:01,778] p10496 {406922825.py:6} INFO - Nodes:id='Utils.Utils' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,779] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,779] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,780] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Shingle', type='Function', properties={}) target=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) type='USED_IN' properties={}\n",
      "[2025-03-19 08:46:01,780] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='METHOD_IN' properties={}\n",
      "[2025-03-19 08:46:01,780] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Utils.Utils', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,781] p10496 {406922825.py:2} INFO - document #7\n",
      "[2025-03-19 08:46:01,781] p10496 {406922825.py:6} INFO - Nodes:id='Minhash' type='Function' properties={}\n",
      "[2025-03-19 08:46:01,781] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,782] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,782] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,782] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Minhash', type='Function', properties={}) target=Node(id='Utils.Utils', type='Module', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-19 08:46:01,783] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Minhash', type='Function', properties={}) type='USES' properties={}\n",
      "[2025-03-19 08:46:01,783] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) type='HAS_METHOD' properties={}\n",
      "[2025-03-19 08:46:01,783] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,784] p10496 {406922825.py:2} INFO - document #8\n",
      "[2025-03-19 08:46:01,784] p10496 {406922825.py:6} INFO - Nodes:id='Parallel' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,785] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,785] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,785] p10496 {406922825.py:6} INFO - Nodes:id='Lsh_Module' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,785] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='METHOD_IN' properties={}\n",
      "[2025-03-19 08:46:01,786] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Parallel', type='Class', properties={}) target=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-19 08:46:01,786] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Lsh_Module', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,786] p10496 {406922825.py:2} INFO - document #9\n",
      "[2025-03-19 08:46:01,787] p10496 {406922825.py:6} INFO - Nodes:id='Delayed' type='Function' properties={}\n",
      "[2025-03-19 08:46:01,787] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,787] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,788] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,789] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Delayed', type='Function', properties={}) target=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) type='IMPORTS' properties={}\n",
      "[2025-03-19 08:46:01,789] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,789] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,790] p10496 {406922825.py:2} INFO - document #10\n",
      "[2025-03-19 08:46:01,790] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,793] p10496 {406922825.py:6} INFO - Nodes:id='__Init__' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,794] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='__Init__', type='Method', properties={}) type='CONSISTS_OF' properties={}\n",
      "[2025-03-19 08:46:01,794] p10496 {406922825.py:2} INFO - document #11\n",
      "[2025-03-19 08:46:01,795] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,795] p10496 {406922825.py:6} INFO - Nodes:id='Remove_Duplicates' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,795] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Remove_Duplicates', type='Method', properties={}) type='CONTAINS' properties={}\n",
      "[2025-03-19 08:46:01,796] p10496 {406922825.py:2} INFO - document #12\n",
      "[2025-03-19 08:46:01,797] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,797] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,797] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) type='HAS_METHOD' properties={}\n",
      "[2025-03-19 08:46:01,798] p10496 {406922825.py:2} INFO - document #13\n",
      "[2025-03-19 08:46:01,798] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,798] p10496 {406922825.py:6} INFO - Nodes:id='Banding' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,799] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Banding', type='Method', properties={}) type='CONSISTS_OF' properties={}\n",
      "[2025-03-19 08:46:01,799] p10496 {406922825.py:2} INFO - document #14\n",
      "[2025-03-19 08:46:01,799] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,800] p10496 {406922825.py:6} INFO - Nodes:id='__Init__' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,800] p10496 {406922825.py:6} INFO - Nodes:id='Num_Hashes' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,800] p10496 {406922825.py:6} INFO - Nodes:id='Num_Bands' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,801] p10496 {406922825.py:6} INFO - Nodes:id='Rows_Per_Band' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,801] p10496 {406922825.py:6} INFO - Nodes:id='K' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,802] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,802] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='__Init__', type='Method', properties={}) type='HAS_METHOD' properties={}\n",
      "[2025-03-19 08:46:01,802] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,803] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='__Init__', type='Method', properties={}) target=Node(id='Num_Hashes', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,803] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='__Init__', type='Method', properties={}) target=Node(id='Num_Bands', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,804] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='__Init__', type='Method', properties={}) target=Node(id='Rows_Per_Band', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,804] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='__Init__', type='Method', properties={}) target=Node(id='K', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,805] p10496 {406922825.py:2} INFO - document #15\n",
      "[2025-03-19 08:46:01,806] p10496 {406922825.py:6} INFO - Nodes:id='Remove_Duplicates' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,806] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,806] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,807] p10496 {406922825.py:6} INFO - Nodes:id='Docs' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,807] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Remove_Duplicates', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='METHOD_IN' properties={}\n",
      "[2025-03-19 08:46:01,807] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,808] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Docs', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,808] p10496 {406922825.py:2} INFO - document #16\n",
      "[2025-03-19 08:46:01,809] p10496 {406922825.py:6} INFO - Nodes:id='Compute_Minhash_Signatures' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,809] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,810] p10496 {406922825.py:6} INFO - Nodes:id='Module Lsh' type='Module' properties={}\n",
      "[2025-03-19 08:46:01,810] p10496 {406922825.py:6} INFO - Nodes:id='Docs' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,811] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='METHOD_IN_CLASS' properties={}\n",
      "[2025-03-19 08:46:01,811] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Module Lsh', type='Module', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,811] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Compute_Minhash_Signatures', type='Method', properties={}) target=Node(id='Docs', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n",
      "[2025-03-19 08:46:01,812] p10496 {406922825.py:2} INFO - document #17\n",
      "[2025-03-19 08:46:01,813] p10496 {406922825.py:6} INFO - Nodes:id='Banding' type='Method' properties={}\n",
      "[2025-03-19 08:46:01,813] p10496 {406922825.py:6} INFO - Nodes:id='Lsh' type='Class' properties={}\n",
      "[2025-03-19 08:46:01,814] p10496 {406922825.py:6} INFO - Nodes:id='Signatures' type='Parameter' properties={}\n",
      "[2025-03-19 08:46:01,814] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Banding', type='Method', properties={}) target=Node(id='Lsh', type='Class', properties={}) type='DEFINED_IN' properties={}\n",
      "[2025-03-19 08:46:01,815] p10496 {406922825.py:8} INFO - Relationships:source=Node(id='Lsh', type='Class', properties={}) target=Node(id='Signatures', type='Parameter', properties={}) type='HAS_PARAMETER' properties={}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(graph_documents):\n",
    "    logger.info(f\"document #{i+1}\")\n",
    "    nodes = doc.nodes\n",
    "    relationships = doc.relationships\n",
    "    for n in nodes:\n",
    "        logger.info(f\"Nodes:{n}\")\n",
    "    for r in relationships:\n",
    "        logger.info(f\"Relationships:{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with legend saved as graph.html\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Pyvis network\n",
    "net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# Create a NetworkX graph for analysis\n",
    "G = nx.Graph()\n",
    "\n",
    "# Dictionary to store node attributes\n",
    "node_types = {}\n",
    "\n",
    "# Add nodes and edges, extracting types\n",
    "for graph in graph_documents:\n",
    "    for rel in graph.relationships:\n",
    "        source, target = str(rel.source.id), str(rel.target.id)\n",
    "        source_type = rel.source.type\n",
    "        target_type = rel.target.type\n",
    "        rel_type = rel.type  # Relationship type (e.g., 'IMPORTS')\n",
    "\n",
    "        # Store node types\n",
    "        node_types[source] = source_type\n",
    "        node_types[target] = target_type\n",
    "\n",
    "        # Add nodes if not already added\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "\n",
    "        # Add edge with label\n",
    "        G.add_edge(source, target, label=rel_type)\n",
    "\n",
    "# Get unique node types and assign colors\n",
    "unique_types = list(set(node_types.values()))\n",
    "color_map = plt.get_cmap(\"tab10\")  # Use a categorical colormap\n",
    "type_colors = {t: color_map(i / len(unique_types)) for i, t in enumerate(unique_types)}\n",
    "\n",
    "# Convert colors to RGBA format\n",
    "type_colors_rgba = {\n",
    "    t: f'rgba({int(c[0] * 255)}, {int(c[1] * 255)}, {int(c[2] * 255)}, 0.8)' for t, c in type_colors.items()\n",
    "}\n",
    "\n",
    "# Determine node sizes based on degrees\n",
    "degrees = dict(G.degree())\n",
    "min_size, max_size = 10, 50\n",
    "size_scale = {node: min_size + (max_size - min_size) * (deg / max(degrees.values())) for node, deg in degrees.items()}\n",
    "\n",
    "# Add nodes with dynamic colors and sizes\n",
    "for node in G.nodes():\n",
    "    node_type = node_types.get(node, \"default\")\n",
    "    net.add_node(\n",
    "        node,\n",
    "        label=node,  # Show node ID\n",
    "        size=size_scale[node],  # Adjust size\n",
    "        color=type_colors_rgba.get(node_type, \"gray\")  # Assign color based on type\n",
    "    )\n",
    "\n",
    "# Add edges with labels for relationship type\n",
    "for edge in G.edges(data=True):\n",
    "    source, target, attr = edge\n",
    "    rel_label = attr.get(\"label\", \"\")  # Get relationship type\n",
    "    net.add_edge(source, target, title=rel_label, label=rel_label)  # Show label on hover and as text\n",
    "\n",
    "# Save the graph\n",
    "net.save_graph(\"graph.html\")\n",
    "\n",
    "# Generate legend HTML block\n",
    "legend_html = \"\"\"\n",
    "<div id=\"legend\" style=\"position: absolute; top: 10px; left: 10px; background: white; padding: 10px; border-radius: 8px; box-shadow: 0px 0px 5px rgba(0,0,0,0.2); font-family: Arial, sans-serif; z-index: 1000;\">\n",
    "    <h4 style=\"margin: 0; padding-bottom: 5px;\">Node Legend</h4>\n",
    "\"\"\"\n",
    "\n",
    "for node_type, color in type_colors_rgba.items():\n",
    "    legend_html += f'<div style=\"display: flex; align-items: center; margin-bottom: 5px;\"><div style=\"width: 15px; height: 15px; background:{color}; margin-right: 5px; border-radius: 50%;\"></div> {node_type}</div>'\n",
    "\n",
    "legend_html += \"</div>\"\n",
    "\n",
    "# Inject the legend into the HTML file\n",
    "with open(\"graph.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Insert the legend just before closing </body> tag\n",
    "html_content = html_content.replace(\"</body>\", legend_html + \"</body>\")\n",
    "\n",
    "with open(\"graph.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Graph with legend saved as graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create Pyvis network\n",
    "# net = Network(notebook=True, cdn_resources='in_line', height=\"1000px\", width=\"100%\")\n",
    "\n",
    "# # Create a NetworkX graph for analysis\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Dictionary to store node attributes\n",
    "# node_types = {}\n",
    "\n",
    "# # Add nodes and edges, extracting types\n",
    "# for graph in graph_documents:\n",
    "#     for rel in graph.relationships:\n",
    "#         source, target = str(rel.source.id), str(rel.target.id)\n",
    "#         source_type = rel.source.type\n",
    "#         target_type = rel.target.type\n",
    "#         rel_type = rel.type  # Relationship type (e.g., 'IMPORTS')\n",
    "\n",
    "#         # Store node types\n",
    "#         node_types[source] = source_type\n",
    "#         node_types[target] = target_type\n",
    "\n",
    "#         # Add nodes if not already added\n",
    "#         G.add_node(source)\n",
    "#         G.add_node(target)\n",
    "\n",
    "#         # Add edge with label\n",
    "#         G.add_edge(source, target, label=rel_type)\n",
    "\n",
    "# # Get unique node types and assign colors\n",
    "# unique_types = list(set(node_types.values()))\n",
    "# color_map = plt.get_cmap(\"tab10\")  # Use a categorical colormap\n",
    "# type_colors = {t: color_map(i / len(unique_types)) for i, t in enumerate(unique_types)}\n",
    "\n",
    "# # Convert colors to RGBA format\n",
    "# type_colors_rgba = {\n",
    "#     t: f'rgba({int(c[0] * 255)}, {int(c[1] * 255)}, {int(c[2] * 255)}, 0.8)' for t, c in type_colors.items()\n",
    "# }\n",
    "\n",
    "# # Determine node sizes based on degrees\n",
    "# degrees = dict(G.degree())\n",
    "# min_size, max_size = 10, 50\n",
    "# size_scale = {node: min_size + (max_size - min_size) * (deg / max(degrees.values())) for node, deg in degrees.items()}\n",
    "\n",
    "# # Add nodes with dynamic colors and sizes\n",
    "# for node in G.nodes():\n",
    "#     node_type = node_types.get(node, \"default\")\n",
    "#     net.add_node(\n",
    "#         node,\n",
    "#         label=node,  # Show node ID\n",
    "#         size=size_scale[node],  # Adjust size\n",
    "#         color=type_colors_rgba.get(node_type, \"gray\")  # Assign color based on type\n",
    "#     )\n",
    "\n",
    "# # Add edges with labels for relationship type\n",
    "# for edge in G.edges(data=True):\n",
    "#     source, target, attr = edge\n",
    "#     rel_label = attr.get(\"label\", \"\")  # Get relationship type\n",
    "#     net.add_edge(source, target, title=rel_label, label=rel_label)  # Show label on hover and as text\n",
    "\n",
    "# # Save and display\n",
    "# net.save_graph(\"graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not connect to Memgraph database. Please ensure that the url is correct",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:409\u001b[39m, in \u001b[36mBoltSocketBase._connect_secure\u001b[39m\u001b[34m(cls, resolved_address, timeout, keep_alive, ssl_context)\u001b[39m\n\u001b[32m    408\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  C: <OPEN> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, resolved_address)\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m s.settimeout(t)\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt_socket.py:328\u001b[39m, in \u001b[36mBoltSocket.connect\u001b[39m\u001b[34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     s = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect_secure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcp_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssl_context\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     agreed_version, handshake, response = s._handshake(\n\u001b[32m    332\u001b[39m         resolved_address, deadline\n\u001b[32m    333\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:426\u001b[39m, in \u001b[36mBoltSocketBase._connect_secure\u001b[39m\u001b[34m(cls, resolved_address, timeout, keep_alive, ssl_context)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\n\u001b[32m    427\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to establish connection to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_address\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m (reason \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    429\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: Failed to establish connection to ResolvedIPv6Address(('::1', 3000, 0, 0)) (reason [WinError 10061] No connection could be made because the target machine actively refused it)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\langchain_community\\graphs\\memgraph_graph.py:332\u001b[39m, in \u001b[36mMemgraphGraph.__init__\u001b[39m\u001b[34m(self, url, username, password, database, refresh_schema, driver_config)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_connectivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m neo4j.exceptions.ServiceUnavailable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:1082\u001b[39m, in \u001b[36mDriver.verify_connectivity\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1081\u001b[39m session_config = \u001b[38;5;28mself\u001b[39m._read_session_config(config)\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_server_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\driver.py:1297\u001b[39m, in \u001b[36mDriver._get_server_info\u001b[39m\u001b[34m(self, session_config)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._session(session_config) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_server_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:183\u001b[39m, in \u001b[36mSession._get_server_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_ACCESS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mliveness_check_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m server_info = \u001b[38;5;28mself\u001b[39m._connection.server_info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:136\u001b[39m, in \u001b[36mSession._connect\u001b[39m\u001b[34m(self, access_mode, **acquire_kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:199\u001b[39m, in \u001b[36mWorkspace._connect\u001b[39m\u001b[34m(self, access_mode, auth, **acquire_kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m acquire_kwargs_.update(acquire_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[38;5;28mself\u001b[39m._connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    201\u001b[39m     target_db.guessed\n\u001b[32m    202\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pinned_database\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m     \u001b[38;5;66;03m# support SSR.\u001b[39;00m\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# => we need to fall back to explicit home database resolution\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:662\u001b[39m, in \u001b[36mBoltPool.acquire\u001b[39m\u001b[34m(self, access_mode, timeout, database, bookmarks, auth, liveness_check_timeout, database_callback)\u001b[39m\n\u001b[32m    661\u001b[39m deadline = Deadline.from_timeout_or_deadline(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mliveness_check_timeout\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:408\u001b[39m, in \u001b[36mIOPool._acquire\u001b[39m\u001b[34m(self, address, auth, deadline, liveness_check_timeout)\u001b[39m\n\u001b[32m    407\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> trying to hand out new connection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:230\u001b[39m, in \u001b[36mIOPool._acquire_new_later.<locals>.connection_creator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ServiceUnavailable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:624\u001b[39m, in \u001b[36mBoltPool.open.<locals>.opener\u001b[39m\u001b[34m(addr, auth_manager, deadline)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopener\u001b[39m(addr, auth_manager, deadline):\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBolt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrouting_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:369\u001b[39m, in \u001b[36mBolt.open\u001b[39m\u001b[34m(cls, address, auth_manager, deadline, routing_context, pool_config)\u001b[39m\n\u001b[32m    367\u001b[39m     deadline = Deadline(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m s, protocol_version, handshake, data = \u001b[43mBoltSocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtcp_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_resolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m pool_config.protocol_version = protocol_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt_socket.py:376\u001b[39m, in \u001b[36mBoltSocket.connect\u001b[39m\u001b[34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[39m\n\u001b[32m    375\u001b[39m error_strs = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, errors))\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (resolved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress_strs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_strs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrors\u001b[39;00m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mServiceUnavailable\u001b[39m: Couldn't connect to localhost:3000 (resolved to ('[::1]:3000', '127.0.0.1:3000')):\nFailed to establish connection to ResolvedIPv6Address(('::1', 3000, 0, 0)) (reason [WinError 10061] No connection could be made because the target machine actively refused it)\nFailed to establish connection to ResolvedIPv4Address(('127.0.0.1', 3000)) (reason [WinError 10061] No connection could be made because the target machine actively refused it)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m username = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mMEMGRAPH_USERNAME\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m password = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mMEMGRAPH_PASSWORD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m graph = \u001b[43mMemgraphGraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m=\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JaeHoBahng\\Desktop\\Georgetown\\2025_Spring\\DSAN_6725\\project\\spring-2025-final-project-project-group-2\\.venv\\Lib\\site-packages\\langchain_community\\graphs\\memgraph_graph.py:334\u001b[39m, in \u001b[36mMemgraphGraph.__init__\u001b[39m\u001b[34m(self, url, username, password, database, refresh_schema, driver_config)\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m._driver.verify_connectivity()\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m neo4j.exceptions.ServiceUnavailable:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    335\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not connect to Memgraph database. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure that the url is correct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m     )\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m neo4j.exceptions.AuthError:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not connect to Memgraph database. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure that the username and password are correct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Could not connect to Memgraph database. Please ensure that the url is correct"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/graphs/memgraph/\n",
    "\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "import os\n",
    "\n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    " \n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:31,826] p16596 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the database is empty\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    " \n",
    "# Create KG\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:31,854] p16596 {memgraph_graph.py:450} INFO - Schema generation with SHOW SCHEMA INFO query failed. Set --schema-info-enabled=true to use SHOW SCHEMA INFO query. Falling back to alternative queries.\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node labels and properties (name and type) are:\n",
      "- labels: (:Function)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Detail)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Class)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Instruction)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Object)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Variable)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Parameter)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Attribute)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Decorator)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Property)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Argument)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Type)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Agent type)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Component)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Method)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Library)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Concept)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Parser)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Message)\n",
      "  properties:\n",
      "    - id: string\n",
      "\n",
      "Nodes are connected with the following relationships:\n",
      "(:Method)-[:CALLS]->(:Method)\n",
      "(:Method)-[:USES]->(:Argument)\n",
      "(:Method)-[:USES]->(:Attribute)\n",
      "(:Method)-[:USES]->(:Detail)\n",
      "(:Method)-[:USES]->(:Instruction)\n",
      "(:Method)-[:VALIDATES]->(:Parameter)\n",
      "(:Method)-[:CALLS]->(:Function)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Parameter)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Argument)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Attribute)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Detail)\n",
      "(:Method)-[:HAS_PARAMETER]->(:Instruction)\n",
      "(:Method)-[:CREATES_INSTANCE]->(:Class)\n",
      "(:Class)-[:RELATED_TO]->(:Class)\n",
      "(:Method)-[:CONSTRUCTS]->(:Class)\n",
      "(:Concept)-[:USES]->(:Parser)\n",
      "(:Message)-[:IMPORTS]->(:)\n",
      "(:Method)-[:DETERMINES]->(:Variable)\n",
      "(:Library)-[:IMPORTS]->(:Concept)\n",
      "(:Library)-[:ENABLES]->(:Concept)\n",
      "(:Concept)-[:VALIDATES]->(:Function)\n",
      "(:Class)-[:RELATED_TO]->(:Type)\n",
      "(:Library)-[:IMPORTS]->(:Function)\n",
      "(:Library)-[:IMPORTS]->(:Component)\n",
      "(:Concept)-[:RELATED_TO]->(:Type)\n",
      "(:Method)-[:CREATES]->(:Class)\n",
      "(:Method)-[:GENERATES]->(:Variable)\n",
      "(:Class)-[:HAS_ATTRIBUTE]->(:Attribute)\n",
      "(:Method)-[:USES]->(:Parameter)\n",
      "(:Method)-[:RETURNS]->(:Object)\n",
      "(:Concept)-[:USES]->(:Library)\n",
      "(:Concept)-[:RELATED_TO]->(:Class)\n",
      "(:Class)-[:USES]->(:Parser)\n",
      "(:Class)-[:INTERACTS_WITH]->(:Concept)\n",
      "(:Class)-[:INTERACTS_WITH]->(:Class)\n",
      "(:Class)-[:INHERITS_FROM]->(:Concept)\n",
      "(:Library)-[:IMPORTS]->(:Type)\n",
      "(:Class)-[:VALIDATES]->(:Function)\n",
      "(:Class)-[:INHERITS_FROM]->(:Class)\n",
      "(:Method)-[:CREATES]->(:Variable)\n",
      "(:Method)-[:REQUIRES]->(:Attribute)\n",
      "(:Class)-[:DECORATED_WITH]->(:Decorator)\n",
      "(:Property)-[:RETURNS]->(:)\n",
      "(:Method)-[:CREATES_INSTANCE]->(:Parser)\n",
      "(:Attribute)-[:INSTANTIATES]->(:Parser)\n",
      "(:Method)-[:REQUIRES]->(:Argument)\n",
      "(:Property)-[:RETURNS]->(:Agent type)\n",
      "(:Method)-[:RETURNS]->(:Component)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"\n",
    "Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\",temperature=0)\n",
    "schema = graph.schema\n",
    "\n",
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not use atomic operations in your Cypher queries.\n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "MEMGRAPH_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=MEMGRAPH_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    llm,\n",
    "    cypher_prompt=MEMGRAPH_GENERATION_PROMPT,\n",
    "    graph=graph,\n",
    "    model_id=\"gpt-4o-turbo\",\n",
    "    return_intermediate_steps=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:33,195] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-03-15 12:22:33,611] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Steps :  [{'query': \"MATCH (c:Class {id: 'Conversationalagent'})<-[:CREATES_INSTANCE]-(m:Method) RETURN m\"}, {'context': []}]\n",
      "Final Response :  I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "q = \"what methods are related to class Conversationalagent?\"\n",
    "response = chain.invoke(q)\n",
    "print(\"Intermediate Steps : \", response['intermediate_steps'])\n",
    "print(\"Final Response : \", response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c:Class {id: 'Conversationalagent'})-[:TAKES_PARAMETER]->(p:Parameter) RETURN p.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:34,433] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "MATCH (n)-[r]->(m) WHERE n.id = 'Conversationalagent' RETURN n, r, m\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}), 'm': {'id': 'Agent'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}), 'm': {'id': 'Agent'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'DECORATED_WITH', {'id': '@Deprecated'}), 'm': {'id': '@Deprecated'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Ai_Prefix'}), 'm': {'id': 'Ai_Prefix'}}, {'n': {'id': 'Conversationalagent'}, 'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Output_Parser'}), 'm': {'id': 'Output_Parser'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-15 12:22:35,237] p16596 {_client.py:1025} INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is related to the id Conversationalagent',\n",
       " 'result': 'The id Conversationalagent is related to the following: it inherits from the id Agent, is decorated with @Deprecated, and has attributes Ai_Prefix and Output_Parser.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "qa_chainn = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True,allow_dangerous_requests=True)\n",
    "response = qa_chainn.invoke({\"query\": \"what is related to the id Conversationalagent\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}),\n",
       "  'n': {'id': 'Agent'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'INHERITS_FROM', {'id': 'Agent'}),\n",
       "  'n': {'id': 'Agent'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'DECORATED_WITH',\n",
       "   {'id': '@Deprecated'}),\n",
       "  'n': {'id': '@Deprecated'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'}, 'HAS_ATTRIBUTE', {'id': 'Ai_Prefix'}),\n",
       "  'n': {'id': 'Ai_Prefix'}},\n",
       " {'c': {'id': 'Conversationalagent'},\n",
       "  'r': ({'id': 'Conversationalagent'},\n",
       "   'HAS_ATTRIBUTE',\n",
       "   {'id': 'Output_Parser'}),\n",
       "  'n': {'id': 'Output_Parser'}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"MATCH (c)-[r]->(n) WHERE c.id = 'Conversationalagent' RETURN c, r, n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "# How many people played in Top Gun?\n",
    "MATCH (m:Movie {{name:\"Top Gun\"}})<-[:ACTED_IN]-()\n",
    "RETURN count(*) AS numberOfActors\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_transformer_filtered = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Nationality\", \"Concept\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"INVOLVED_IN\", \"COLLABORATES_WITH\"],\n",
    "# )\n",
    "# graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "#     documents\n",
    "# )\n",
    "\n",
    "# print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents_filtered[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \\n to \\\\n\n",
    "\n",
    "code = '''\n",
    "# An agent designed to hold a conversation in addition to using tools.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "from langchain_core._api import deprecated\n",
    "from langchain_core.callbacks import BaseCallbackManager\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import Field\n",
    "\n",
    "from langchain._api.deprecation import AGENT_DEPRECATION_WARNING\n",
    "from langchain.agents.agent import Agent, AgentOutputParser\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n",
    "from langchain.agents.utils import validate_tools_single_input\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "@deprecated(\n",
    "    \"0.1.0\",\n",
    "    message=AGENT_DEPRECATION_WARNING,\n",
    "    removal=\"1.0\",\n",
    ")\n",
    "class ConversationalAgent(Agent):\n",
    "    \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\n",
    "\n",
    "    ai_prefix: str = \"AI\"\n",
    "    \"\"\"Prefix to use before AI output.\"\"\"\n",
    "    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\n",
    "    \"\"\"Output parser for the agent.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_default_output_parser(\n",
    "        cls, ai_prefix: str = \"AI\", **kwargs: Any\n",
    "    ) -> AgentOutputParser:\n",
    "        return ConvoOutputParser(ai_prefix=ai_prefix)\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        \"\"\"Return Identifier of agent type.\"\"\"\n",
    "        return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n",
    "\n",
    "    @property\n",
    "    def observation_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the observation with.\n",
    "\n",
    "        Returns:\n",
    "            \"Observation: \"\n",
    "        \"\"\"\n",
    "        return \"Observation: \"\n",
    "\n",
    "    @property\n",
    "    def llm_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the llm call with.\n",
    "\n",
    "        Returns:\n",
    "            \"Thought: \"\n",
    "        \"\"\"\n",
    "        return \"Thought:\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_prompt(\n",
    "        cls,\n",
    "        tools: Sequence[BaseTool],\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "    ) -> PromptTemplate:\n",
    "        \"\"\"Create prompt in the style of the zero-shot agent.\n",
    "\n",
    "        Args:\n",
    "            tools: List of tools the agent will have access to, used to format the\n",
    "                prompt.\n",
    "            prefix: String to put before the list of tools. Defaults to PREFIX.\n",
    "            suffix: String to put after the list of tools. Defaults to SUFFIX.\n",
    "            format_instructions: Instructions on how to use the tools. Defaults to\n",
    "                FORMAT_INSTRUCTIONS\n",
    "            ai_prefix: String to use before AI output. Defaults to \"AI\".\n",
    "            human_prefix: String to use before human output.\n",
    "                Defaults to \"Human\".\n",
    "            input_variables: List of input variables the final prompt will expect.\n",
    "                Defaults to [\"input\", \"chat_history\", \"agent_scratchpad\"].\n",
    "\n",
    "        Returns:\n",
    "            A PromptTemplate with the template assembled from the pieces here.\n",
    "        \"\"\"\n",
    "        tool_strings = \"\\\\n\".join(\n",
    "            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n",
    "        )\n",
    "        tool_names = \", \".join([tool.name for tool in tools])\n",
    "        format_instructions = format_instructions.format(\n",
    "            tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\n",
    "        )\n",
    "        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "        if input_variables is None:\n",
    "            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
    "        return PromptTemplate(template=template, input_variables=input_variables)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "        super()._validate_tools(tools)\n",
    "        validate_tools_single_input(cls.__name__, tools)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        output_parser: Optional[AgentOutputParser] = None,\n",
    "        prefix: str = PREFIX,\n",
    "        suffix: str = SUFFIX,\n",
    "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
    "        ai_prefix: str = \"AI\",\n",
    "        human_prefix: str = \"Human\",\n",
    "        input_variables: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Agent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "        Args:\n",
    "            llm: The language model to use.\n",
    "            tools: A list of tools to use.\n",
    "            callback_manager: The callback manager to use. Default is None.\n",
    "            output_parser: The output parser to use. Default is None.\n",
    "            prefix: The prefix to use in the prompt. Default is PREFIX.\n",
    "            suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
    "            format_instructions: The format instructions to use.\n",
    "                Default is FORMAT_INSTRUCTIONS.\n",
    "            ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
    "            human_prefix: The prefix to use before human output.\n",
    "                Default is \"Human\".\n",
    "            input_variables: The input variables to use. Default is None.\n",
    "            **kwargs: Any additional keyword arguments to pass to the agent.\n",
    "\n",
    "        Returns:\n",
    "            An agent.\n",
    "        \"\"\"\n",
    "        cls._validate_tools(tools)\n",
    "        prompt = cls.create_prompt(\n",
    "            tools,\n",
    "            ai_prefix=ai_prefix,\n",
    "            human_prefix=human_prefix,\n",
    "            prefix=prefix,\n",
    "            suffix=suffix,\n",
    "            format_instructions=format_instructions,\n",
    "            input_variables=input_variables,\n",
    "        )\n",
    "        llm_chain = LLMChain(  # type: ignore[misc]\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            callback_manager=callback_manager,\n",
    "        )\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        _output_parser = output_parser or cls._get_default_output_parser(\n",
    "            ai_prefix=ai_prefix\n",
    "        )\n",
    "        return cls(\n",
    "            llm_chain=llm_chain,\n",
    "            allowed_tools=tool_names,\n",
    "            ai_prefix=ai_prefix,\n",
    "            output_parser=_output_parser,\n",
    "            **kwargs,\n",
    "        )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: ['__copy__', '__deepcopy__', '__delattr__', '__eq__', '__getattr__', '__getstate__', '__init__', '__iter__', '__pretty__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__str__', '_calculate_keys', '_check_frozen', '_construct_scratchpad', '_copy_and_set_values', '_fix_text', '_iter', 'aplan', 'copy', 'dict', 'get_allowed_tools', 'get_full_inputs', 'json', 'model_copy', 'model_dump', 'model_dump_json', 'model_post_init', 'plan', 'return_stopped_response', 'save', 'tool_run_logging_kwargs', 'validate_prompt']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n",
    "# List all methods of the class\n",
    "methods = inspect.getmembers(ConversationalAgent, predicate=inspect.isfunction)\n",
    "print(\"Methods:\", [method[0] for method in methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "# Save the code as a file\n",
    "with open(\"agent_code.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# Dynamically load the module\n",
    "spec = importlib.util.spec_from_file_location(\"agent_code\", \"agent_code.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "# Retrieve the class\n",
    "ConversationalAgent = getattr(module, \"ConversationalAgent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_code.ConversationalAgent"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConversationalAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code for method 'from_llm_and_tools':\n",
      "\n",
      "    @classmethod\n",
      "    def from_llm_and_tools(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        tools: Sequence[BaseTool],\n",
      "        callback_manager: Optional[BaseCallbackManager] = None,\n",
      "        output_parser: Optional[AgentOutputParser] = None,\n",
      "        prefix: str = PREFIX,\n",
      "        suffix: str = SUFFIX,\n",
      "        format_instructions: str = FORMAT_INSTRUCTIONS,\n",
      "        ai_prefix: str = \"AI\",\n",
      "        human_prefix: str = \"Human\",\n",
      "        input_variables: Optional[List[str]] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Agent:\n",
      "        \"\"\"Construct an agent from an LLM and tools.\n",
      "\n",
      "        Args:\n",
      "            llm: The language model to use.\n",
      "            tools: A list of tools to use.\n",
      "            callback_manager: The callback manager to use. Default is None.\n",
      "            output_parser: The output parser to use. Default is None.\n",
      "            prefix: The prefix to use in the prompt. Default is PREFIX.\n",
      "            suffix: The suffix to use in the prompt. Default is SUFFIX.\n",
      "            format_instructions: The format instructions to use.\n",
      "                Default is FORMAT_INSTRUCTIONS.\n",
      "            ai_prefix: The prefix to use before AI output. Default is \"AI\".\n",
      "            human_prefix: The prefix to use before human output.\n",
      "                Default is \"Human\".\n",
      "            input_variables: The input variables to use. Default is None.\n",
      "            **kwargs: Any additional keyword arguments to pass to the agent.\n",
      "\n",
      "        Returns:\n",
      "            An agent.\n",
      "        \"\"\"\n",
      "        cls._validate_tools(tools)\n",
      "        prompt = cls.create_prompt(\n",
      "            tools,\n",
      "            ai_prefix=ai_prefix,\n",
      "            human_prefix=human_prefix,\n",
      "            prefix=prefix,\n",
      "            suffix=suffix,\n",
      "            format_instructions=format_instructions,\n",
      "            input_variables=input_variables,\n",
      "        )\n",
      "        llm_chain = LLMChain(  # type: ignore[misc]\n",
      "            llm=llm,\n",
      "            prompt=prompt,\n",
      "            callback_manager=callback_manager,\n",
      "        )\n",
      "        tool_names = [tool.name for tool in tools]\n",
      "        _output_parser = output_parser or cls._get_default_output_parser(\n",
      "            ai_prefix=ai_prefix\n",
      "        )\n",
      "        return cls(\n",
      "            llm_chain=llm_chain,\n",
      "            allowed_tools=tool_names,\n",
      "            ai_prefix=ai_prefix,\n",
      "            output_parser=_output_parser,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to get the full source code of a specific method\n",
    "def get_method_source(class_obj, method_name):\n",
    "    \"\"\"Retrieve the full source code of a method from a class.\"\"\"\n",
    "    try:\n",
    "        method = getattr(class_obj, method_name)\n",
    "        return inspect.getsource(method)\n",
    "    except AttributeError:\n",
    "        return f\"Method '{method_name}' not found.\"\n",
    "    except TypeError:\n",
    "        return f\"Could not retrieve source for '{method_name}'.\"\n",
    "\n",
    "# Example Usage: Get the code for a specific method\n",
    "method_name = \"from_llm_and_tools\"  # Change this to the method you want\n",
    "# ConversationalAgent = 'agent_code.ConversationalAgent'\n",
    "\n",
    "method_code = get_method_source(ConversationalAgent, method_name)\n",
    "\n",
    "print(f\"\\nCode for method '{method_name}':\\n\")\n",
    "print(method_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'ConversationalAgent' found in: ./agent_code.py\n",
      "\n",
      "Could not retrieve source for class 'ConversationalAgent'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_source(module, class_name):\n",
    "    \"\"\"Retrieve the source code of a class if it exists in a module.\"\"\"\n",
    "    class_obj = getattr(module, class_name, None)\n",
    "    if class_obj:\n",
    "        try:\n",
    "            return inspect.getsourcefile(ConversationalAgent)\n",
    "        except TypeError:\n",
    "            return f\"Could not retrieve source for class '{class_name}'.\"\n",
    "    return None\n",
    "\n",
    "def search_for_class(root_dir, class_name=\"ConversationalAgent\"):\n",
    "    \"\"\"Search for the given class in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if module:\n",
    "            class_source = get_class_source(module, class_name)\n",
    "            if class_source:\n",
    "                print(f\"Class '{class_name}' found in: {file}\\n\")\n",
    "                print(class_source)\n",
    "                return  # Stop after finding the first occurrence\n",
    "    print(f\"Class '{class_name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_class(root_repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'ConversationalAgent' not found in any file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_class_source(module, search_name):\n",
    "    \"\"\"Retrieve the source code of a class if it exists in a module.\"\"\"\n",
    "    class_obj = getattr(module, search_name, None)\n",
    "    if class_obj:\n",
    "        try:\n",
    "            return inspect.getsource(class_obj)  # Extract class definition only\n",
    "        except TypeError:\n",
    "            return None  # Unable to get source, likely a built-in or compiled class\n",
    "    return None\n",
    "\n",
    "def search_for_class(root_dir, search_name):\n",
    "    \"\"\"Search for the given class in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if module:\n",
    "            class_source = get_class_source(module, search_name)\n",
    "            if class_source:\n",
    "                print(f\"Class '{search_name}' found in: {file}\\n\")\n",
    "                print(class_source)  # Print only the class definition\n",
    "                return  # Stop after finding the first occurrence\n",
    "    print(f\"Class '{search_name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_class(root_repo_path, search_name = 'ConversationalAgent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'ConversationalAgent'\n",
    "class_obj = getattr(module, class_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_code.ConversationalAgent"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_agent_type' found in class 'Agent' in: ./agent_code.py\n",
      "\n",
      "    @property\n",
      "    def _agent_type(self) -> str:\n",
      "        \"\"\"Return Identifier of an agent type.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def find_python_files(root_dir):\n",
    "    \"\"\"Find all Python files in the root directory and subdirectories.\"\"\"\n",
    "    python_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(dirpath, file))\n",
    "    return python_files\n",
    "\n",
    "def load_module_from_path(file_path):\n",
    "    \"\"\"Dynamically load a Python module from a file path.\"\"\"\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    try:\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_source(obj):\n",
    "    \"\"\"Retrieve the source code of a function, method, class, or property.\"\"\"\n",
    "    try:\n",
    "        if isinstance(obj, classmethod):\n",
    "            return inspect.getsource(obj.__func__)  # Unwrap class method\n",
    "        elif isinstance(obj, staticmethod):\n",
    "            return inspect.getsource(obj.__func__)  # Unwrap static method\n",
    "        elif isinstance(obj, property):\n",
    "            return inspect.getsource(obj.fget)  # Get property getter\n",
    "        else:\n",
    "            return inspect.getsource(obj)  # Default for functions, methods, classes\n",
    "    except TypeError:\n",
    "        return None  # If the object is not directly accessible\n",
    "\n",
    "def search_for_name(root_dir, name):\n",
    "    \"\"\"Search for a class, function, or method in all Python files within the root directory.\"\"\"\n",
    "    python_files = find_python_files(root_dir)\n",
    "    \n",
    "    for file in python_files:\n",
    "        module = load_module_from_path(file)\n",
    "        if not module:\n",
    "            continue\n",
    "        \n",
    "        # 1️⃣ First, search for a class or function at the module level\n",
    "        obj = getattr(module, name, None)\n",
    "        if obj:\n",
    "            source_code = get_source(obj)\n",
    "            if source_code:\n",
    "                print(f\"'{name}' found in: {file}\\n\")\n",
    "                print(source_code)\n",
    "                return  # Stop after finding the first occurrence\n",
    "\n",
    "        # 2️⃣ Next, search inside classes for methods and properties\n",
    "        for class_name, class_obj in inspect.getmembers(module, inspect.isclass):\n",
    "            # Search for class methods, instance methods, and properties\n",
    "            for method_name, method_obj in inspect.getmembers(class_obj):\n",
    "                if method_name == name:\n",
    "                    source_code = get_source(method_obj)\n",
    "                    if source_code:\n",
    "                        print(f\"'{name}' found in class '{class_name}' in: {file}\\n\")\n",
    "                        print(source_code)\n",
    "                        return  # Stop after finding the first occurrence\n",
    "\n",
    "    print(f\"'{name}' not found in any file.\")\n",
    "\n",
    "# Run the search\n",
    "root_repo_path = \"./\"  # Change this to your actual repo path\n",
    "search_for_name(root_repo_path, \"_agent_type\")  # Change to search for any class, method, or function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ConversationalAgent' not found in any file.\n"
     ]
    }
   ],
   "source": [
    "search_for_name(root_repo_path, \"ConversationalAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
